{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b91b7a",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM) — Scikit-Learn Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87c816",
   "metadata": {},
   "source": [
    "\n",
    "**Goals**\n",
    "- Understand HMMs and the three classic problems (Forward, Viterbi, EM)\n",
    "- Explore a discrete dataset and a continuous (Gaussian) dataset\n",
    "- Train and decode using a scikit-learn–compatible API (`hmmlearn`) if available\n",
    "- Fall back to a pure-NumPy discrete HMM implementation otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    from hmmlearn.hmm import MultinomialHMM, GaussianHMM\n",
    "    HMMLEARN_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    HMMLEARN_AVAILABLE = False\n",
    "    print(\"hmmlearn not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752194d",
   "metadata": {},
   "source": [
    "## Generate Synthetic Datasets (Discrete & Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97096974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_discrete_hmm(T=600):\n",
    "    pi = np.array([0.5, 0.3, 0.2])\n",
    "    A  = np.array([[0.85, 0.10, 0.05],\n",
    "                   [0.10, 0.80, 0.10],\n",
    "                   [0.05, 0.15, 0.80]])\n",
    "    B  = np.array([\n",
    "        [0.30, 0.25, 0.20, 0.10, 0.05, 0.04, 0.03, 0.03],\n",
    "        [0.10, 0.10, 0.15, 0.25, 0.20, 0.05, 0.10, 0.05],\n",
    "        [0.05, 0.05, 0.05, 0.10, 0.15, 0.25, 0.20, 0.15],\n",
    "    ])\n",
    "    n_states, n_obs = A.shape[0], B.shape[1]\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    obs    = np.zeros(T, dtype=int)\n",
    "    states[0] = np.random.choice(n_states, p=pi)\n",
    "    obs[0]    = np.random.choice(n_obs, p=B[states[0]])\n",
    "    for t in range(1, T):\n",
    "        states[t] = np.random.choice(n_states, p=A[states[t-1]])\n",
    "        obs[t]    = np.random.choice(n_obs, p=B[states[t]])\n",
    "    return obs, states, pi, A, B\n",
    "\n",
    "def sample_gaussian_hmm(T=600, d=4):\n",
    "    pi = np.array([0.4, 0.4, 0.2])\n",
    "    A  = np.array([[0.90, 0.08, 0.02],\n",
    "                   [0.08, 0.88, 0.04],\n",
    "                   [0.05, 0.10, 0.85]])\n",
    "    mus = np.array([np.zeros(d), np.ones(d), np.concatenate([np.full(d//2,-2.0), np.full(d-d//2, 2.0)])])\n",
    "    covs= np.stack([np.eye(d)*0.4, np.eye(d)*0.5, np.eye(d)*0.7])\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    X = np.zeros((T,d))\n",
    "    states[0] = np.random.choice(3, p=pi)\n",
    "    X[0] = np.random.multivariate_normal(mus[states[0]], covs[states[0]])\n",
    "    for t in range(1,T):\n",
    "        states[t] = np.random.choice(3, p=A[states[t-1]])\n",
    "        X[t] = np.random.multivariate_normal(mus[states[t]], covs[states[t]])\n",
    "    return X, states, pi, A, mus, covs\n",
    "\n",
    "disc_obs, disc_states_true, disc_pi, disc_A, disc_B = sample_discrete_hmm()\n",
    "cont_X, cont_states_true, cont_pi, cont_A, cont_mus, cont_covs = sample_gaussian_hmm()\n",
    "print(\"Discrete obs sample:\", disc_obs[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b290fd9",
   "metadata": {},
   "source": [
    "## Quick EDA (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def heatmap(M, title):\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(M, aspect='auto')\n",
    "    plt.title(title); plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "heatmap(disc_A, \"True Transition Matrix A\")\n",
    "heatmap(disc_B, \"True Emission Matrix B\")\n",
    "\n",
    "counts = np.bincount(disc_obs, minlength=disc_B.shape[1])\n",
    "plt.figure()\n",
    "plt.bar(np.arange(disc_B.shape[1]), counts/counts.sum())\n",
    "plt.title(\"Observation frequencies\"); plt.xlabel(\"obs id\"); plt.ylabel(\"freq\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761f33d",
   "metadata": {},
   "source": [
    "## Pure NumPy Discrete HMM (Forward, Backward, Viterbi, EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_rows(M):\n",
    "    M = M.copy().astype(float)\n",
    "    row_sums = M.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums==0] = 1.0\n",
    "    return M / row_sums\n",
    "\n",
    "class DiscreteHMMNumpy:\n",
    "    def __init__(self, n_components, n_observations, max_iter=80, tol=1e-4, seed=0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.n_components = n_components\n",
    "        self.n_observations = n_observations\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.startprob_ = normalize_rows(rng.random((1,n_components)))[0]\n",
    "        self.transmat_  = normalize_rows(rng.random((n_components,n_components)))\n",
    "        self.emissionprob_ = normalize_rows(rng.random((n_components,n_observations)))\n",
    "\n",
    "    def _forward(self, obs):\n",
    "        T, N = len(obs), self.n_components\n",
    "        alpha = np.zeros((T,N)); scale = np.zeros(T)\n",
    "        alpha[0] = self.startprob_ * self.emissionprob_[:, obs[0]]\n",
    "        scale[0] = alpha[0].sum(); alpha[0] /= scale[0] + 1e-12\n",
    "        for t in range(1,T):\n",
    "            alpha[t] = (alpha[t-1] @ self.transmat_) * self.emissionprob_[:, obs[t]]\n",
    "            scale[t] = alpha[t].sum(); alpha[t] /= scale[t] + 1e-12\n",
    "        return alpha, scale, float(np.sum(np.log(scale + 1e-12)))\n",
    "\n",
    "    def _backward(self, obs, scale):\n",
    "        T, N = len(obs), self.n_components\n",
    "        beta = np.zeros((T,N)); beta[-1] = 1.0 / (scale[-1] + 1e-12)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            beta[t] = (self.transmat_ @ (self.emissionprob_[:, obs[t+1]] * beta[t+1]))\n",
    "            beta[t] /= (scale[t] + 1e-12)\n",
    "        return beta\n",
    "\n",
    "    def score(self, obs):\n",
    "        _, _, logp = self._forward(obs)\n",
    "        return logp\n",
    "\n",
    "    def predict(self, obs):\n",
    "        T, N = len(obs), self.n_components\n",
    "        logA = np.log(self.transmat_ + 1e-12)\n",
    "        logB = np.log(self.emissionprob_ + 1e-12)\n",
    "        logpi= np.log(self.startprob_ + 1e-12)\n",
    "        delta = np.zeros((T,N)); psi = np.zeros((T,N), dtype=int)\n",
    "        delta[0] = logpi + logB[:, obs[0]]\n",
    "        for t in range(1,T):\n",
    "            for j in range(N):\n",
    "                vals = delta[t-1] + logA[:,j]\n",
    "                psi[t,j] = int(np.argmax(vals))\n",
    "                delta[t,j] = np.max(vals) + logB[j, obs[t]]\n",
    "        states = np.zeros(T, dtype=int)\n",
    "        states[-1] = int(np.argmax(delta[-1]))\n",
    "        for t in range(T-2,-1,-1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "        return states\n",
    "\n",
    "    def fit(self, obs):\n",
    "        T = len(obs); last = -1e18\n",
    "        for _ in range(self.max_iter):\n",
    "            alpha, scale, logp = self._forward(obs)\n",
    "            beta = self._backward(obs, scale)\n",
    "            gamma = alpha * beta\n",
    "            gamma /= gamma.sum(axis=1, keepdims=True)\n",
    "            xi = np.zeros_like(self.transmat_)\n",
    "            for t in range(T-1):\n",
    "                numer = (alpha[t][:,None] * self.transmat_ * self.emissionprob_[:, obs[t+1]][None,:] * beta[t+1][None,:])\n",
    "                denom = numer.sum()\n",
    "                if denom > 0: xi += numer / (denom + 1e-12)\n",
    "            self.startprob_ = gamma[0] / (gamma[0].sum() + 1e-12)\n",
    "            self.transmat_  = normalize_rows(xi)\n",
    "            newB = np.zeros_like(self.emissionprob_)\n",
    "            for k in range(self.n_observations):\n",
    "                mask = (obs == k)\n",
    "                if mask.any(): newB[:,k] = gamma[mask].sum(axis=0)\n",
    "            self.emissionprob_ = normalize_rows(newB)\n",
    "            if logp - last < self.tol: break\n",
    "            last = logp\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181745e",
   "metadata": {},
   "source": [
    "## Train & Decode (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = len(disc_obs); split = int(0.7*T)\n",
    "train_obs = disc_obs[:split]; test_obs = disc_obs[split:]\n",
    "test_states_true = disc_states_true[split:]\n",
    "K = 3; M = 8\n",
    "\n",
    "if HMMLEARN_AVAILABLE:\n",
    "    model = MultinomialHMM(n_components=K, n_iter=100, tol=1e-3, init_params=\"ste\")\n",
    "    model.n_features = M\n",
    "    model.fit(train_obs.reshape(-1,1))\n",
    "    logp_train = model.score(train_obs.reshape(-1,1))\n",
    "    logp_test  = model.score(test_obs.reshape(-1,1))\n",
    "    pred = model.predict(test_obs.reshape(-1,1))\n",
    "else:\n",
    "    model = DiscreteHMMNumpy(n_components=K, n_observations=M, max_iter=80, tol=1e-4, seed=7)\n",
    "    model.fit(train_obs)\n",
    "    logp_train = model.score(train_obs)\n",
    "    logp_test  = model.score(test_obs)\n",
    "    pred = model.predict(test_obs)\n",
    "\n",
    "print(\"Train log-likelihood:\", round(logp_train,2))\n",
    "print(\"Test  log-likelihood:\", round(logp_test,2))\n",
    "\n",
    "# permutation-invariant accuracy\n",
    "def align_acc(true_s, pred_s):\n",
    "    K = len(np.unique(true_s))\n",
    "    C = np.zeros((K,K), dtype=int)\n",
    "    for t,p in zip(true_s, pred_s): C[t,p]+=1\n",
    "    import itertools\n",
    "    best=0\n",
    "    for perm in itertools.permutations(range(K)):\n",
    "        score = sum(C[i,perm[i]] for i in range(K))\n",
    "        best = max(best, score)\n",
    "    return best/len(pred_s)\n",
    "\n",
    "print(\"Viterbi accuracy (permutation-invariant):\", round(align_acc(test_states_true, pred)*100,1), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee490e20",
   "metadata": {},
   "source": [
    "## Gaussian HMM (Continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "X2 = PCA(n_components=2).fit_transform(cont_X)\n",
    "plt.figure(); plt.scatter(X2[:,0], X2[:,1], s=8); plt.title(\"Continuous features (PCA 2D)\"); plt.show()\n",
    "\n",
    "if HMMLEARN_AVAILABLE:\n",
    "    ghmm = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=100, tol=1e-3)\n",
    "    ghmm.fit(cont_X)\n",
    "    logp = ghmm.score(cont_X)\n",
    "    pred_g = ghmm.predict(cont_X)\n",
    "    print(\"GaussianHMM log-likelihood:\", round(logp,2))\n",
    "else:\n",
    "    print(\"hmmlearn not available -> skipping GaussianHMM fit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d4413",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cbef42",
   "metadata": {},
   "source": [
    "\n",
    "1) Change the emission matrix to make each state emit unique symbols; decode again.  \n",
    "2) Add Dirichlet pseudocounts in the M-step and inspect stability on short sequences.  \n",
    "3) Try K in {2,3,4,5}, compute BIC on the test split, and pick the best K.  \n",
    "4) For the Gaussian case, compare diag vs full covariances if `hmmlearn` is available.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
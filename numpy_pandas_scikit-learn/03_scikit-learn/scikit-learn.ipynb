{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build and evaluate classification, regression, and clustering models\n",
    "- Master the complete ML pipeline from data to predictions\n",
    "- Apply preprocessing techniques and model evaluation metrics\n",
    "- Understand when to use different algorithms and how to tune them\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals, Pandas data preprocessing (complete previous notebooks first)\n",
    "\n",
    "**Estimated Time:** ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn is the go-to library for machine learning in Python. This notebook brings together everything you've learned in NumPy and Pandas to build actual ML models that can make predictions on real data.\n",
    "\n",
    "**Why Scikit-learn?** It provides:\n",
    "- Consistent API across all algorithms (fit, predict, score)\n",
    "- Built-in preprocessing tools that work seamlessly with Pandas\n",
    "- Comprehensive model evaluation and validation tools\n",
    "- Production-ready implementations of proven algorithms\n",
    "\n",
    "**Learning Path Connection:** This notebook uses:\n",
    "- **NumPy skills**: Array operations, mathematical functions, broadcasting\n",
    "- **Pandas skills**: Data cleaning, feature engineering, train/test splits\n",
    "- **New ML skills**: Model training, evaluation, and prediction\n",
    "\n",
    "**What You'll Build:** Complete ML projects including customer classification, sales prediction, and customer segmentation - exactly what data scientists do every day!\n",
    "\n",
    "**üéØ Success Indicators:** By the end, you should be able to:\n",
    "- Train models and make accurate predictions on new data\n",
    "- Evaluate model performance using appropriate metrics\n",
    "- Choose the right algorithm for different types of problems\n",
    "- Build complete ML pipelines from raw data to final predictions\n",
    "\n",
    "**üí° Beginner Tips:**\n",
    "- Start simple - basic models often work surprisingly well\n",
    "- Always split your data before training (never test on training data!)\n",
    "- Focus on understanding the problem before choosing algorithms\n",
    "- Model evaluation is as important as model training\n",
    "\n",
    "**üîó ML Problem Types We'll Cover:**\n",
    "- **Classification**: Predicting categories (premium vs regular customers)\n",
    "- **Regression**: Predicting numbers (sales amounts, prices)\n",
    "- **Clustering**: Finding hidden groups in data (customer segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn core modules\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# ML Algorithms we'll use\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set random seed for reproducibility (remember this from NumPy and Pandas!)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Scikit-learn ready! Using reproducible random seed: 42\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Import sklearn and check version\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(\"\\nüöÄ Ready to build ML models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding ML Problem Types\n",
    "\n",
    "Before diving into algorithms, let's understand the three main types of ML problems. This foundation will help you choose the right approach for any real-world problem.\n",
    "\n",
    "**Connection to Previous Notebooks:**\n",
    "- **NumPy**: Provided the mathematical foundation (arrays, linear algebra)\n",
    "- **Pandas**: Handled data cleaning and preprocessing\n",
    "- **Scikit-learn**: Now we apply algorithms to make predictions\n",
    "\n",
    "**The Three Types of ML Problems:**\n",
    "\n",
    "1. **Supervised Learning**: Learning from labeled examples\n",
    "   - **Classification**: Predicting categories (spam/not spam, premium/regular)\n",
    "   - **Regression**: Predicting continuous numbers (price, temperature, sales)\n",
    "\n",
    "2. **Unsupervised Learning**: Finding patterns in data without labels\n",
    "   - **Clustering**: Grouping similar items (customer segments, product categories)\n",
    "   - **Dimensionality Reduction**: Simplifying complex data while keeping important patterns\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through trial and error (not covered in this notebook)\n",
    "\n",
    "**How to Choose:**\n",
    "- Got labeled data and want to predict categories? ‚Üí **Classification**\n",
    "- Got labeled data and want to predict numbers? ‚Üí **Regression**  \n",
    "- No labels but want to find hidden patterns? ‚Üí **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same customer dataset from Pandas notebook for consistency\n",
    "print(\"Creating Customer Dataset (same as Pandas notebook for consistency)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate the exact same dataset as Pandas notebook\n",
    "np.random.seed(42)  # Same seed = same data!\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                 n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'experience_years': np.random.exponential(5, n_samples),\n",
    "    'num_purchases': np.random.poisson(3, n_samples),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Our target!\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan\n",
    "\n",
    "print(f\"Dataset created: {df.shape[0]} customers, {df.shape[1]} features\")\n",
    "print(f\"Target variable: is_premium (0=Regular, 1=Premium)\")\n",
    "print(f\"Premium customers: {df['is_premium'].sum()} ({df['is_premium'].mean():.1%})\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüéØ CLASSIFICATION GOAL: Predict which customers will become premium members\")\n",
    "print(\"This is a binary classification problem (2 classes: 0 or 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification: Predicting Customer Premium Status\n",
    "\n",
    "Classification is about predicting categories or classes. Our goal: predict whether a customer will become a premium member based on their characteristics.\n",
    "\n",
    "**Real-world Applications:**\n",
    "- Email spam detection (spam/not spam)\n",
    "- Medical diagnosis (disease/healthy)\n",
    "- Customer churn prediction (will leave/will stay)\n",
    "- Image recognition (cat/dog/bird)\n",
    "\n",
    "**Our Classification Problem:**\n",
    "- **Features (X)**: age, income, education, experience, etc.\n",
    "- **Target (y)**: is_premium (0=Regular, 1=Premium)\n",
    "- **Goal**: Build a model that can predict premium status for new customers\n",
    "\n",
    "**The ML Workflow:**\n",
    "1. **Prepare Data**: Clean, encode, and split\n",
    "2. **Train Model**: Fit algorithm on training data\n",
    "3. **Evaluate**: Test performance on unseen data\n",
    "4. **Predict**: Make predictions on new customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing (applying Pandas skills!)\n",
    "print(\"Step 1: Data Preprocessing\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create a copy for ML processing\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Handle missing values (remember from Pandas!)\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# Fill missing values with median/mean\n",
    "df_ml['income'].fillna(df_ml['income'].median(), inplace=True)\n",
    "df_ml['satisfaction_score'].fillna(df_ml['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# PARAMETER EXPLANATION: LabelEncoder vs OneHotEncoder\n",
    "print(\"\\nPARAMETER EXPLANATION: Encoding Categorical Variables\")\n",
    "print(\"‚Ä¢ LabelEncoder: Converts categories to numbers (0, 1, 2, 3...)\")\n",
    "print(\"  - Use for: Ordinal data (High School < Bachelor < Master < PhD)\")\n",
    "print(\"  - Pros: Simple, compact, preserves order\")\n",
    "print(\"  - Cons: Implies numerical relationship between categories\")\n",
    "print(\"‚Ä¢ OneHotEncoder: Creates binary columns for each category\")\n",
    "print(\"  - Use for: Nominal data (North, South, East, West - no order)\")\n",
    "print(\"  - Pros: No false numerical relationships\")\n",
    "print(\"  - Cons: Creates many columns, can cause 'curse of dimensionality'\")\n",
    "print(\"‚Ä¢ ML Rule: Use LabelEncoder for ordinal, OneHot for nominal\")\n",
    "\n",
    "# Encode education (ordinal - has natural order)\n",
    "education_mapping = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_ml['education_encoded'] = df_ml['education'].map(education_mapping)\n",
    "\n",
    "print(\"\\nEducation encoding (ordinal):\")\n",
    "print(df_ml[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "# One-hot encode region (nominal - no natural order)\n",
    "region_dummies = pd.get_dummies(df_ml['region'], prefix='region')\n",
    "df_ml = pd.concat([df_ml, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\nRegion encoding (one-hot):\")\n",
    "print(f\"Original region column: {df_ml['region'].unique()}\")\n",
    "print(f\"New binary columns: {list(region_dummies.columns)}\")\n",
    "print(\"Sample of encoded regions:\")\n",
    "print(df_ml[['region'] + list(region_dummies.columns)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Selection and Preparation\n",
    "print(\"Step 2: Feature Selection\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Select features for our model (X) and target (y)\n",
    "feature_columns = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score',\n",
    "    'region_East', 'region_North', 'region_South', 'region_West'\n",
    "]\n",
    "\n",
    "X = df_ml[feature_columns].copy()\n",
    "y = df_ml['is_premium'].copy()\n",
    "\n",
    "print(f\"Features (X): {X.shape[1]} columns\")\n",
    "print(f\"Target (y): {y.shape[0]} samples\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")\n",
    "print(f\"Data types: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split (crucial for honest evaluation!)\n",
    "print(\"Step 3: Train-Test Split\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# PARAMETER EXPLANATION: train_test_split parameters\n",
    "print(\"PARAMETER EXPLANATION: train_test_split()\")\n",
    "print(\"‚Ä¢ test_size: Fraction of data to use for testing (0.2 = 20%)\")\n",
    "print(\"‚Ä¢ random_state: Seed for reproducible splits (same as np.random.seed)\")\n",
    "print(\"‚Ä¢ stratify: Ensures same class distribution in train and test sets\")\n",
    "print(\"‚Ä¢ Why stratify: Prevents imbalanced splits (e.g., all premium in train)\")\n",
    "print(\"‚Ä¢ Common test_size values: 0.2 (80/20), 0.3 (70/30), 0.25 (75/25)\")\n",
    "print(\"‚Ä¢ ML Rule: NEVER look at test data during model development!\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # Reproducible splits\n",
    "    stratify=y          # Keep same class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nClass distribution check:\")\n",
    "print(f\"Original: {y.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Training: {y_train.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(\"‚úÖ Distributions match - stratification worked!\")\n",
    "\n",
    "print(\"\\nüö® CRITICAL ML RULE: Test set is now 'locked away' until final evaluation!\")\n",
    "print(\"We'll only use X_train and y_train for model development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Scaling (important for many algorithms)\n",
    "print(\"Step 4: Feature Scaling\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# PARAMETER EXPLANATION: Why scaling matters\n",
    "print(\"WHY FEATURE SCALING MATTERS:\")\n",
    "print(\"‚Ä¢ Income: ranges from $20,000 to $200,000\")\n",
    "print(\"‚Ä¢ Age: ranges from 18 to 65\")\n",
    "print(\"‚Ä¢ Without scaling: Income dominates because of larger numbers\")\n",
    "print(\"‚Ä¢ With scaling: All features have equal influence\")\n",
    "print(\"‚Ä¢ Algorithms that need scaling: Logistic Regression, SVM, Neural Networks\")\n",
    "print(\"‚Ä¢ Algorithms that don't: Decision Trees, Random Forest\")\n",
    "\n",
    "# Check feature scales before scaling\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_train.describe().round(2))\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same scaling as training!\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_train_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHT: All features now have mean‚âà0 and std‚âà1\")\n",
    "print(\"This ensures fair treatment of all features in the model.\")\n",
    "\n",
    "# CRITICAL ML CONCEPT: Fit on train, transform on test\n",
    "print(\"\\nüö® CRITICAL CONCEPT: Data Leakage Prevention\")\n",
    "print(\"‚Ä¢ scaler.fit_transform(X_train): Learn scaling parameters from training data\")\n",
    "print(\"‚Ä¢ scaler.transform(X_test): Apply same scaling to test data\")\n",
    "print(\"‚Ä¢ NEVER fit scaler on test data - that's data leakage!\")\n",
    "print(\"‚Ä¢ Same rule applies to all preprocessing: fit on train, transform on test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Algorithms\n",
    "\n",
    "Now let's train different classification algorithms and compare their performance. Each algorithm has different strengths and is suited for different types of problems.\n",
    "\n",
    "**Algorithms We'll Compare:**\n",
    "1. **Logistic Regression**: Simple, interpretable, good baseline\n",
    "2. **Decision Tree**: Easy to understand, handles non-linear patterns\n",
    "3. **Random Forest**: Combines many trees, usually more accurate\n",
    "4. **K-Nearest Neighbors**: Simple concept, good for local patterns\n",
    "\n",
    "**The Scikit-learn Pattern:**\n",
    "All algorithms follow the same 3-step pattern:\n",
    "1. **Create**: `model = Algorithm()`\n",
    "2. **Train**: `model.fit(X_train, y_train)`\n",
    "3. **Predict**: `predictions = model.predict(X_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Logistic Regression\n",
    "print(\"üîç Algorithm 1: Logistic Regression\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Logistic Regression parameters\n",
    "print(\"ALGORITHM EXPLANATION: Logistic Regression\")\n",
    "print(\"‚Ä¢ What it does: Finds the best line to separate classes\")\n",
    "print(\"‚Ä¢ Strengths: Fast, interpretable, probabilistic predictions\")\n",
    "print(\"‚Ä¢ Weaknesses: Assumes linear relationships\")\n",
    "print(\"‚Ä¢ Best for: When you need to understand feature importance\")\n",
    "print(\"‚Ä¢ Output: Probability between 0 and 1 (>0.5 = class 1)\")\n",
    "print(\"‚Ä¢ Connection to NumPy: Uses matrix operations for optimization\")\n",
    "\n",
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "print(f\"\\nüìä Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log:.3f} ({accuracy_log:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_log == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_log[:10],\n",
    "    'Probability': y_pred_proba_log[:10].round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_log[:10])\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\nüéØ Feature Importance (coefficients):\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "print(\"\\nüí° Interpretation: Larger absolute coefficients = more important features\")\n",
    "print(\"Positive coefficients increase premium probability, negative decrease it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Decision Tree\n",
    "print(\"üå≥ Algorithm 2: Decision Tree\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree parameters\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree\")\n",
    "print(\"‚Ä¢ What it does: Creates a series of yes/no questions to classify data\")\n",
    "print(\"‚Ä¢ Strengths: Easy to understand, handles non-linear patterns, no scaling needed\")\n",
    "print(\"‚Ä¢ Weaknesses: Can overfit, unstable (small data changes = different tree)\")\n",
    "print(\"‚Ä¢ Best for: When you need interpretable rules (if age > 30 AND income > 50k...)\")\n",
    "print(\"‚Ä¢ max_depth: Limits tree depth to prevent overfitting\")\n",
    "print(\"‚Ä¢ min_samples_split: Minimum samples needed to split a node\")\n",
    "\n",
    "# Create and train the model (using original features, not scaled)\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,           # Limit depth to prevent overfitting\n",
    "    min_samples_split=20,  # Need at least 20 samples to split\n",
    "    random_state=42\n",
    ")\n",
    "tree_clf.fit(X_train, y_train)  # Note: using unscaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "y_pred_proba_tree = tree_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"\\nüìä Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_tree:.3f} ({accuracy_tree:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_tree == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (different from logistic regression!)\n",
    "print(\"\\nüéØ Feature Importance (based on information gain):\")\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': tree_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_importance)\n",
    "print(\"\\nüí° Interpretation: Higher importance = more useful for splitting data\")\n",
    "print(\"Tree importance shows which features create the purest splits\")\n",
    "\n",
    "# Show a few decision rules (simplified)\n",
    "print(\"\\nüå≥ Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"‚Ä¢ If income > $45,000 AND satisfaction > 3.2 ‚Üí Likely Premium\")\n",
    "print(\"‚Ä¢ If age < 25 AND num_purchases < 2 ‚Üí Likely Regular\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Random Forest\n",
    "print(\"üå≤üå≥üå≤ Algorithm 3: Random Forest\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest parameters\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest\")\n",
    "print(\"‚Ä¢ What it does: Combines predictions from many decision trees\")\n",
    "print(\"‚Ä¢ Strengths: Usually more accurate, reduces overfitting, handles missing values\")\n",
    "print(\"‚Ä¢ Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"‚Ä¢ Best for: When accuracy is more important than interpretability\")\n",
    "print(\"‚Ä¢ n_estimators: Number of trees (more trees = better but slower)\")\n",
    "print(\"‚Ä¢ max_depth: Depth of each tree\")\n",
    "print(\"‚Ä¢ Voting: Each tree votes, majority wins (ensemble method)\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,      # Use 100 trees\n",
    "    max_depth=5,           # Limit depth of each tree\n",
    "    min_samples_split=20,  # Same as single tree\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)  # Using unscaled data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nüìä Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.3f} ({accuracy_rf:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_rf == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\nüéØ Feature Importance (averaged across 100 trees):\")\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_importance)\n",
    "print(\"\\nüí° Interpretation: More stable importance scores than single tree\")\n",
    "print(\"Random Forest importance is more reliable due to averaging\")\n",
    "\n",
    "# Show confidence in predictions\n",
    "print(\"\\nüéØ Prediction Confidence (first 10 samples):\")\n",
    "confidence_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_rf[:10],\n",
    "    'Confidence': np.maximum(y_pred_proba_rf[:10], 1-y_pred_proba_rf[:10]).round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_rf[:10])\n",
    "})\n",
    "print(confidence_df)\n",
    "print(\"Higher confidence = more certain prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4: K-Nearest Neighbors\n",
    "print(\"üë• Algorithm 4: K-Nearest Neighbors (KNN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PARAMETER EXPLANATION: KNN parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Nearest Neighbors\")\n",
    "print(\"‚Ä¢ What it does: Classifies based on the K closest training examples\")\n",
    "print(\"‚Ä¢ Strengths: Simple concept, works well with local patterns\")\n",
    "print(\"‚Ä¢ Weaknesses: Slow with large datasets, sensitive to irrelevant features\")\n",
    "print(\"‚Ä¢ Best for: When similar items should have similar labels\")\n",
    "print(\"‚Ä¢ n_neighbors (K): How many neighbors to consider (odd numbers avoid ties)\")\n",
    "print(\"‚Ä¢ Distance: Usually Euclidean distance (requires scaling!)\")\n",
    "print(\"‚Ä¢ Lazy learning: No training phase, all work done during prediction\")\n",
    "\n",
    "# Create and train the model\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=5,  # Look at 5 nearest neighbors\n",
    "    weights='distance'  # Closer neighbors have more influence\n",
    ")\n",
    "knn_clf.fit(X_train_scaled, y_train)  # KNN needs scaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"\\nüìä K-Nearest Neighbors Results:\")\n",
    "print(f\"Accuracy: {accuracy_knn:.3f} ({accuracy_knn:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_knn == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# KNN doesn't have feature importance, but we can show prediction examples\n",
    "print(\"\\nüéØ How KNN Makes Predictions (conceptual):\")\n",
    "print(\"For each test sample:\")\n",
    "print(\"1. Find the 5 most similar customers in training data\")\n",
    "print(\"2. Look at their premium status (0 or 1)\")\n",
    "print(\"3. Take majority vote (e.g., 3 premium + 2 regular = predict premium)\")\n",
    "print(\"4. Weight by distance (closer neighbors count more)\")\n",
    "\n",
    "# Show some prediction probabilities\n",
    "print(\"\\nüìä KNN Prediction Examples (first 5 samples):\")\n",
    "knn_examples = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:5].values,\n",
    "    'Predicted': y_pred_knn[:5],\n",
    "    'Probability': y_pred_proba_knn[:5].round(3),\n",
    "    'Interpretation': [\n",
    "        f\"{int(p*5)}/5 neighbors were premium\" for p in y_pred_proba_knn[:5]\n",
    "    ]\n",
    "})\n",
    "print(knn_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison\n",
    "\n",
    "Accuracy is just one metric. Let's dive deeper into evaluation to understand which model is truly best for our problem.\n",
    "\n",
    "**Why Multiple Metrics Matter:**\n",
    "- **Accuracy**: Overall correctness, but can be misleading with imbalanced data\n",
    "- **Precision**: Of predicted positives, how many were actually positive?\n",
    "- **Recall**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Shows exactly where the model makes mistakes\n",
    "\n",
    "**Business Context Matters:**\n",
    "- High precision: Avoid false positives (don't waste premium offers on unlikely customers)\n",
    "- High recall: Catch all potential premiums (don't miss valuable customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models side by side\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate accuracies for all models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbors'],\n",
    "    'Accuracy': [accuracy_log, accuracy_tree, accuracy_rf, accuracy_knn],\n",
    "    'Correct_Predictions': [\n",
    "        (y_pred_log == y_test).sum(),\n",
    "        (y_pred_tree == y_test).sum(), \n",
    "        (y_pred_rf == y_test).sum(),\n",
    "        (y_pred_knn == y_test).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by accuracy\n",
    "models_comparison = models_comparison.sort_values('Accuracy', ascending=False)\n",
    "models_comparison['Accuracy_Percent'] = (models_comparison['Accuracy'] * 100).round(1)\n",
    "\n",
    "print(models_comparison)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = models_comparison.iloc[0]['Model']\n",
    "best_accuracy = models_comparison.iloc[0]['Accuracy']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} with {best_accuracy:.1%} accuracy\")\n",
    "\n",
    "# But let's look deeper with classification reports\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "models_and_predictions = [\n",
    "    ('Logistic Regression', y_pred_log),\n",
    "    ('Decision Tree', y_pred_tree),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('K-Nearest Neighbors', y_pred_knn)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * len(model_name))\n",
    "    print(classification_report(y_test, predictions, target_names=['Regular', 'Premium']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices - Show exactly where models make mistakes\n",
    "print(\"üîç CONFUSION MATRICES - Where Do Models Make Mistakes?\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# PARAMETER EXPLANATION: Confusion Matrix\n",
    "print(\"CONFUSION MATRIX EXPLANATION:\")\n",
    "print(\"‚Ä¢ Rows: Actual classes (what really happened)\")\n",
    "print(\"‚Ä¢ Columns: Predicted classes (what model predicted)\")\n",
    "print(\"‚Ä¢ Diagonal: Correct predictions\")\n",
    "print(\"‚Ä¢ Off-diagonal: Mistakes\")\n",
    "print(\"‚Ä¢ Top-left: True Negatives (correctly predicted Regular)\")\n",
    "print(\"‚Ä¢ Top-right: False Positives (predicted Premium, actually Regular)\")\n",
    "print(\"‚Ä¢ Bottom-left: False Negatives (predicted Regular, actually Premium)\")\n",
    "print(\"‚Ä¢ Bottom-right: True Positives (correctly predicted Premium)\")\n",
    "\n",
    "# Print numerical confusion matrices\n",
    "print(\"\\nNumerical Confusion Matrices:\")\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"Actual    Regular  Premium\")\n",
    "    print(f\"Regular      {cm[0,0]:3d}      {cm[0,1]:3d}\")\n",
    "    print(f\"Premium      {cm[1,0]:3d}      {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate error types\n",
    "    false_positives = cm[0,1]  # Predicted premium, actually regular\n",
    "    false_negatives = cm[1,0]  # Predicted regular, actually premium\n",
    "    \n",
    "    print(f\"False Positives: {false_positives} (wasted premium offers)\")\n",
    "    print(f\"False Negatives: {false_negatives} (missed premium customers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Impact Analysis\n",
    "print(\"üí∞ BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "print(\"Let's translate model performance into business terms:\")\n",
    "print(\"\\nScenario: Premium membership campaign\")\n",
    "print(\"‚Ä¢ Cost of premium offer: $50 per customer\")\n",
    "print(\"‚Ä¢ Revenue from premium customer: $200 per year\")\n",
    "print(\"‚Ä¢ Net profit from correct premium prediction: $150\")\n",
    "print(\"‚Ä¢ Cost of false positive (wasted offer): $50\")\n",
    "print(\"‚Ä¢ Cost of false negative (missed customer): $150 (lost revenue)\")\n",
    "\n",
    "# Calculate business impact for each model\n",
    "offer_cost = 50\n",
    "premium_revenue = 200\n",
    "net_profit = premium_revenue - offer_cost\n",
    "\n",
    "print(\"\\nüíº Business Impact by Model:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    true_positives = cm[1,1]   # Correctly identified premium customers\n",
    "    false_positives = cm[0,1]  # Wasted offers to regular customers\n",
    "    false_negatives = cm[1,0]  # Missed premium customers\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    profit_from_tp = true_positives * net_profit\n",
    "    cost_from_fp = false_positives * offer_cost\n",
    "    lost_revenue_fn = false_negatives * net_profit\n",
    "    \n",
    "    total_impact = profit_from_tp - cost_from_fp - lost_revenue_fn\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Profit from correct predictions: ${profit_from_tp:,}\")\n",
    "    print(f\"  Cost from wasted offers: ${cost_from_fp:,}\")\n",
    "    print(f\"  Lost revenue from missed customers: ${lost_revenue_fn:,}\")\n",
    "    print(f\"  NET BUSINESS IMPACT: ${total_impact:,}\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHT: The 'best' model depends on business priorities!\")\n",
    "print(\"‚Ä¢ If minimizing wasted offers is critical ‚Üí Choose high precision model\")\n",
    "print(\"‚Ä¢ If catching all premium customers is critical ‚Üí Choose high recall model\")\n",
    "print(\"‚Ä¢ For balanced approach ‚Üí Choose high F1-score model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression: Predicting Customer Lifetime Value\n",
    "\n",
    "Now let's switch from predicting categories (classification) to predicting continuous numbers (regression). We'll predict customer lifetime value based on their characteristics.\n",
    "\n",
    "**Real-world Regression Applications:**\n",
    "- House price prediction (real estate)\n",
    "- Stock price forecasting (finance)\n",
    "- Sales revenue prediction (business)\n",
    "- Temperature forecasting (weather)\n",
    "- Medical dosage optimization (healthcare)\n",
    "\n",
    "**Our Regression Problem:**\n",
    "- **Features (X)**: Same customer characteristics as before\n",
    "- **Target (y)**: Customer lifetime value in dollars\n",
    "- **Goal**: Predict how much revenue each customer will generate\n",
    "\n",
    "**Key Differences from Classification:**\n",
    "- **Output**: Continuous numbers instead of discrete categories\n",
    "- **Metrics**: MSE, MAE, R¬≤ instead of accuracy, precision, recall\n",
    "- **Algorithms**: Linear/Polynomial Regression, Decision Tree/Random Forest Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression target: Customer Lifetime Value (CLV)\n",
    "print(\"Creating Regression Target: Customer Lifetime Value\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Generate realistic CLV based on customer features\n",
    "np.random.seed(42)  # Consistent with our other data\n",
    "\n",
    "# CLV formula: Base value + bonuses based on customer characteristics + noise\n",
    "base_clv = 500  # Base customer value\n",
    "\n",
    "# Calculate CLV components (realistic business logic)\n",
    "age_bonus = (df_ml['age'] - 25) * 10  # Older customers worth more\n",
    "income_bonus = (df_ml['income'] / 1000) * 2  # Higher income = higher CLV\n",
    "education_bonus = df_ml['education_encoded'] * 100  # Education increases value\n",
    "satisfaction_bonus = df_ml['satisfaction_score'] * 150  # Happy customers spend more\n",
    "purchase_bonus = df_ml['num_purchases'] * 80  # Purchase history matters\n",
    "premium_bonus = df_ml['is_premium'] * 800  # Premium customers worth much more\n",
    "\n",
    "# Combine all factors\n",
    "clv_deterministic = (base_clv + age_bonus + income_bonus + \n",
    "                    education_bonus + satisfaction_bonus + \n",
    "                    purchase_bonus + premium_bonus)\n",
    "\n",
    "# Add realistic noise (business is never perfectly predictable)\n",
    "noise = np.random.normal(0, 200, len(df_ml))  # Random variation\n",
    "clv = clv_deterministic + noise\n",
    "\n",
    "# Ensure CLV is positive (can't have negative customer value)\n",
    "clv = np.maximum(clv, 100)  # Minimum CLV of $100\n",
    "\n",
    "# Add to our dataframe\n",
    "df_ml['customer_lifetime_value'] = clv\n",
    "\n",
    "print(f\"Customer Lifetime Value Statistics:\")\n",
    "print(f\"Mean CLV: ${clv.mean():.2f}\")\n",
    "print(f\"Median CLV: ${clv.median():.2f}\")\n",
    "print(f\"Min CLV: ${clv.min():.2f}\")\n",
    "print(f\"Max CLV: ${clv.max():.2f}\")\n",
    "print(f\"Standard Deviation: ${clv.std():.2f}\")\n",
    "\n",
    "# Show relationship between features and CLV\n",
    "print(\"\\nüîç CLV Correlations with Features:\")\n",
    "clv_correlations = df_ml[feature_columns + ['customer_lifetime_value']].corr()['customer_lifetime_value'].sort_values(ascending=False)\n",
    "print(clv_correlations.drop('customer_lifetime_value').round(3))\n",
    "\n",
    "print(\"\\nüí° Business Insights:\")\n",
    "print(\"‚Ä¢ Premium customers have significantly higher CLV\")\n",
    "print(\"‚Ä¢ Income and satisfaction strongly correlate with CLV\")\n",
    "print(\"‚Ä¢ Education level impacts long-term customer value\")\n",
    "print(\"‚Ä¢ Purchase history is a strong predictor\")\n",
    "\n",
    "print(\"\\nüéØ REGRESSION GOAL: Predict CLV for new customers to optimize marketing spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "print(\"Preparing Regression Data\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Use same features as classification, but different target\n",
    "X_reg = df_ml[feature_columns].copy()\n",
    "y_reg = df_ml['customer_lifetime_value'].copy()\n",
    "\n",
    "print(f\"Regression features: {X_reg.shape[1]} columns\")\n",
    "print(f\"Regression target: {y_reg.shape[0]} CLV values\")\n",
    "print(f\"Target range: ${y_reg.min():.0f} to ${y_reg.max():.0f}\")\n",
    "\n",
    "# Split data for regression (same random state for consistency)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42  # Same split as classification for comparison\n",
    ")\n",
    "\n",
    "print(f\"\\nRegression data split:\")\n",
    "print(f\"Training: {X_train_reg.shape[0]} samples\")\n",
    "print(f\"Testing: {X_test_reg.shape[0]} samples\")\n",
    "\n",
    "# Scale features for regression (some algorithms need it)\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_reg_scaled = pd.DataFrame(X_train_reg_scaled, columns=X_train_reg.columns, index=X_train_reg.index)\n",
    "X_test_reg_scaled = pd.DataFrame(X_test_reg_scaled, columns=X_test_reg.columns, index=X_test_reg.index)\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"Training CLV mean: ${y_train_reg.mean():.2f}\")\n",
    "print(f\"Testing CLV mean: ${y_test_reg.mean():.2f}\")\n",
    "print(\"‚úÖ Similar distributions - good split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Algorithms\n",
    "\n",
    "Let's train different regression algorithms to predict customer lifetime value. Each has different strengths for different types of relationships.\n",
    "\n",
    "**Regression Algorithms We'll Compare:**\n",
    "1. **Linear Regression**: Simple, interpretable, assumes linear relationships\n",
    "2. **Polynomial Regression**: Captures curved relationships\n",
    "3. **Decision Tree Regressor**: Handles non-linear patterns, no scaling needed\n",
    "4. **Random Forest Regressor**: Ensemble method, usually more accurate\n",
    "\n",
    "**Regression Metrics:**\n",
    "- **MAE (Mean Absolute Error)**: Average prediction error in dollars\n",
    "- **MSE (Mean Squared Error)**: Penalizes large errors more heavily\n",
    "- **RMSE (Root MSE)**: MSE in original units (dollars)\n",
    "- **R¬≤ Score**: Percentage of variance explained (0-1, higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Linear Regression\n",
    "print(\"üìà Algorithm 1: Linear Regression\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Linear Regression\n",
    "print(\"ALGORITHM EXPLANATION: Linear Regression\")\n",
    "print(\"‚Ä¢ What it does: Finds the best straight line through the data\")\n",
    "print(\"‚Ä¢ Strengths: Simple, fast, interpretable coefficients\")\n",
    "print(\"‚Ä¢ Weaknesses: Assumes linear relationships only\")\n",
    "print(\"‚Ä¢ Best for: When relationships are roughly linear\")\n",
    "print(\"‚Ä¢ Output: Continuous predictions (any real number)\")\n",
    "print(\"‚Ä¢ Connection to NumPy: Uses matrix operations (X^T * X)^-1 * X^T * y\")\n",
    "\n",
    "# Create and train the model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae_linear = mean_absolute_error(y_test_reg, y_pred_linear)\n",
    "mse_linear = mean_squared_error(y_test_reg, y_pred_linear)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "r2_linear = r2_score(y_test_reg, y_pred_linear)\n",
    "\n",
    "print(f\"\\nüìä Linear Regression Results:\")\n",
    "print(f\"MAE: ${mae_linear:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_linear:.2f} (root mean squared error)\")\n",
    "print(f\"R¬≤ Score: {r2_linear:.3f} ({r2_linear:.1%} of variance explained)\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "linear_results = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values.round(2),\n",
    "    'Predicted_CLV': y_pred_linear[:10].round(2),\n",
    "    'Error': (y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2),\n",
    "    'Abs_Error': np.abs(y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2)\n",
    "})\n",
    "print(linear_results)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\nüéØ Feature Coefficients (impact on CLV):\")\n",
    "linear_coef = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Coefficient': linear_reg.coef_,\n",
    "    'Abs_Coefficient': np.abs(linear_reg.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(linear_coef)\n",
    "print(f\"\\nIntercept: ${linear_reg.intercept_:.2f}\")\n",
    "print(\"\\nüí° Interpretation: Each coefficient shows CLV change per unit increase in feature\")\n",
    "print(\"Positive coefficients increase CLV, negative coefficients decrease it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Polynomial Regression (Linear Regression with polynomial features)\n",
    "print(\"üìä Algorithm 2: Polynomial Regression\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Polynomial Features\n",
    "print(\"ALGORITHM EXPLANATION: Polynomial Regression\")\n",
    "print(\"‚Ä¢ What it does: Creates curved relationships by adding x¬≤, x¬≥, x*y terms\")\n",
    "print(\"‚Ä¢ Strengths: Captures non-linear patterns, still interpretable\")\n",
    "print(\"‚Ä¢ Weaknesses: Can overfit easily, creates many features\")\n",
    "print(\"‚Ä¢ Best for: When you see curved relationships in data\")\n",
    "print(\"‚Ä¢ degree=2: Adds squared terms (x¬≤) for curves\")\n",
    "print(\"‚Ä¢ interaction_only=False: Includes both x¬≤ and x*y terms\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2 for quadratic relationships)\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_reg_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_reg_scaled)\n",
    "\n",
    "print(f\"\\nFeature expansion:\")\n",
    "print(f\"Original features: {X_train_reg_scaled.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"Added {X_train_poly.shape[1] - X_train_reg_scaled.shape[1]} polynomial terms\")\n",
    "\n",
    "# Train polynomial regression\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_poly = mean_absolute_error(y_test_reg, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_test_reg, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_test_reg, y_pred_poly)\n",
    "\n",
    "print(f\"\\nüìä Polynomial Regression Results:\")\n",
    "print(f\"MAE: ${mae_poly:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_poly:.2f} (root mean squared error)\")\n",
    "print(f\"R¬≤ Score: {r2_poly:.3f} ({r2_poly:.1%} of variance explained)\")\n",
    "\n",
    "# Compare with linear regression\n",
    "print(f\"\\nüìà Improvement over Linear Regression:\")\n",
    "mae_improvement = ((mae_linear - mae_poly) / mae_linear) * 100\n",
    "r2_improvement = r2_poly - r2_linear\n",
    "print(f\"MAE improvement: {mae_improvement:.1f}% better\")\n",
    "print(f\"R¬≤ improvement: +{r2_improvement:.3f} ({r2_improvement*100:.1f} percentage points)\")\n",
    "\n",
    "if r2_poly > r2_linear:\n",
    "    print(\"‚úÖ Polynomial features captured additional patterns!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Polynomial features didn't help - relationships might be mostly linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Decision Tree Regressor\n",
    "print(\"üå≥ Algorithm 3: Decision Tree Regressor\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree Regressor\")\n",
    "print(\"‚Ä¢ What it does: Creates rules to predict continuous values\")\n",
    "print(\"‚Ä¢ Strengths: Handles non-linear patterns, no scaling needed, interpretable\")\n",
    "print(\"‚Ä¢ Weaknesses: Can overfit, unstable with small data changes\")\n",
    "print(\"‚Ä¢ Best for: When relationships are complex and non-linear\")\n",
    "print(\"‚Ä¢ Prediction: Average of target values in each leaf node\")\n",
    "print(\"‚Ä¢ Example rule: If income > $50k AND age > 30 ‚Üí Predict CLV = $2,500\")\n",
    "\n",
    "# Create and train the model (using unscaled data)\n",
    "tree_reg = DecisionTreeRegressor(\n",
    "    max_depth=6,           # Slightly deeper for regression\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,   # Ensure meaningful leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "tree_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree_reg = tree_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_tree = mean_absolute_error(y_test_reg, y_pred_tree_reg)\n",
    "mse_tree = mean_squared_error(y_test_reg, y_pred_tree_reg)\n",
    "rmse_tree = np.sqrt(mse_tree)\n",
    "r2_tree = r2_score(y_test_reg, y_pred_tree_reg)\n",
    "\n",
    "print(f\"\\nüìä Decision Tree Regressor Results:\")\n",
    "print(f\"MAE: ${mae_tree:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_tree:.2f} (root mean squared error)\")\n",
    "print(f\"R¬≤ Score: {r2_tree:.3f} ({r2_tree:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nüéØ Feature Importance (for splitting):\")\n",
    "tree_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': tree_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_reg_importance)\n",
    "print(\"\\nüí° Interpretation: Higher importance = more useful for predicting CLV\")\n",
    "\n",
    "# Show some example decision paths (conceptual)\n",
    "print(\"\\nüå≥ Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"‚Ä¢ If income > $45,000 AND satisfaction > 3.5 ‚Üí Predict CLV ‚âà $2,800\")\n",
    "print(\"‚Ä¢ If age < 30 AND num_purchases < 3 ‚Üí Predict CLV ‚âà $1,200\")\n",
    "print(\"‚Ä¢ If premium=1 AND education=PhD ‚Üí Predict CLV ‚âà $4,500\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4: Random Forest Regressor\n",
    "print(\"üå≤üå≥üå≤ Algorithm 4: Random Forest Regressor\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest Regressor\")\n",
    "print(\"‚Ä¢ What it does: Averages predictions from many decision trees\")\n",
    "print(\"‚Ä¢ Strengths: Usually most accurate, reduces overfitting, handles missing values\")\n",
    "print(\"‚Ä¢ Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"‚Ä¢ Best for: When accuracy is more important than interpretability\")\n",
    "print(\"‚Ä¢ Prediction: Average of all tree predictions\")\n",
    "print(\"‚Ä¢ Example: Tree1=$2,400 + Tree2=$2,600 + Tree3=$2,500 ‚Üí Predict $2,500\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,      # Use 100 trees\n",
    "    max_depth=6,           # Same depth as single tree\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,   # Ensure meaningful predictions\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf_reg = rf_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test_reg, y_pred_rf_reg)\n",
    "mse_rf = mean_squared_error(y_test_reg, y_pred_rf_reg)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test_reg, y_pred_rf_reg)\n",
    "\n",
    "print(f\"\\nüìä Random Forest Regressor Results:\")\n",
    "print(f\"MAE: ${mae_rf:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_rf:.2f} (root mean squared error)\")\n",
    "print(f\"R¬≤ Score: {r2_rf:.3f} ({r2_rf:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\nüéØ Feature Importance (averaged across 100 trees):\")\n",
    "rf_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': rf_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_reg_importance)\n",
    "print(\"\\nüí° Interpretation: More stable importance scores than single tree\")\n",
    "\n",
    "# Show prediction confidence (using tree variance)\n",
    "print(\"\\nüéØ Prediction Examples with Confidence (first 5 samples):\")\n",
    "# Get predictions from individual trees for confidence estimation\n",
    "tree_predictions = np.array([tree.predict(X_test_reg.iloc[:5]) for tree in rf_reg.estimators_])\n",
    "prediction_std = np.std(tree_predictions, axis=0)\n",
    "\n",
    "rf_confidence = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:5].values.round(2),\n",
    "    'Predicted_CLV': y_pred_rf_reg[:5].round(2),\n",
    "    'Prediction_Std': prediction_std.round(2),\n",
    "    'Confidence_Range': [f\"¬±${std:.0f}\" for std in prediction_std]\n",
    "})\n",
    "print(rf_confidence)\n",
    "print(\"Lower standard deviation = more confident prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Model Comparison\n",
    "\n",
    "Let's compare all regression models to understand which performs best for predicting customer lifetime value.\n",
    "\n",
    "**Regression Metrics Explained:**\n",
    "- **MAE**: Mean Absolute Error - average prediction error in dollars (lower is better)\n",
    "- **RMSE**: Root Mean Squared Error - penalizes large errors more (lower is better)\n",
    "- **R¬≤ Score**: Coefficient of determination - percentage of variance explained (higher is better)\n",
    "\n",
    "**Business Context:**\n",
    "- **MAE**: \"On average, our predictions are off by $X\"\n",
    "- **RMSE**: \"Our model has larger penalties for big mistakes\"\n",
    "- **R¬≤**: \"Our model explains X% of why CLV varies between customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all regression models\n",
    "print(\"üìä REGRESSION MODEL COMPARISON\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "regression_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'MAE': [mae_linear, mae_poly, mae_tree, mae_rf],\n",
    "    'RMSE': [rmse_linear, rmse_poly, rmse_tree, rmse_rf],\n",
    "    'R¬≤_Score': [r2_linear, r2_poly, r2_tree, r2_rf]\n",
    "})\n",
    "\n",
    "# Sort by R¬≤ score (higher is better)\n",
    "regression_comparison = regression_comparison.sort_values('R¬≤_Score', ascending=False)\n",
    "regression_comparison['R¬≤_Percent'] = (regression_comparison['R¬≤_Score'] * 100).round(1)\n",
    "\n",
    "print(\"Model Performance Ranking (by R¬≤ Score):\")\n",
    "print(regression_comparison.round(2))\n",
    "\n",
    "# Identify best model\n",
    "best_reg_model = regression_comparison.iloc[0]['Model']\n",
    "best_r2 = regression_comparison.iloc[0]['R¬≤_Score']\n",
    "best_mae = regression_comparison.iloc[0]['MAE']\n",
    "\n",
    "print(f\"\\nüèÜ Best Regression Model: {best_reg_model}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.3f} ({best_r2:.1%} variance explained)\")\n",
    "print(f\"   Average Error: ${best_mae:.2f}\")\n",
    "\n",
    "# Calculate baseline comparison\n",
    "baseline_mae = mean_absolute_error(y_test_reg, [y_train_reg.mean()] * len(y_test_reg))\n",
    "print(f\"\\nüìè Baseline Comparison (predicting mean CLV):\")\n",
    "print(f\"   Baseline MAE: ${baseline_mae:.2f}\")\n",
    "print(f\"   Best Model MAE: ${best_mae:.2f}\")\n",
    "improvement = ((baseline_mae - best_mae) / baseline_mae) * 100\n",
    "print(f\"   Improvement: {improvement:.1f}% better than baseline\")\n",
    "\n",
    "# Show prediction accuracy ranges\n",
    "print(f\"\\nüéØ Prediction Accuracy Interpretation:\")\n",
    "print(f\"‚Ä¢ Our best model is typically off by ${best_mae:.0f} when predicting CLV\")\n",
    "print(f\"‚Ä¢ For a customer with ${y_test_reg.mean():.0f} actual CLV:\")\n",
    "print(f\"  - Prediction range: ${y_test_reg.mean()-best_mae:.0f} to ${y_test_reg.mean()+best_mae:.0f}\")\n",
    "print(f\"  - That's ¬±{(best_mae/y_test_reg.mean())*100:.1f}% relative error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed prediction analysis\n",
    "print(\"üîç DETAILED PREDICTION ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Compare predictions from all models\n",
    "prediction_comparison = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values,\n",
    "    'Linear_Pred': y_pred_linear[:10],\n",
    "    'Poly_Pred': y_pred_poly[:10],\n",
    "    'Tree_Pred': y_pred_tree_reg[:10],\n",
    "    'RF_Pred': y_pred_rf_reg[:10]\n",
    "})\n",
    "\n",
    "# Calculate errors for each model\n",
    "for model in ['Linear', 'Poly', 'Tree', 'RF']:\n",
    "    prediction_comparison[f'{model}_Error'] = (\n",
    "        prediction_comparison['Actual_CLV'] - prediction_comparison[f'{model}_Pred']\n",
    "    ).abs()\n",
    "\n",
    "print(\"Prediction Comparison (first 10 test samples):\")\n",
    "print(prediction_comparison.round(2))\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\nüìà Error Pattern Analysis:\")\n",
    "models_and_preds = [\n",
    "    ('Linear Regression', y_pred_linear),\n",
    "    ('Polynomial Regression', y_pred_poly),\n",
    "    ('Decision Tree', y_pred_tree_reg),\n",
    "    ('Random Forest', y_pred_rf_reg)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    errors = np.abs(y_test_reg - predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean Error: ${errors.mean():.2f}\")\n",
    "    print(f\"  Median Error: ${errors.median():.2f}\")\n",
    "    print(f\"  Max Error: ${errors.max():.2f}\")\n",
    "    print(f\"  % predictions within $500: {(errors <= 500).mean():.1%}\")\n",
    "    print(f\"  % predictions within $1000: {(errors <= 1000).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact of regression predictions\n",
    "print(\"üí∞ BUSINESS IMPACT OF CLV PREDICTIONS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"Business Scenario: Marketing Budget Allocation\")\n",
    "print(\"‚Ä¢ High CLV customers (>$3000): Premium marketing ($200 spend)\")\n",
    "print(\"‚Ä¢ Medium CLV customers ($1500-$3000): Standard marketing ($100 spend)\")\n",
    "print(\"‚Ä¢ Low CLV customers (<$1500): Basic marketing ($50 spend)\")\n",
    "print(\"‚Ä¢ Goal: Maximize ROI by targeting right customers with right campaigns\")\n",
    "\n",
    "# Define CLV segments\n",
    "def classify_clv(clv):\n",
    "    if clv >= 3000:\n",
    "        return 'High'\n",
    "    elif clv >= 1500:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Marketing costs by segment\n",
    "marketing_costs = {'High': 200, 'Medium': 100, 'Low': 50}\n",
    "\n",
    "print(\"\\nüíº Marketing ROI Analysis by Model:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    # Classify actual and predicted CLV\n",
    "    actual_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "    predicted_segments = [classify_clv(clv) for clv in predictions]\n",
    "    \n",
    "    # Calculate marketing spend based on predictions\n",
    "    predicted_spend = sum(marketing_costs[seg] for seg in predicted_segments)\n",
    "    \n",
    "    # Calculate actual ROI (revenue - marketing cost)\n",
    "    actual_revenue = y_test_reg.sum()\n",
    "    roi = actual_revenue - predicted_spend\n",
    "    roi_ratio = actual_revenue / predicted_spend\n",
    "    \n",
    "    # Calculate segment accuracy\n",
    "    segment_accuracy = sum(1 for a, p in zip(actual_segments, predicted_segments) if a == p) / len(actual_segments)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Total marketing spend: ${predicted_spend:,}\")\n",
    "    print(f\"  Total customer revenue: ${actual_revenue:,.0f}\")\n",
    "    print(f\"  Net ROI: ${roi:,.0f}\")\n",
    "    print(f\"  ROI ratio: {roi_ratio:.2f}x\")\n",
    "    print(f\"  Segment classification accuracy: {segment_accuracy:.1%}\")\n",
    "\n",
    "# Optimal allocation (if we knew true CLV)\n",
    "optimal_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "optimal_spend = sum(marketing_costs[seg] for seg in optimal_segments)\n",
    "optimal_roi = y_test_reg.sum() - optimal_spend\n",
    "\n",
    "print(f\"\\nüéØ Optimal Allocation (perfect predictions):\")\n",
    "print(f\"  Marketing spend: ${optimal_spend:,}\")\n",
    "print(f\"  Net ROI: ${optimal_roi:,.0f}\")\n",
    "print(f\"  ROI ratio: {y_test_reg.sum()/optimal_spend:.2f}x\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Better CLV predictions ‚Üí Better marketing allocation ‚Üí Higher ROI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering: Discovering Customer Segments\n",
    "\n",
    "Now let's explore unsupervised learning with clustering. Unlike classification and regression, we don't have a target variable - we're looking for hidden patterns in the data.\n",
    "\n",
    "**Real-world Clustering Applications:**\n",
    "- Customer segmentation (marketing)\n",
    "- Market research (identifying consumer groups)\n",
    "- Gene sequencing (biology)\n",
    "- Image segmentation (computer vision)\n",
    "- Anomaly detection (fraud, network security)\n",
    "\n",
    "**Our Clustering Problem:**\n",
    "- **Goal**: Discover natural customer segments based on behavior and characteristics\n",
    "- **Features**: Customer demographics and behavior (no target variable!)\n",
    "- **Output**: Group assignments (Cluster 0, 1, 2, etc.)\n",
    "- **Business Value**: Targeted marketing, personalized products, customer insights\n",
    "\n",
    "**Key Differences from Supervised Learning:**\n",
    "- **No labels**: We don't know the \"right\" answer beforehand\n",
    "- **Exploratory**: We're discovering patterns, not predicting outcomes\n",
    "- **Evaluation**: Harder to measure - we use internal metrics and business interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "print(\"Preparing Data for Customer Segmentation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select features for clustering (exclude target variables and IDs)\n",
    "clustering_features = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score'\n",
    "    # Note: Excluding region dummies and premium status for unsupervised learning\n",
    "]\n",
    "\n",
    "X_cluster = df_ml[clustering_features].copy()\n",
    "\n",
    "print(f\"Clustering features: {list(X_cluster.columns)}\")\n",
    "print(f\"Number of customers: {X_cluster.shape[0]}\")\n",
    "print(f\"Number of features: {X_cluster.shape[1]}\")\n",
    "\n",
    "# Check data quality\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values: {X_cluster.isnull().sum().sum()}\")\n",
    "print(f\"Data types: {X_cluster.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Scale features for clustering (very important!)\n",
    "print(\"\\nüîß SCALING FOR CLUSTERING:\")\n",
    "print(\"‚Ä¢ Clustering algorithms use distance calculations\")\n",
    "print(\"‚Ä¢ Features with larger scales dominate the distance\")\n",
    "print(\"‚Ä¢ Example: Income ($50,000) vs Age (30) - income dominates\")\n",
    "print(\"‚Ä¢ Solution: Scale all features to similar ranges\")\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "X_cluster_scaled = pd.DataFrame(X_cluster_scaled, columns=X_cluster.columns, index=X_cluster.index)\n",
    "\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_cluster.describe().round(2))\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_cluster_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\n‚úÖ All features now have mean‚âà0 and std‚âà1 - ready for clustering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means Clustering\n",
    "print(\"üéØ Algorithm: K-Means Clustering\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: K-Means parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Means Clustering\")\n",
    "print(\"‚Ä¢ What it does: Groups data into k clusters based on similarity\")\n",
    "print(\"‚Ä¢ How it works: Finds k cluster centers that minimize distances to points\")\n",
    "print(\"‚Ä¢ Strengths: Fast, simple, works well with spherical clusters\")\n",
    "print(\"‚Ä¢ Weaknesses: Assumes spherical clusters, sensitive to initialization\")\n",
    "print(\"‚Ä¢ n_clusters: Number of clusters to create\")\n",
    "print(\"‚Ä¢ random_state: Seed for reproducible results\")\n",
    "print(\"‚Ä¢ n_init: Number of random initializations (best result is kept)\")\n",
    "\n",
    "# For simplicity, we'll use 3 clusters (common for customer segmentation)\n",
    "optimal_k = 3\n",
    "print(f\"\\nüéØ Using k={optimal_k} clusters for clear customer segments\")\n",
    "\n",
    "# Create and fit K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=42,\n",
    "    n_init=10  # Try 10 different initializations\n",
    ")\n",
    "\n",
    "# Fit the model and get cluster assignments\n",
    "cluster_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to our dataframe\n",
    "df_clustered = df_ml.copy()\n",
    "df_clustered['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nüìä Clustering Results:\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(f\"Final WCSS: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully segmented {len(df_clustered)} customers into {optimal_k} clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and interpret customer segments\n",
    "print(\"üîç CUSTOMER SEGMENT ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate cluster centers in original scale for interpretation\n",
    "cluster_centers_scaled = kmeans.cluster_centers_\n",
    "cluster_centers_original = scaler_cluster.inverse_transform(cluster_centers_scaled)\n",
    "\n",
    "# Create cluster centers DataFrame\n",
    "cluster_centers_df = pd.DataFrame(\n",
    "    cluster_centers_original,\n",
    "    columns=clustering_features,\n",
    "    index=[f'Cluster_{i}' for i in range(optimal_k)]\n",
    ")\n",
    "\n",
    "print(\"Cluster Centers (Average Values):\")\n",
    "print(cluster_centers_df.round(2))\n",
    "\n",
    "# Detailed analysis by cluster\n",
    "print(\"\\nüìã DETAILED CLUSTER PROFILES:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nüéØ CLUSTER {cluster_id} PROFILE ({len(cluster_data)} customers):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Demographics\n",
    "    print(f\"Demographics:\")\n",
    "    print(f\"  Average Age: {cluster_data['age'].mean():.1f} years\")\n",
    "    print(f\"  Average Income: ${cluster_data['income'].mean():.0f}\")\n",
    "    print(f\"  Education: {cluster_data['education'].mode().iloc[0]} (most common)\")\n",
    "    \n",
    "    # Behavior\n",
    "    print(f\"Behavior:\")\n",
    "    print(f\"  Average Purchases: {cluster_data['num_purchases'].mean():.1f}\")\n",
    "    print(f\"  Average Satisfaction: {cluster_data['satisfaction_score'].mean():.2f}/5.0\")\n",
    "    print(f\"  Average Experience: {cluster_data['experience_years'].mean():.1f} years\")\n",
    "    \n",
    "    # Business metrics\n",
    "    print(f\"Business Value:\")\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    print(f\"  Premium Rate: {premium_rate:.1%}\")\n",
    "    print(f\"  Average CLV: ${avg_clv:.0f}\")\n",
    "    \n",
    "    # Region distribution\n",
    "    top_region = cluster_data['region'].mode().iloc[0]\n",
    "    region_pct = (cluster_data['region'] == top_region).mean()\n",
    "    print(f\"  Top Region: {top_region} ({region_pct:.1%})\")\n",
    "\n",
    "# Compare clusters side by side\n",
    "print(\"\\nüìä CLUSTER COMPARISON TABLE:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "comparison_metrics = []\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    metrics = {\n",
    "        'Cluster': f'Cluster_{cluster_id}',\n",
    "        'Size': len(cluster_data),\n",
    "        'Avg_Age': cluster_data['age'].mean(),\n",
    "        'Avg_Income': cluster_data['income'].mean(),\n",
    "        'Avg_Satisfaction': cluster_data['satisfaction_score'].mean(),\n",
    "        'Premium_Rate': cluster_data['is_premium'].mean(),\n",
    "        'Avg_CLV': cluster_data['customer_lifetime_value'].mean()\n",
    "    }\n",
    "    comparison_metrics.append(metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_metrics)\n",
    "print(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business interpretation and actionable insights\n",
    "print(\"üíº BUSINESS INTERPRETATION & MARKETING STRATEGY\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Analyze each cluster for business insights\n",
    "cluster_insights = []\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    avg_age = cluster_data['age'].mean()\n",
    "    avg_income = cluster_data['income'].mean()\n",
    "    avg_satisfaction = cluster_data['satisfaction_score'].mean()\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    size = len(cluster_data)\n",
    "    \n",
    "    # Generate business interpretation\n",
    "    if avg_clv > 2500 and premium_rate > 0.4:\n",
    "        segment_type = \"High-Value Customers\"\n",
    "        strategy = \"VIP treatment, loyalty programs, premium services\"\n",
    "        priority = \"HIGH\"\n",
    "    elif avg_clv > 1800 and avg_satisfaction > 3.5:\n",
    "        segment_type = \"Growth Potential\"\n",
    "        strategy = \"Upselling, premium conversion campaigns\"\n",
    "        priority = \"MEDIUM\"\n",
    "    else:\n",
    "        segment_type = \"Standard Customers\"\n",
    "        strategy = \"Retention programs, satisfaction improvement\"\n",
    "        priority = \"LOW\"\n",
    "    \n",
    "    cluster_insights.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'segment_type': segment_type,\n",
    "        'strategy': strategy,\n",
    "        'priority': priority,\n",
    "        'size': size,\n",
    "        'avg_clv': avg_clv\n",
    "    })\n",
    "\n",
    "# Display business insights\n",
    "for insight in cluster_insights:\n",
    "    print(f\"\\nüéØ CLUSTER {insight['cluster_id']}: {insight['segment_type']}\")\n",
    "    print(f\"   Size: {insight['size']} customers\")\n",
    "    print(f\"   Average CLV: ${insight['avg_clv']:.0f}\")\n",
    "    print(f\"   Priority: {insight['priority']}\")\n",
    "    print(f\"   Strategy: {insight['strategy']}\")\n",
    "\n",
    "# Calculate business impact\n",
    "print(f\"\\nüí∞ BUSINESS IMPACT ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "total_clv = df_clustered['customer_lifetime_value'].sum()\n",
    "print(f\"Total Customer Value: ${total_clv:,.0f}\")\n",
    "\n",
    "for insight in cluster_insights:\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == insight['cluster_id']]\n",
    "    cluster_clv = cluster_data['customer_lifetime_value'].sum()\n",
    "    clv_percentage = (cluster_clv / total_clv) * 100\n",
    "    \n",
    "    print(f\"\\nCluster {insight['cluster_id']} ({insight['segment_type']}):\")\n",
    "    print(f\"  Total Value: ${cluster_clv:,.0f} ({clv_percentage:.1f}% of total)\")\n",
    "    print(f\"  Size: {insight['size']} customers ({insight['size']/len(df_clustered)*100:.1f}% of base)\")\n",
    "    print(f\"  Value per Customer: ${cluster_clv/insight['size']:.0f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ Customer segmentation reveals distinct behavioral patterns\")\n",
    "print(\"‚Ä¢ High-value segments deserve premium marketing investment\")\n",
    "print(\"‚Ä¢ Growth potential segments are prime for upselling campaigns\")\n",
    "print(\"‚Ä¢ Targeted strategies can improve overall customer lifetime value\")\n",
    "print(\"‚Ä¢ Regular re-segmentation helps track customer evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation and Model Selection\n",
    "\n",
    "Let's dive deeper into cross-validation - a crucial technique for reliable model evaluation that every ML practitioner needs to master.\n",
    "\n",
    "**Why Cross-Validation Matters:**\n",
    "- Single train/test split might be lucky or unlucky\n",
    "- Small datasets need every sample for both training and testing\n",
    "- Provides more reliable performance estimates\n",
    "- Helps detect overfitting and model instability\n",
    "\n",
    "**When to Use Cross-Validation:**\n",
    "- Always for model selection and hyperparameter tuning\n",
    "- When you have limited data (< 10,000 samples)\n",
    "- To compare multiple algorithms fairly\n",
    "- Before deploying models to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Cross-Validation Analysis\n",
    "print(\"üîÑ COMPREHENSIVE CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# PARAMETER EXPLANATION: Cross-validation types\n",
    "print(\"CROSS-VALIDATION TYPES EXPLAINED:\")\n",
    "print(\"‚Ä¢ K-Fold CV: Split data into k equal parts, train on k-1, test on 1\")\n",
    "print(\"‚Ä¢ Stratified K-Fold: Maintains class distribution in each fold\")\n",
    "print(\"‚Ä¢ Leave-One-Out: Use each sample as test set once (for small datasets)\")\n",
    "print(\"‚Ä¢ Time Series CV: Respects temporal order (for time-dependent data)\")\n",
    "print(\"‚Ä¢ Common k values: 5 (fast), 10 (thorough), depends on dataset size\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Set up cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Models to compare with cross-validation\n",
    "cv_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=20, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Perform comprehensive cross-validation\n",
    "cv_results = {}\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "print(\"\\nüìä Cross-Validation Results (5-Fold Stratified):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, model in cv_models.items():\n",
    "    # Use appropriate data (scaled for algorithms that need it)\n",
    "    if model_name in ['Logistic Regression', 'K-Nearest Neighbors']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    # Perform cross-validation with multiple metrics\n",
    "    cv_scores = cross_validate(model, X_cv, y_train, cv=cv_folds, \n",
    "                              scoring=scoring_metrics, return_train_score=True)\n",
    "    \n",
    "    cv_results[model_name] = cv_scores\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric in scoring_metrics:\n",
    "        test_scores = cv_scores[f'test_{metric}']\n",
    "        train_scores = cv_scores[f'train_{metric}']\n",
    "        \n",
    "        print(f\"  {metric.capitalize()}:\")\n",
    "        print(f\"    CV Test:  {test_scores.mean():.3f} ¬± {test_scores.std():.3f}\")\n",
    "        print(f\"    CV Train: {train_scores.mean():.3f} ¬± {train_scores.std():.3f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        overfitting = train_scores.mean() - test_scores.mean()\n",
    "        if overfitting > 0.05:  # 5% gap indicates potential overfitting\n",
    "            print(f\"    ‚ö†Ô∏è Potential overfitting: {overfitting:.3f} gap\")\n",
    "        else:\n",
    "            print(f\"    ‚úÖ Good generalization: {overfitting:.3f} gap\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\nüèÜ MODEL RANKING BY CV ACCURACY:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "model_rankings = []\n",
    "for model_name, scores in cv_results.items():\n",
    "    accuracy_mean = scores['test_accuracy'].mean()\n",
    "    accuracy_std = scores['test_accuracy'].std()\n",
    "    model_rankings.append((model_name, accuracy_mean, accuracy_std))\n",
    "\n",
    "# Sort by accuracy\n",
    "model_rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (model_name, acc_mean, acc_std) in enumerate(model_rankings, 1):\n",
    "    print(f\"{i}. {model_name}: {acc_mean:.3f} ¬± {acc_std:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Cross-Validation Insights:\")\n",
    "print(\"‚Ä¢ Lower standard deviation = more consistent performance\")\n",
    "print(\"‚Ä¢ Small train-test gap = good generalization\")\n",
    "print(\"‚Ä¢ Use CV results for final model selection\")\n",
    "print(\"‚Ä¢ Always validate on separate holdout set before deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are the settings you configure before training (like max_depth for trees). Tuning them can significantly improve model performance.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Parameters**: Learned during training (weights, coefficients)\n",
    "- **Hyperparameters**: Set before training (learning rate, tree depth)\n",
    "- **Grid Search**: Try all combinations of hyperparameter values\n",
    "- **Random Search**: Try random combinations (often more efficient)\n",
    "- **Validation**: Always use cross-validation for hyperparameter tuning\n",
    "\n",
    "**Common Hyperparameters by Algorithm:**\n",
    "- **Random Forest**: n_estimators, max_depth, min_samples_split\n",
    "- **Logistic Regression**: C (regularization), penalty type\n",
    "- **KNN**: n_neighbors, weights, distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Example\n",
    "print(\"üîß HYPERPARAMETER TUNING EXAMPLE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: GridSearchCV\n",
    "print(\"GRID SEARCH EXPLANATION:\")\n",
    "print(\"‚Ä¢ What it does: Tries all combinations of hyperparameter values\")\n",
    "print(\"‚Ä¢ How it works: Uses cross-validation to evaluate each combination\")\n",
    "print(\"‚Ä¢ Pros: Thorough, finds optimal combination\")\n",
    "print(\"‚Ä¢ Cons: Can be slow with many parameters\")\n",
    "print(\"‚Ä¢ cv parameter: Number of cross-validation folds\")\n",
    "print(\"‚Ä¢ scoring: Metric to optimize (accuracy, f1, etc.)\")\n",
    "\n",
    "# Example: Tune Random Forest hyperparameters\n",
    "print(\"\\nüå≤ Tuning Random Forest Hyperparameters:\")\n",
    "\n",
    "# Define hyperparameter grid (start small for demo)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],        # Number of trees\n",
    "    'max_depth': [3, 5, 7, None],          # Tree depth\n",
    "    'min_samples_split': [10, 20, 50],     # Min samples to split\n",
    "    'min_samples_leaf': [5, 10, 20]        # Min samples in leaf\n",
    "}\n",
    "\n",
    "print(f\"Hyperparameter combinations to test: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}\")\n",
    "\n",
    "# Perform grid search (using smaller grid for speed)\n",
    "small_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [10, 20]\n",
    "}\n",
    "\n",
    "print(f\"\\nActual combinations tested: {len(small_param_grid['n_estimators']) * len(small_param_grid['max_depth']) * len(small_param_grid['min_samples_split'])}\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=small_param_grid,\n",
    "    cv=3,  # 3-fold CV for speed\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "print(\"\\nRunning grid search (this may take a moment...)\")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(f\"\\nüìä Grid Search Results:\")\n",
    "print(f\"Best CV Score: {rf_grid_search.best_score_:.3f}\")\n",
    "print(f\"Best Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# Compare with default model\n",
    "default_rf = RandomForestClassifier(random_state=42)\n",
    "default_scores = cross_val_score(default_rf, X_train, y_train, cv=3)\n",
    "\n",
    "print(f\"\\nüìà Improvement Analysis:\")\n",
    "print(f\"Default RF CV Score: {default_scores.mean():.3f} ¬± {default_scores.std():.3f}\")\n",
    "print(f\"Tuned RF CV Score:   {rf_grid_search.best_score_:.3f}\")\n",
    "improvement = rf_grid_search.best_score_ - default_scores.mean()\n",
    "print(f\"Improvement: {improvement:.3f} ({improvement*100:.1f} percentage points)\")\n",
    "\n",
    "# Test the tuned model\n",
    "tuned_predictions = rf_grid_search.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_predictions)\n",
    "print(f\"\\nTuned Model Test Accuracy: {tuned_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Hyperparameter Tuning Tips:\")\n",
    "print(\"‚Ä¢ Start with wide ranges, then narrow down\")\n",
    "print(\"‚Ä¢ Use RandomizedSearchCV for large parameter spaces\")\n",
    "print(\"‚Ä¢ Always validate on separate test set\")\n",
    "print(\"‚Ä¢ Consider computational cost vs. performance gain\")\n",
    "print(\"‚Ä¢ Document your tuning process for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretation and Feature Importance\n",
    "\n",
    "Understanding what your models learned is crucial for trust, debugging, and business insights. Let's explore different ways to interpret ML models.\n",
    "\n",
    "**Why Model Interpretation Matters:**\n",
    "- **Trust**: Understand why the model makes certain predictions\n",
    "- **Debugging**: Identify if the model learned the right patterns\n",
    "- **Business Insights**: Discover which factors drive outcomes\n",
    "- **Compliance**: Some industries require explainable models\n",
    "- **Feature Selection**: Remove irrelevant or harmful features\n",
    "\n",
    "**Types of Interpretability:**\n",
    "- **Global**: How the model works overall\n",
    "- **Local**: Why a specific prediction was made\n",
    "- **Model-specific**: Built-in interpretability (tree rules, coefficients)\n",
    "- **Model-agnostic**: Works with any model (SHAP, LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Feature Importance Analysis\n",
    "print(\"üîç COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Use our best tuned model\n",
    "best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# 1. Built-in Feature Importance\n",
    "print(\"1Ô∏è‚É£ BUILT-IN FEATURE IMPORTANCE (Random Forest):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_model.feature_importances_,\n",
    "    'Importance_Pct': best_model.feature_importances_ * 100\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.round(4))\n",
    "\n",
    "# Visualize top features\n",
    "print(\"\\nüìä Top 5 Most Important Features:\")\n",
    "top_features = feature_importance_df.head(5)\n",
    "for idx, row in top_features.iterrows():\n",
    "    bar_length = int(row['Importance_Pct'] / 2)  # Scale for display\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"{row['Feature']:<20} {bar} {row['Importance_Pct']:.1f}%\")\n",
    "\n",
    "# 2. Permutation Importance (more reliable)\n",
    "print(\"\\n2Ô∏è‚É£ PERMUTATION IMPORTANCE (More Reliable):\")\n",
    "print(\"-\" * 45)\n",
    "print(\"‚Ä¢ Shuffles each feature and measures performance drop\")\n",
    "print(\"‚Ä¢ More reliable than built-in importance\")\n",
    "print(\"‚Ä¢ Shows actual impact on model predictions\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test, y_test, \n",
    "    n_repeats=5, random_state=42, scoring='accuracy'\n",
    ")\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance_Mean': perm_importance.importances_mean,\n",
    "    'Importance_Std': perm_importance.importances_std\n",
    "}).sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance Results:\")\n",
    "print(perm_importance_df.round(4))\n",
    "\n",
    "# 3. Feature Importance Interpretation\n",
    "print(\"\\n3Ô∏è‚É£ BUSINESS INTERPRETATION:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get top 3 features for interpretation\n",
    "top_3_features = perm_importance_df.head(3)['Feature'].tolist()\n",
    "\n",
    "feature_interpretations = {\n",
    "    'satisfaction_score': 'Customer satisfaction is the strongest predictor - happy customers become premium',\n",
    "    'income': 'Higher income customers are more likely to afford premium services',\n",
    "    'num_purchases': 'Purchase history indicates engagement and likelihood to upgrade',\n",
    "    'age': 'Age correlates with disposable income and service preferences',\n",
    "    'education_encoded': 'Education level affects income and technology adoption',\n",
    "    'experience_years': 'Experience indicates established relationship with company'\n",
    "}\n",
    "\n",
    "print(\"Key Business Insights:\")\n",
    "for i, feature in enumerate(top_3_features, 1):\n",
    "    interpretation = feature_interpretations.get(feature, 'Important predictor for premium status')\n",
    "    print(f\"{i}. {feature}: {interpretation}\")\n",
    "\n",
    "# 4. Feature Selection Recommendations\n",
    "print(\"\\n4Ô∏è‚É£ FEATURE SELECTION RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify low-importance features\n",
    "low_importance = perm_importance_df[perm_importance_df['Importance_Mean'] < 0.01]\n",
    "\n",
    "if len(low_importance) > 0:\n",
    "    print(\"Features with very low importance (consider removing):\")\n",
    "    for feature in low_importance['Feature']:\n",
    "        print(f\"‚Ä¢ {feature}\")\n",
    "    print(\"\\nBenefits of removing low-importance features:\")\n",
    "    print(\"‚Ä¢ Faster training and prediction\")\n",
    "    print(\"‚Ä¢ Reduced overfitting risk\")\n",
    "    print(\"‚Ä¢ Simpler model maintenance\")\n",
    "    print(\"‚Ä¢ Lower data collection costs\")\n",
    "else:\n",
    "    print(\"‚úÖ All features show meaningful importance - keep current feature set\")\n",
    "\n",
    "print(\"\\nüí° Model Interpretation Best Practices:\")\n",
    "print(\"‚Ä¢ Use multiple interpretation methods for validation\")\n",
    "print(\"‚Ä¢ Consider business context when interpreting features\")\n",
    "print(\"‚Ä¢ Test feature removal impact before final decisions\")\n",
    "print(\"‚Ä¢ Document interpretation findings for stakeholders\")\n",
    "print(\"‚Ä¢ Regularly re-evaluate feature importance as data changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through machine learning with NumPy, Pandas, and scikit-learn. Let's summarize what you've accomplished and outline your path forward.\n",
    "\n",
    "## üéâ **What You've Mastered**\n",
    "\n",
    "### **NumPy Foundation**\n",
    "- ‚úÖ Array creation, manipulation, and broadcasting\n",
    "- ‚úÖ Linear algebra operations for ML\n",
    "- ‚úÖ Performance optimization techniques\n",
    "- ‚úÖ Integration with ML libraries\n",
    "\n",
    "### **Pandas Data Processing**\n",
    "- ‚úÖ Real-world data cleaning and preprocessing\n",
    "- ‚úÖ Feature engineering and categorical encoding\n",
    "- ‚úÖ Handling missing values and outliers\n",
    "- ‚úÖ Data splitting and validation preparation\n",
    "\n",
    "### **Scikit-learn Machine Learning**\n",
    "- ‚úÖ **Classification**: Customer premium prediction (4 algorithms)\n",
    "- ‚úÖ **Regression**: Customer lifetime value prediction (4 algorithms)\n",
    "- ‚úÖ **Clustering**: Customer segmentation (unsupervised learning)\n",
    "- ‚úÖ **Model Evaluation**: Comprehensive metrics and business impact\n",
    "- ‚úÖ **Cross-Validation**: Reliable model selection techniques\n",
    "- ‚úÖ **Hyperparameter Tuning**: Model optimization strategies\n",
    "- ‚úÖ **Model Interpretation**: Understanding feature importance\n",
    "\n",
    "## üöÄ **You're Now Ready For**\n",
    "\n",
    "### **Immediate Applications**\n",
    "- Build ML models for real business problems\n",
    "- Participate in data science projects at work\n",
    "- Contribute to ML discussions with confidence\n",
    "- Start personal ML projects with your own data\n",
    "\n",
    "### **Advanced Topics to Explore Next**\n",
    "1. **Deep Learning**: Neural networks with TensorFlow/PyTorch\n",
    "2. **Specialized ML**: NLP, computer vision, time series\n",
    "3. **Ensemble Methods**: XGBoost, LightGBM, stacking\n",
    "4. **MLOps**: Model deployment, monitoring, and maintenance\n",
    "5. **Advanced Evaluation**: A/B testing, causal inference\n",
    "\n",
    "## üìö **Recommended Learning Path**\n",
    "\n",
    "### **Beginner ‚Üí Intermediate (Next 3-6 months)**\n",
    "1. **Practice Projects**: Apply these skills to 2-3 personal datasets\n",
    "2. **Kaggle Competitions**: Start with beginner-friendly competitions\n",
    "3. **Specialized Libraries**: Explore XGBoost, Seaborn for visualization\n",
    "4. **Advanced Pandas**: Time series analysis, advanced groupby operations\n",
    "\n",
    "### **Intermediate ‚Üí Advanced (6-12 months)**\n",
    "1. **Deep Learning**: Start with image classification or NLP projects\n",
    "2. **Production Skills**: Learn Docker, cloud deployment (AWS/GCP)\n",
    "3. **Advanced Statistics**: Bayesian methods, experimental design\n",
    "4. **Domain Specialization**: Choose an area (finance, healthcare, etc.)\n",
    "\n",
    "## üéØ **Key Takeaways for Success**\n",
    "\n",
    "### **Technical Skills**\n",
    "- **Always start with data exploration** before building models\n",
    "- **Cross-validation is essential** for reliable model evaluation\n",
    "- **Feature engineering often matters more** than algorithm choice\n",
    "- **Business context drives** technical decisions\n",
    "\n",
    "### **Best Practices**\n",
    "- **Document your process** for reproducibility\n",
    "- **Start simple** then add complexity gradually\n",
    "- **Validate assumptions** with data exploration\n",
    "- **Communicate results** in business terms\n",
    "\n",
    "### **Continuous Learning**\n",
    "- **Stay updated** with new libraries and techniques\n",
    "- **Practice regularly** with diverse datasets\n",
    "- **Join communities** (Reddit r/MachineLearning, Stack Overflow)\n",
    "- **Read research papers** to understand cutting-edge methods\n",
    "\n",
    "## üåü **Final Words**\n",
    "\n",
    "You've built a solid foundation in machine learning that will serve you well throughout your career. The combination of NumPy's computational power, Pandas' data manipulation capabilities, and scikit-learn's ML algorithms gives you the tools to tackle most real-world ML problems.\n",
    "\n",
    "Remember: **Machine learning is as much about asking the right questions as it is about building models.** Focus on understanding the business problem, exploring the data thoroughly, and communicating your findings clearly.\n",
    "\n",
    "**Keep practicing, stay curious, and happy machine learning!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*These notebooks represent your first step into the exciting world of machine learning. The skills you've learned here will grow and evolve as you tackle new challenges and explore advanced techniques. Welcome to the ML community!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
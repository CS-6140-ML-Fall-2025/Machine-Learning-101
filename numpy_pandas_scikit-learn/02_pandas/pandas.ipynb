{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pandas for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives (short):**\n",
    "- Learn how to load and explore tabular data with Pandas\n",
    "- Clean, transform, and prepare features for modeling\n",
    "- Convert cleaned data to NumPy arrays for downstream tools\n",
    "\n",
    "**Prerequisites:** Basic Python and NumPy\n",
    "\n",
    "**Estimated Time:** ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is the go-to library for working with tabular data in Python. This notebook focuses on simple, clear examples and short explanations aimed at beginners. Advanced techniques are marked as optional."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.513770Z",
     "start_time": "2025-09-11T05:57:23.270384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrame Creation and Basic Operations\n",
    "\n",
    "Understanding how to create and manipulate DataFrames is fundamental to ML data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.529643Z",
     "start_time": "2025-09-11T05:57:23.518444Z"
    }
   },
   "source": [
    "# Create sample ML dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data for ML\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'experience_years': np.random.exponential(5, n_samples),\n",
    "    'num_purchases': np.random.poisson(3, n_samples),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_samples, freq='D')[:n_samples]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan\n",
    "\n",
    "print(\"Sample ML Dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ML Dataset:\n",
      "   customer_id  age        income    education  experience_years  \\\n",
      "0            1   40  44341.562353     Bachelor          1.800687   \n",
      "1            2   33  34972.483357  High School          4.143785   \n",
      "2            3   42  22693.077136     Bachelor          8.143228   \n",
      "3            4   53  15939.117886  High School          0.737563   \n",
      "4            5   32  31229.288168       Master          4.345832   \n",
      "\n",
      "   num_purchases  satisfaction_score  is_premium region signup_date  \n",
      "0              3            1.991730           0   East  2020-01-01  \n",
      "1              3            4.711166           0   East  2020-01-02  \n",
      "2              7            4.728536           1  North  2020-01-03  \n",
      "3              3            3.882346           1   East  2020-01-04  \n",
      "4              3            4.063053           0   East  2020-01-05  \n",
      "\n",
      "Dataset shape: (1000, 10)\n",
      "Memory usage: 186.23 KB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.550257Z",
     "start_time": "2025-09-11T05:57:23.538021Z"
    }
   },
   "source": [
    "# Basic DataFrame information (essential for ML)\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   customer_id         1000 non-null   int64         \n",
      " 1   age                 1000 non-null   int64         \n",
      " 2   income              980 non-null    float64       \n",
      " 3   education           1000 non-null   object        \n",
      " 4   experience_years    1000 non-null   float64       \n",
      " 5   num_purchases       1000 non-null   int64         \n",
      " 6   satisfaction_score  980 non-null    float64       \n",
      " 7   is_premium          1000 non-null   int64         \n",
      " 8   region              1000 non-null   object        \n",
      " 9   signup_date         1000 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int64(4), object(2)\n",
      "memory usage: 78.3+ KB\n",
      "None\n",
      "\n",
      "Data Types:\n",
      "customer_id                    int64\n",
      "age                            int64\n",
      "income                       float64\n",
      "education                     object\n",
      "experience_years             float64\n",
      "num_purchases                  int64\n",
      "satisfaction_score           float64\n",
      "is_premium                     int64\n",
      "region                        object\n",
      "signup_date           datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "customer_id            0\n",
      "age                    0\n",
      "income                20\n",
      "education              0\n",
      "experience_years       0\n",
      "num_purchases          0\n",
      "satisfaction_score    20\n",
      "is_premium             0\n",
      "region                 0\n",
      "signup_date            0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "       customer_id          age         income  experience_years  \\\n",
      "count  1000.000000  1000.000000     980.000000       1000.000000   \n",
      "mean    500.500000    34.743000   25729.730374          4.712565   \n",
      "min       1.000000    -3.000000    5063.461821          0.000154   \n",
      "25%     250.750000    27.000000   16266.763557          1.354730   \n",
      "50%     500.500000    35.000000   22732.225225          3.295243   \n",
      "75%     750.250000    42.000000   31711.722205          6.413300   \n",
      "max    1000.000000    81.000000  105754.353809         38.617648   \n",
      "std     288.819436    11.748233   13254.373746          4.769008   \n",
      "\n",
      "       num_purchases  satisfaction_score  is_premium          signup_date  \n",
      "count    1000.000000          980.000000  1000.00000                 1000  \n",
      "mean        2.933000            2.998455     0.29400  2021-05-14 12:00:00  \n",
      "min         0.000000            1.000211     0.00000  2020-01-01 00:00:00  \n",
      "25%         2.000000            2.078501     0.00000  2020-09-06 18:00:00  \n",
      "50%         3.000000            2.989607     0.00000  2021-05-14 12:00:00  \n",
      "75%         4.000000            3.929172     1.00000  2022-01-19 06:00:00  \n",
      "max         9.000000            4.994469     1.00000  2022-09-26 00:00:00  \n",
      "std         1.578291            1.117896     0.45582                  NaN  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Analysis\n",
    "\n",
    "Understanding your data is crucial before building ML models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.563801Z",
     "start_time": "2025-09-11T05:57:23.559650Z"
    }
   },
   "source": [
    "# Categorical data analysis\n",
    "print(\"Categorical Data Analysis:\")\n",
    "\n",
    "# Value counts for categorical features\n",
    "print(\"Education distribution:\")\n",
    "print(df['education'].value_counts())\n",
    "print(\"\\nEducation percentages:\")\n",
    "print(df['education'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nRegion distribution:\")\n",
    "print(df['region'].value_counts())\n",
    "\n",
    "print(\"\\nPremium customers:\")\n",
    "print(df['is_premium'].value_counts())\n",
    "print(f\"Premium rate: {df['is_premium'].mean():.2%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data Analysis:\n",
      "Education distribution:\n",
      "education\n",
      "Bachelor       382\n",
      "High School    315\n",
      "Master         209\n",
      "PhD             94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Education percentages:\n",
      "education\n",
      "Bachelor       38.2\n",
      "High School    31.5\n",
      "Master         20.9\n",
      "PhD             9.4\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Region distribution:\n",
      "region\n",
      "South    271\n",
      "West     260\n",
      "North    235\n",
      "East     234\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Premium customers:\n",
      "is_premium\n",
      "0    706\n",
      "1    294\n",
      "Name: count, dtype: int64\n",
      "Premium rate: 29.40%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.579509Z",
     "start_time": "2025-09-11T05:57:23.574950Z"
    }
   },
   "source": [
    "# Numerical data analysis\n",
    "print(\"Numerical Data Analysis:\")\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
    "\n",
    "# Correlation analysis (important for feature selection)\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"\\nCorrelation with target (is_premium):\")\n",
    "target_corr = correlation_matrix['is_premium'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Identify highly correlated features (multicollinearity)\n",
    "print(\"\\nHighly correlated feature pairs (|correlation| > 0.5):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "\n",
    "for col1, col2, corr in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2}: {corr:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Data Analysis:\n",
      "Numerical columns: ['customer_id', 'age', 'income', 'experience_years', 'num_purchases', 'satisfaction_score', 'is_premium']\n",
      "\n",
      "Correlation with target (is_premium):\n",
      "is_premium            1.000000\n",
      "customer_id           0.058631\n",
      "income                0.022244\n",
      "age                   0.008329\n",
      "satisfaction_score   -0.006930\n",
      "experience_years     -0.036799\n",
      "num_purchases        -0.042162\n",
      "Name: is_premium, dtype: float64\n",
      "\n",
      "Highly correlated feature pairs (|correlation| > 0.5):\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.598940Z",
     "start_time": "2025-09-11T05:57:23.592028Z"
    }
   },
   "source": [
    "# Groupby analysis (understanding patterns)\n",
    "print(\"Group Analysis:\")\n",
    "\n",
    "# Analyze by education level\n",
    "education_analysis = df.groupby('education').agg({\n",
    "    'age': ['mean', 'std'],\n",
    "    'income': ['mean', 'median'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Analysis by Education Level:\")\n",
    "print(education_analysis)\n",
    "\n",
    "# Analyze by region\n",
    "print(\"\\nAnalysis by Region:\")\n",
    "region_analysis = df.groupby('region').agg({\n",
    "    'income': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'satisfaction_score': 'mean'\n",
    "}).round(2)\n",
    "print(region_analysis)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Analysis:\n",
      "Analysis by Education Level:\n",
      "               age           income           satisfaction_score is_premium  \\\n",
      "              mean    std      mean    median               mean       mean   \n",
      "education                                                                     \n",
      "Bachelor     34.49  11.28  25722.33  22188.83               3.02       0.29   \n",
      "High School  33.90  12.14  25138.59  22588.63               3.00       0.28   \n",
      "Master       36.34  11.76  27224.65  23229.79               3.02       0.32   \n",
      "PhD          35.02  12.08  24331.07  22946.36               2.86       0.29   \n",
      "\n",
      "            customer_id  \n",
      "                  count  \n",
      "education                \n",
      "Bachelor            382  \n",
      "High School         315  \n",
      "Master              209  \n",
      "PhD                  94  \n",
      "\n",
      "Analysis by Region:\n",
      "          income  is_premium  satisfaction_score\n",
      "region                                          \n",
      "East    26469.21        0.27                2.90\n",
      "North   25249.80        0.28                2.97\n",
      "South   25172.40        0.32                3.01\n",
      "West    26063.05        0.30                3.10\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Essential steps before feeding data to ML models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.613480Z",
     "start_time": "2025-09-11T05:57:23.608501Z"
    }
   },
   "source": [
    "# Handle missing values\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Strategy 1: Fill numerical missing values with median\n",
    "df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
    "df_clean['satisfaction_score'].fillna(df_clean['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Alternative strategies\n",
    "print(\"\\nAlternative missing value strategies:\")\n",
    "print(\"1. Forward fill: df.fillna(method='ffill')\")\n",
    "print(\"2. Backward fill: df.fillna(method='bfill')\")\n",
    "print(\"3. Interpolation: df.interpolate()\")\n",
    "print(\"4. Drop rows: df.dropna()\")\n",
    "print(\"5. Drop columns: df.dropna(axis=1)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Missing Values:\n",
      "Missing values before cleaning:\n",
      "customer_id            0\n",
      "age                    0\n",
      "income                20\n",
      "education              0\n",
      "experience_years       0\n",
      "num_purchases          0\n",
      "satisfaction_score    20\n",
      "is_premium             0\n",
      "region                 0\n",
      "signup_date            0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "customer_id           0\n",
      "age                   0\n",
      "income                0\n",
      "education             0\n",
      "experience_years      0\n",
      "num_purchases         0\n",
      "satisfaction_score    0\n",
      "is_premium            0\n",
      "region                0\n",
      "signup_date           0\n",
      "dtype: int64\n",
      "\n",
      "Alternative missing value strategies:\n",
      "1. Forward fill: df.fillna(method='ffill')\n",
      "2. Backward fill: df.fillna(method='bfill')\n",
      "3. Interpolation: df.interpolate()\n",
      "4. Drop rows: df.dropna()\n",
      "5. Drop columns: df.dropna(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_50712/3316047092.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_50712/3316047092.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['satisfaction_score'].fillna(df_clean['satisfaction_score'].mean(), inplace=True)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.649668Z",
     "start_time": "2025-09-11T05:57:23.640714Z"
    }
   },
   "source": [
    "# Handle outliers\n",
    "print(\"Outlier Detection and Handling:\")\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "# Check for outliers in income\n",
    "income_outliers = detect_outliers_iqr(df_clean['income'])\n",
    "print(f\"Income outliers: {income_outliers.sum()} ({income_outliers.mean():.1%})\")\n",
    "\n",
    "# Visualize outliers\n",
    "print(\"Income statistics:\")\n",
    "print(f\"Mean: ${df_clean['income'].mean():.0f}\")\n",
    "print(f\"Median: ${df_clean['income'].median():.0f}\")\n",
    "print(f\"95th percentile: ${df_clean['income'].quantile(0.95):.0f}\")\n",
    "print(f\"99th percentile: ${df_clean['income'].quantile(0.99):.0f}\")\n",
    "print(f\"Max: ${df_clean['income'].max():.0f}\")\n",
    "\n",
    "# Handle outliers (cap at 95th percentile)\n",
    "income_cap = df_clean['income'].quantile(0.95)\n",
    "df_clean['income_capped'] = df_clean['income'].clip(upper=income_cap)\n",
    "\n",
    "print(\"\\nAfter capping at 95th percentile:\")\n",
    "print(f\"Max income: ${df_clean['income_capped'].max():.0f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Detection and Handling:\n",
      "Income outliers: 37 (3.7%)\n",
      "Income statistics:\n",
      "Mean: $25670\n",
      "Median: $22732\n",
      "95th percentile: $50839\n",
      "99th percentile: $69259\n",
      "Max: $105754\n",
      "\n",
      "After capping at 95th percentile:\n",
      "Max income: $50839\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.666587Z",
     "start_time": "2025-09-11T05:57:23.658915Z"
    }
   },
   "source": [
    "# Data type optimization (important for large datasets)\n",
    "print(\"Data Type Optimization:\")\n",
    "print(f\"Memory usage before optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize integer columns\n",
    "int_cols = df_clean.select_dtypes(include=['int64']).columns\n",
    "for col in int_cols:\n",
    "    if col != 'customer_id':  # Keep ID as int64\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], downcast='integer')\n",
    "\n",
    "# Optimize float columns\n",
    "float_cols = df_clean.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], downcast='float')\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "categorical_cols = ['education', 'region']\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "print(f\"Memory usage after optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {(1 - df_clean.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_clean.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type Optimization:\n",
      "Memory usage before optimization: 194.04 KB\n",
      "Memory usage after optimization: 44.90 KB\n",
      "Memory reduction: 75.9%\n",
      "\n",
      "Optimized data types:\n",
      "customer_id                    int64\n",
      "age                             int8\n",
      "income                       float64\n",
      "education                   category\n",
      "experience_years             float32\n",
      "                           ...      \n",
      "satisfaction_score           float32\n",
      "is_premium                      int8\n",
      "region                      category\n",
      "signup_date           datetime64[ns]\n",
      "income_capped                float64\n",
      "Length: 11, dtype: object\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creating new features that can improve ML model performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.683678Z",
     "start_time": "2025-09-11T05:57:23.675379Z"
    }
   },
   "source": [
    "# Feature engineering examples\n",
    "print(\"Feature Engineering:\")\n",
    "\n",
    "# 1. Binning continuous variables\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'],\n",
    "                              bins=[0, 25, 35, 50, 100],\n",
    "                              labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "df_clean['income_tier'] = pd.qcut(df_clean['income_capped'],\n",
    "                                 q=4,\n",
    "                                 labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Age group distribution:\")\n",
    "print(df_clean['age_group'].value_counts())\n",
    "\n",
    "print(\"\\nIncome tier distribution:\")\n",
    "print(df_clean['income_tier'].value_counts())\n",
    "\n",
    "# 2. Mathematical transformations\n",
    "df_clean['log_income'] = np.log1p(df_clean['income_capped'])  # log(1+x) to handle zeros\n",
    "df_clean['income_per_purchase'] = df_clean['income_capped'] / (df_clean['num_purchases'] + 1)\n",
    "df_clean['satisfaction_squared'] = df_clean['satisfaction_score'] ** 2\n",
    "\n",
    "# 3. Date-based features\n",
    "df_clean['signup_year'] = df_clean['signup_date'].dt.year\n",
    "df_clean['signup_month'] = df_clean['signup_date'].dt.month\n",
    "df_clean['signup_dayofweek'] = df_clean['signup_date'].dt.dayofweek\n",
    "df_clean['days_since_signup'] = (datetime.now() - df_clean['signup_date']).dt.days\n",
    "\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = ['age_group', 'income_tier', 'log_income', 'income_per_purchase',\n",
    "                'satisfaction_squared', 'signup_year', 'signup_month', 'days_since_signup']\n",
    "print(df_clean[new_features].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering:\n",
      "Age group distribution:\n",
      "age_group\n",
      "Middle-aged    374\n",
      "Adult          309\n",
      "Young          218\n",
      "Senior          98\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income tier distribution:\n",
      "income_tier\n",
      "Medium       260\n",
      "Low          250\n",
      "Very High    250\n",
      "High         240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New features created:\n",
      "     age_group income_tier  log_income  income_per_purchase  \\\n",
      "0  Middle-aged   Very High   10.699700         11085.390588   \n",
      "1        Adult   Very High   10.462345          8743.120839   \n",
      "2  Middle-aged      Medium   10.029859          2836.634642   \n",
      "3       Senior         Low    9.676594          3984.779471   \n",
      "4        Adult        High   10.349144          7807.322042   \n",
      "\n",
      "   satisfaction_squared  signup_year  signup_month  days_since_signup  \n",
      "0              3.966988         2020             1               2079  \n",
      "1             22.195084         2020             1               2078  \n",
      "2             22.359049         2020             1               2077  \n",
      "3             15.072608         2020             1               2076  \n",
      "4             16.508400         2020             1               2075  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.699471Z",
     "start_time": "2025-09-11T05:57:23.692049Z"
    }
   },
   "source": [
    "# Interaction features\n",
    "print(\"Interaction Features:\")\n",
    "\n",
    "# Create interaction between important features\n",
    "df_clean['age_income_interaction'] = df_clean['age'] * df_clean['log_income']\n",
    "df_clean['experience_satisfaction'] = df_clean['experience_years'] * df_clean['satisfaction_score']\n",
    "\n",
    "# Boolean combinations\n",
    "df_clean['high_income_high_satisfaction'] = (\n",
    "    (df_clean['income_tier'] == 'Very High') &\n",
    "    (df_clean['satisfaction_score'] > 4)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['experienced_premium'] = (\n",
    "    (df_clean['experience_years'] > 5) &\n",
    "    (df_clean['is_premium'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "print(\"Interaction features:\")\n",
    "interaction_features = ['age_income_interaction', 'experience_satisfaction',\n",
    "                       'high_income_high_satisfaction', 'experienced_premium']\n",
    "print(df_clean[interaction_features].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Features:\n",
      "Interaction features:\n",
      "       age_income_interaction  experience_satisfaction  \\\n",
      "count             1000.000000              1000.000000   \n",
      "mean               348.022457                13.925125   \n",
      "std                118.560771                15.730806   \n",
      "min                -30.808238                 0.000356   \n",
      "25%                267.392329                 3.523085   \n",
      "50%                348.322690                 9.498555   \n",
      "75%                425.520200                18.350727   \n",
      "max                877.751173               175.326508   \n",
      "\n",
      "       high_income_high_satisfaction  experienced_premium  \n",
      "count                    1000.000000          1000.000000  \n",
      "mean                        0.067000             0.084000  \n",
      "std                         0.250147             0.277527  \n",
      "min                         0.000000             0.000000  \n",
      "25%                         0.000000             0.000000  \n",
      "50%                         0.000000             0.000000  \n",
      "75%                         0.000000             0.000000  \n",
      "max                         1.000000             1.000000  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.717235Z",
     "start_time": "2025-09-11T05:57:23.708218Z"
    }
   },
   "source": [
    "# Aggregation features (useful for time series or grouped data)\n",
    "print(\"Aggregation Features:\")\n",
    "\n",
    "# Features based on region\n",
    "region_stats = df_clean.groupby('region').agg({\n",
    "    'income_capped': ['mean', 'std'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "region_stats.columns = ['_'.join(col).strip() for col in region_stats.columns]\n",
    "region_stats = region_stats.add_prefix('region_')\n",
    "\n",
    "# Merge back to main dataframe\n",
    "df_clean = df_clean.merge(region_stats, left_on='region', right_index=True, how='left')\n",
    "\n",
    "print(\"Region-based features:\")\n",
    "region_features = [col for col in df_clean.columns if col.startswith('region_')]\n",
    "print(df_clean[['region'] + region_features].head())\n",
    "\n",
    "# Relative features (compare individual to group)\n",
    "df_clean['income_vs_region_mean'] = df_clean['income_capped'] / df_clean['region_income_capped_mean']\n",
    "df_clean['satisfaction_vs_region_mean'] = df_clean['satisfaction_score'] / df_clean['region_satisfaction_score_mean']\n",
    "\n",
    "print(\"\\nRelative features:\")\n",
    "print(df_clean[['income_vs_region_mean', 'satisfaction_vs_region_mean']].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Features:\n",
      "Region-based features:\n",
      "  region  region_income_capped_mean  region_income_capped_std  \\\n",
      "0   East                  25677.381                 10974.508   \n",
      "1   East                  25677.381                 10974.508   \n",
      "2  North                  24758.733                 11104.631   \n",
      "3   East                  25677.381                 10974.508   \n",
      "4   East                  25677.381                 10974.508   \n",
      "\n",
      "   region_satisfaction_score_mean  region_is_premium_mean  \n",
      "0                           2.901                   0.269  \n",
      "1                           2.901                   0.269  \n",
      "2                           2.971                   0.281  \n",
      "3                           2.901                   0.269  \n",
      "4                           2.901                   0.269  \n",
      "\n",
      "Relative features:\n",
      "       income_vs_region_mean  satisfaction_vs_region_mean\n",
      "count            1000.000000                  1000.000000\n",
      "mean                1.000000                     0.999992\n",
      "std                 0.452976                     0.368513\n",
      "min                 0.202645                     0.328418\n",
      "25%                 0.654898                     0.696641\n",
      "50%                 0.904276                     0.995834\n",
      "75%                 1.251632                     1.306528\n",
      "max                 2.085304                     1.721367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_50712/2201827955.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  region_stats = df_clean.groupby('region').agg({\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "Converting categorical variables to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.734561Z",
     "start_time": "2025-09-11T05:57:23.728206Z"
    }
   },
   "source": [
    "# One-hot encoding\n",
    "print(\"One-Hot Encoding:\")\n",
    "\n",
    "# Select categorical columns for encoding\n",
    "categorical_cols = ['education', 'region', 'age_group', 'income_tier']\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Shape before encoding: {df_clean.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Show new columns\n",
    "new_cols = [col for col in df_encoded.columns if any(cat in col for cat in categorical_cols)]\n",
    "print(f\"\\nNew encoded columns ({len(new_cols)}):\")\n",
    "for col in new_cols[:10]:  # Show first 10\n",
    "    print(f\"  {col}\")\n",
    "if len(new_cols) > 10:\n",
    "    print(f\"  ... and {len(new_cols) - 10} more\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding:\n",
      "Shape before encoding: (1000, 30)\n",
      "Shape after encoding: (1000, 38)\n",
      "\n",
      "New encoded columns (18):\n",
      "  region_income_capped_mean\n",
      "  region_income_capped_std\n",
      "  region_satisfaction_score_mean\n",
      "  region_is_premium_mean\n",
      "  income_vs_region_mean\n",
      "  satisfaction_vs_region_mean\n",
      "  education_High School\n",
      "  education_Master\n",
      "  education_PhD\n",
      "  region_North\n",
      "  ... and 8 more\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.288633Z",
     "start_time": "2025-09-11T05:57:23.737753Z"
    }
   },
   "source": [
    "# Label encoding (for ordinal variables)\n",
    "print(\"Label Encoding:\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy for label encoding\n",
    "df_label_encoded = df_clean.copy()\n",
    "\n",
    "# Education has natural ordering\n",
    "education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_label_encoded['education_encoded'] = df_label_encoded['education'].map(education_order)\n",
    "\n",
    "# For non-ordinal categories, use LabelEncoder\n",
    "le_region = LabelEncoder()\n",
    "df_label_encoded['region_encoded'] = le_region.fit_transform(df_label_encoded['region'])\n",
    "\n",
    "print(\"Education encoding:\")\n",
    "print(df_label_encoded[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "print(\"\\nRegion encoding:\")\n",
    "print(df_label_encoded[['region', 'region_encoded']].drop_duplicates().sort_values('region_encoded'))\n",
    "\n",
    "# Show encoding mapping\n",
    "print(\"\\nRegion encoding mapping:\")\n",
    "for i, region in enumerate(le_region.classes_):\n",
    "    print(f\"  {region}: {i}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding:\n",
      "Education encoding:\n",
      "     education education_encoded\n",
      "0     Bachelor                 1\n",
      "1  High School                 0\n",
      "4       Master                 2\n",
      "5          PhD                 3\n",
      "\n",
      "Region encoding:\n",
      "  region  region_encoded\n",
      "0   East               0\n",
      "2  North               1\n",
      "6  South               2\n",
      "9   West               3\n",
      "\n",
      "Region encoding mapping:\n",
      "  East: 0\n",
      "  North: 1\n",
      "  South: 2\n",
      "  West: 3\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.297140Z",
     "start_time": "2025-09-11T05:57:24.291607Z"
    }
   },
   "source": [
    "# Target encoding (advanced technique)\n",
    "print(\"Target Encoding:\")\n",
    "\n",
    "# Calculate mean target value for each category\n",
    "def target_encode(df, categorical_col, target_col, smoothing=1):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing to prevent overfitting\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = df[target_col].mean()\n",
    "\n",
    "    # Calculate category means and counts\n",
    "    category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
    "\n",
    "    # Apply smoothing\n",
    "    smoothed_means = (\n",
    "        (category_stats['mean'] * category_stats['count'] + global_mean * smoothing) /\n",
    "        (category_stats['count'] + smoothing)\n",
    "    )\n",
    "\n",
    "    return smoothed_means\n",
    "\n",
    "# Target encode education based on premium rate\n",
    "education_target_encoding = target_encode(df_clean, 'education', 'is_premium')\n",
    "df_clean['education_target_encoded'] = df_clean['education'].map(education_target_encoding)\n",
    "\n",
    "print(\"Education target encoding (premium rate):\")\n",
    "print(education_target_encoding.sort_values(ascending=False))\n",
    "\n",
    "# Target encode region\n",
    "region_target_encoding = target_encode(df_clean, 'region', 'is_premium')\n",
    "df_clean['region_target_encoded'] = df_clean['region'].map(region_target_encoding)\n",
    "\n",
    "print(\"\\nRegion target encoding (premium rate):\")\n",
    "print(region_target_encoding.sort_values(ascending=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding:\n",
      "Education target encoding (premium rate):\n",
      "education\n",
      "Master         0.320448\n",
      "Bachelor       0.293196\n",
      "PhD            0.287305\n",
      "High School    0.279411\n",
      "dtype: float64\n",
      "\n",
      "Region target encoding (premium rate):\n",
      "region\n",
      "South    0.317257\n",
      "West     0.303808\n",
      "North    0.280907\n",
      "East     0.269336\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_50712/2758571233.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_50712/2758571233.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Normalization\n",
    "\n",
    "Preparing numerical features for ML algorithms that are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.326245Z",
     "start_time": "2025-09-11T05:57:24.307473Z"
    }
   },
   "source": [
    "# Feature scaling\n",
    "print(\"Feature Scaling:\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['age', 'income_capped', 'experience_years', 'satisfaction_score',\n",
    "                     'log_income', 'days_since_signup']\n",
    "\n",
    "print(\"Original feature statistics:\")\n",
    "print(df_clean[numerical_features].describe())\n",
    "\n",
    "# Standard scaling (z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard_scaled = df_clean.copy()\n",
    "df_standard_scaled[numerical_features] = scaler_standard.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
    "print(df_standard_scaled[numerical_features].describe())\n",
    "\n",
    "# Min-Max scaling (0-1 range)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax_scaled = df_clean.copy()\n",
    "df_minmax_scaled[numerical_features] = scaler_minmax.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Min-Max Scaling (range 0-1):\")\n",
    "print(df_minmax_scaled[numerical_features].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scaling:\n",
      "Original feature statistics:\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000         1000.000000   \n",
      "mean     34.743000   25075.212815          4.712564            2.998455   \n",
      "std      11.748233   11361.041618          4.769008            1.106650   \n",
      "min      -3.000000    5063.461821          0.000154            1.000211   \n",
      "25%      27.000000   16372.665420          1.354730            2.094302   \n",
      "50%      35.000000   22732.225225          3.295244            2.998455   \n",
      "75%      42.000000   31391.594681          6.413300            3.903685   \n",
      "max      81.000000   50838.771470         38.617649            4.994469   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000        1000.000000  \n",
      "mean     10.024256        1579.500000  \n",
      "std       0.471203         288.819436  \n",
      "min       8.530003        1080.000000  \n",
      "25%       9.703429        1329.750000  \n",
      "50%      10.031583        1579.500000  \n",
      "75%      10.354327        1829.250000  \n",
      "max      10.836434        2079.000000  \n",
      "\n",
      "After Standard Scaling (mean=0, std=1):\n",
      "                age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1.000000e+03   1.000000e+03      1.000000e+03        1.000000e+03   \n",
      "mean  -1.687539e-16  -3.552714e-18      2.486900e-17        1.421085e-17   \n",
      "std    1.000500e+00   1.000500e+00      1.000500e+00        1.000500e+00   \n",
      "min   -3.214261e+00  -1.762318e+00     -9.886267e-01       -1.806573e+00   \n",
      "25%   -6.594076e-01  -7.663824e-01     -7.044473e-01       -8.174271e-01   \n",
      "50%    2.188658e-02  -2.063332e-01     -2.973428e-01        1.110080e-08   \n",
      "75%    6.180190e-01   5.562467e-01      3.568009e-01        8.184002e-01   \n",
      "max    3.939328e+00   2.268846e+00      7.113020e+00        1.804557e+00   \n",
      "\n",
      "         log_income  days_since_signup  \n",
      "count  1.000000e+03         1000.00000  \n",
      "mean   3.417711e-15            0.00000  \n",
      "std    1.000500e+00            1.00050  \n",
      "min   -3.172729e+00           -1.73032  \n",
      "25%   -6.812079e-01           -0.86516  \n",
      "50%    1.555627e-02            0.00000  \n",
      "75%    7.008356e-01            0.86516  \n",
      "max    1.724487e+00            1.73032  \n",
      "\n",
      "After Min-Max Scaling (range 0-1):\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000         1000.000000   \n",
      "mean      0.449321       0.437173          0.122028            0.500279   \n",
      "std       0.139860       0.248191          0.123493            0.277060   \n",
      "min       0.000000       0.000000          0.000000            0.000000   \n",
      "25%       0.357143       0.247059          0.035077            0.273916   \n",
      "50%       0.452381       0.385989          0.085326            0.500279   \n",
      "75%       0.535714       0.575160          0.166068            0.726912   \n",
      "max       1.000000       1.000000          1.000000            1.000000   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000        1000.000000  \n",
      "mean      0.647864           0.500000  \n",
      "std       0.204300           0.289109  \n",
      "min       0.000000           0.000000  \n",
      "25%       0.508763           0.250000  \n",
      "50%       0.651040           0.500000  \n",
      "75%       0.790973           0.750000  \n",
      "max       1.000000           1.000000  \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.346729Z",
     "start_time": "2025-09-11T05:57:24.335082Z"
    }
   },
   "source": [
    "# Robust scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust_scaled = df_clean.copy()\n",
    "df_robust_scaled[numerical_features] = scaler_robust.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"After Robust Scaling (median=0, IQR=1):\")\n",
    "print(df_robust_scaled[numerical_features].describe())\n",
    "\n",
    "# Compare scaling methods visually\n",
    "print(\"\\nScaling Comparison for 'income_capped':\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df_clean['income_capped'],\n",
    "    'Standard': df_standard_scaled['income_capped'],\n",
    "    'MinMax': df_minmax_scaled['income_capped'],\n",
    "    'Robust': df_robust_scaled['income_capped']\n",
    "})\n",
    "\n",
    "print(comparison_df.describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Robust Scaling (median=0, IQR=1):\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000        1.000000e+03   \n",
      "mean     -0.017133       0.156002          0.280182       -6.786049e-09   \n",
      "std       0.783216       0.756448          0.942758        6.116174e-01   \n",
      "min      -2.533333      -1.176433         -0.651388       -1.104379e+00   \n",
      "25%      -0.533333      -0.423436         -0.383609       -4.997026e-01   \n",
      "50%       0.000000       0.000000          0.000000        0.000000e+00   \n",
      "75%       0.466667       0.576564          0.616391        5.002974e-01   \n",
      "max       3.066667       1.871408          6.982686        1.103147e+00   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000       1.000000e+03  \n",
      "mean     -0.011256      -2.842171e-17  \n",
      "std       0.723928       5.782171e-01  \n",
      "min      -2.306936      -1.000000e+00  \n",
      "25%      -0.504155      -5.000000e-01  \n",
      "50%       0.000000       0.000000e+00  \n",
      "75%       0.495845       5.000000e-01  \n",
      "max       1.236525       1.000000e+00  \n",
      "\n",
      "Scaling Comparison for 'income_capped':\n",
      "           Original      Standard       MinMax       Robust\n",
      "count   1000.000000  1.000000e+03  1000.000000  1000.000000\n",
      "mean   25075.212815 -3.552714e-18     0.437173     0.156002\n",
      "std    11361.041618  1.000500e+00     0.248191     0.756448\n",
      "min     5063.461821 -1.762318e+00     0.000000    -1.176433\n",
      "25%    16372.665420 -7.663824e-01     0.247059    -0.423436\n",
      "50%    22732.225225 -2.063332e-01     0.385989     0.000000\n",
      "75%    31391.594681  5.562467e-01     0.575160     0.576564\n",
      "max    50838.771470  2.268846e+00     1.000000     1.871408\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting and Sampling\n",
    "\n",
    "Preparing data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.427607Z",
     "start_time": "2025-09-11T05:57:24.356531Z"
    }
   },
   "source": [
    "# Train-validation-test split\n",
    "print(\"Data Splitting:\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df_encoded.columns\n",
    "                  if col not in ['customer_id', 'is_premium', 'signup_date', 'education', 'region']]\n",
    "\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['is_premium']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation (80% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Validation: {X_val.shape[0]} ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test: {X_test.shape[0]} ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(f\"Train: {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test: {y_test.mean():.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splitting:\n",
      "Features shape: (1000, 35)\n",
      "Target shape: (1000,)\n",
      "Target distribution: {0: 706, 1: 294}\n",
      "\n",
      "Split sizes:\n",
      "Train: 600 (60.0%)\n",
      "Validation: 200 (20.0%)\n",
      "Test: 200 (20.0%)\n",
      "\n",
      "Target distribution:\n",
      "Train: 0.293\n",
      "Validation: 0.295\n",
      "Test: 0.295\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.446305Z",
     "start_time": "2025-09-11T05:57:24.440209Z"
    }
   },
   "source": [
    "# Handling imbalanced data\n",
    "print(\"Handling Imbalanced Data:\")\n",
    "\n",
    "# Check class imbalance\n",
    "class_counts = y_train.value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Class distribution: {class_counts.to_dict()}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 2:  # If significantly imbalanced\n",
    "    print(\"\\nDataset is imbalanced. Strategies to consider:\")\n",
    "\n",
    "    # 1. Undersampling majority class\n",
    "    majority_class = y_train.value_counts().index[0]\n",
    "    minority_class = y_train.value_counts().index[1]\n",
    "\n",
    "    majority_indices = y_train[y_train == majority_class].index\n",
    "    minority_indices = y_train[y_train == minority_class].index\n",
    "\n",
    "    # Random undersample majority class\n",
    "    undersampled_majority = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "    balanced_indices = np.concatenate([undersampled_majority, minority_indices])\n",
    "\n",
    "    X_train_balanced = X_train.loc[balanced_indices]\n",
    "    y_train_balanced = y_train.loc[balanced_indices]\n",
    "\n",
    "    print(f\"1. Undersampling - New size: {len(X_train_balanced)}\")\n",
    "    print(f\"   New distribution: {y_train_balanced.value_counts().to_dict()}\")\n",
    "\n",
    "    # 2. Class weights (for algorithms that support it)\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights, strict=False))\n",
    "\n",
    "    print(f\"2. Class weights: {class_weight_dict}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset is reasonably balanced.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Imbalanced Data:\n",
      "Class distribution: {0: 424, 1: 176}\n",
      "Imbalance ratio: 2.41:1\n",
      "\n",
      "Dataset is imbalanced. Strategies to consider:\n",
      "1. Undersampling - New size: 352\n",
      "   New distribution: {0: 176, 1: 176}\n",
      "2. Class weights: {np.int8(0): np.float64(0.7075471698113207), np.int8(1): np.float64(1.7045454545454546)}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Converting cleaned data for other tools (framework-neutral)\n",
    "\n",
    "After cleaning and encoding, you will often convert DataFrame features to NumPy arrays. The guidance below is framework-neutral and focuses on checks and formats most tools expect.\n",
    "\n",
    "- Use numeric dtypes for features (float32 is common for inputs)\n",
    "- Use integer dtypes for labels (int32/int64 depending on the tool)\n",
    "- Verify shapes: (n_samples, n_features) for feature matrices\n",
    "- Check for NaNs and infinite values before exporting\n",
    "\n",
    "Example: convert train/val/test splits to NumPy and save them for later reuse."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.461688Z",
     "start_time": "2025-09-11T05:57:24.456749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Converting DataFrame to NumPy arrays (neutral):\")\n",
    "\n",
    "# Convert only numeric columns to NumPy arrays (ensure appropriate dtype)\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_train_np = X_train[numeric_columns].values.astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.int64)\n",
    "\n",
    "X_val_np = X_val[numeric_columns].values.astype(np.float32)\n",
    "y_val_np = y_val.values.astype(np.int64)\n",
    "\n",
    "X_test_np = X_test[numeric_columns].values.astype(np.float32)\n",
    "y_test_np = y_test.values.astype(np.int64)\n",
    "\n",
    "print(f\"Training data shape: {X_train_np.shape}\")\n",
    "print(f\"Training data dtype: {X_train_np.dtype}\")\n",
    "print(f\"Training labels dtype: {y_train_np.dtype}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\" - Save these NumPy arrays to disk for reproducibility and to reuse in other tools\")\n",
    "print(\" - Save preprocessing objects (scalers, encoders) so the same transforms are applied in production\")\n",
    "\n",
    "# Example saving (commented out so the notebook can run without writing files)\n",
    "# np.save('data/processed/X_train.npy', X_train_np)\n",
    "# np.save('data/processed/y_train.npy', y_train_np)\n",
    "# np.save('data/processed/X_val.npy', X_val_np)\n",
    "# np.save('data/processed/y_val.npy', y_val_np)\n",
    "# np.save('data/processed/X_test.npy', X_test_np)\n",
    "# np.save('data/processed/y_test.npy', y_test_np)\n",
    "\n",
    "# Save feature names and preprocessing info\n",
    "# import pickle\n",
    "# with open('data/processed/preprocessing_info.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessing_summary, f)\n",
    "\n",
    "# Save scalers for future use\n",
    "# with open('data/processed/scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler_standard, f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DataFrame to NumPy arrays (neutral):\n",
      "Training data shape: (600, 23)\n",
      "Training data dtype: float32\n",
      "Training labels dtype: int64\n",
      "\n",
      "Notes:\n",
      " - Save these NumPy arrays to disk for reproducibility and to reuse in other tools\n",
      " - Save preprocessing objects (scalers, encoders) so the same transforms are applied in production\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways (updated)\n",
    "\n",
    "**What we've learned (short):**\n",
    "- Explore data with .info(), .describe(), and .value_counts()\n",
    "- Handle missing values and outliers with straightforward strategies\n",
    "- Create new features and encode categorical variables\n",
    "- Scale numeric features and split data into train/val/test\n",
    "- Convert cleaned DataFrames to NumPy arrays for downstream tools\n",
    "\n",
    "**Next steps:** keep the preprocessing code and scalers so you can reproduce results later.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedaa3de",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to PyTorch\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Course:** Intro to Machine Learning / Deep Learning  \n",
    "**Last updated:** 2025-10-30\n",
    "\n",
    "Welcome! This notebook is a hands-on, beginner-friendly introduction to **PyTorch**, a popular deep learning framework with an imperative (define-by-run) style. We'll cover:\n",
    "\n",
    "- Tensors and basic tensor operations\n",
    "- Automatic differentiation with `autograd`\n",
    "- Building neural networks with `torch.nn` and `torch.optim`\n",
    "- Datasets, transforms, and `DataLoader`\n",
    "- Training and evaluating a simple CNN on **MNIST**\n",
    "\n",
    "> **Prereqs:** Basic Python & NumPy, and a high-level understanding of linear algebra and neural nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715cebc",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "1. Create and manipulate tensors, move them across CPU/GPU, and understand broadcasting.\n",
    "2. Use PyTorch's **autograd** to compute gradients and perform a simple gradient descent step.\n",
    "3. Define models with `nn.Module` and configure an optimizer and loss function.\n",
    "4. Use `torchvision.datasets` and `DataLoader` to feed data into your model.\n",
    "5. Train, evaluate, save, and load a PyTorch model on the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25fd3c",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment Setup\n",
    "\n",
    "If you don't already have PyTorch and TorchVision installed, uncomment and run the following cell.  \n",
    "Pick the correct command for your platform from https://pytorch.org/get-started/locally/ if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523adce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --upgrade pip\n",
    "# CPU-only (works on most machines):\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# GPU (CUDA): follow instructions from https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad48238",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports and Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca247ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeeebf4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Tensors\n",
    "\n",
    "PyTorch's core data structure is the **Tensor**: an n-dimensional array similar to NumPy's `ndarray`, but with GPU support and automatic differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0330b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creation\n",
    "a = torch.tensor([1.0, 2.0, 3.0])              # from Python list\n",
    "b = torch.zeros((2,3))                          # zeros\n",
    "c = torch.ones((2,3), dtype=torch.float32)      # ones with dtype\n",
    "d = torch.randn((2,3))                          # from normal distribution\n",
    "e = torch.arange(0, 12).reshape(3,4)            # range + reshape\n",
    "\n",
    "print('a:', a)\n",
    "print('b.shape:', b.shape, 'dtype:', b.dtype)\n",
    "print('c.device:', c.device)\n",
    "print('d mean/std:', d.mean().item(), d.std().item())\n",
    "print('e:\\n', e)\n",
    "\n",
    "# NumPy <-> Torch\n",
    "np_arr = np.array([[1,2,3],[4,5,6]], dtype=np.float32)\n",
    "t_from_np = torch.from_numpy(np_arr)    # shares memory!\n",
    "np_back = t_from_np.numpy()\n",
    "print('Shared memory check:', np.shares_memory(np_arr, np_back))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Indexing / slicing / broadcasting\n",
    "x = torch.arange(0, 12).reshape(3,4).float()\n",
    "print('x:\\n', x)\n",
    "print('x[0, 1:3]:', x[0, 1:3])\n",
    "\n",
    "row = x[0]             # view (no copy)\n",
    "row_clone = row.clone()\n",
    "row[:] = -1            # in-place change propagates (be careful!)\n",
    "print('After in-place change, x:\\n', x)\n",
    "\n",
    "# Broadcasting: elementwise add a (3x1) vector to (3x4) matrix\n",
    "v = torch.tensor([[10.0],[20.0],[30.0]])  # shape (3,1)\n",
    "y = x + v                                 # broadcast along columns\n",
    "print('Broadcasted y shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Moving data between devices\n",
    "x_cpu = torch.randn(2, 2)\n",
    "x_gpu = x_cpu.to(device)\n",
    "print('Device of x_gpu:', x_gpu.device)\n",
    "\n",
    "# Best practice: create tensors directly on device when appropriate\n",
    "x_on_dev = torch.randn(1024, 1024, device=device)\n",
    "x_on_dev = x_on_dev @ x_on_dev.T  # matrix multiply\n",
    "x_on_dev.mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9b1f3",
   "metadata": {},
   "source": [
    "\n",
    "### âœï¸ Quick Exercise\n",
    "\n",
    "- Create a tensor `z` of shape `(4, 5)` filled with random numbers.\n",
    "- Normalize each column to have mean 0 and std 1. (Use broadcasting.)\n",
    "- Verify your result with `z.mean(dim=0)` and `z.std(dim=0)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e56372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Your code here\n",
    "# z = ...\n",
    "# z_norm = ...\n",
    "# print(z_norm.mean(dim=0), z_norm.std(dim=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861c684",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Automatic Differentiation with `autograd`\n",
    "\n",
    "`torch.autograd` tracks operations on tensors with `requires_grad=True` and can compute gradients via **backpropagation** using `.backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95411cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A simple scalar function: f(w) = (w - 3)^2\n",
    "w = torch.tensor([5.0], requires_grad=True)\n",
    "f = (w - 3) ** 2\n",
    "f.backward()                  # df/dw = 2*(w - 3) at w=5\n",
    "print('w.grad:', w.grad)      # expected 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient Descent on a quadratic: minimize f(w) = (w - 7)^2\n",
    "w = torch.tensor([0.0], requires_grad=True)\n",
    "lr = 0.1\n",
    "for step in range(30):\n",
    "    loss = (w - 7)**2\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad      # update\n",
    "        w.grad.zero_()\n",
    "    if step % 5 == 0:\n",
    "        print(f'step={step:02d}  w={w.item():.4f}  loss={loss.item():.6f}')\n",
    "print('Converged w â‰ˆ', w.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88500b5",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** Autograd builds a dynamic computation graph on-the-fly. Use `with torch.no_grad():` to disable gradient tracking during evaluation or parameter updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a3e23",
   "metadata": {},
   "source": [
    "\n",
    "## 4. `nn.Module` and `torch.optim`\n",
    "\n",
    "Let's define a tiny **Multi-Layer Perceptron (MLP)** and a training step loop skeleton.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_features=784, hidden=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 1, 28, 28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "model = TinyMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "sum(p.numel() for p in model.parameters()), model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb282bf7",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Datasets, Transforms, and DataLoader (MNIST)\n",
    "\n",
    "We'll use `torchvision.datasets.MNIST`. If your environment has no internet access, download the dataset beforehand or change the root path to a cached location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # [0,1] tensor of shape (C,H,W)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST normalization\n",
    "])\n",
    "\n",
    "data_root = './data'  # change this if needed\n",
    "\n",
    "# Dataset & split\n",
    "train_full = datasets.MNIST(root=data_root, train=True, download=True, transform=transform)\n",
    "test_ds    = datasets.MNIST(root=data_root, train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_full))\n",
    "val_size   = len(train_full) - train_size\n",
    "train_ds, val_ds = random_split(train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ed2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Peek at some samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print('Batch shape:', images.shape, 'Labels shape:', labels.shape)\n",
    "\n",
    "grid = images[:16].squeeze(1)  # (16, 28, 28)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(6,6))\n",
    "for ax, img, lab in zip(axes.flatten(), grid, labels[:16]):\n",
    "    ax.imshow(img.numpy(), cmap='gray')\n",
    "    ax.set_title(int(lab))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c99d06",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Training Utilities\n",
    "\n",
    "We'll write small helper functions for one epoch of training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fde0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed8d0d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Train a Simple MLP (Baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661dbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 5\n",
    "best_val_acc = 0.0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\"model\": model.state_dict()}, \"mnist_mlp_best.pt\")\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Epoch {epoch:02d} | {dt:.1f}s  train: loss={tr_loss:.4f}, acc={tr_acc:.4f}  \"\n",
    "          f\"val: loss={val_loss:.4f}, acc={val_acc:.4f}\")\n",
    "\n",
    "print(\"Best val acc:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss Curves\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade725ab",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Upgrade to a Small CNN\n",
    "\n",
    "A small convolutional network (LeNet-style) typically performs better on MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "cnn = SmallCNN().to(device)\n",
    "optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS_CNN = 5\n",
    "best_val_acc_cnn = 0.0\n",
    "for epoch in range(1, EPOCHS_CNN+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(cnn, train_loader, optimizer_cnn, criterion, device)\n",
    "    val_loss, val_acc = evaluate(cnn, val_loader, criterion, device)\n",
    "    if val_acc > best_val_acc_cnn:\n",
    "        best_val_acc_cnn = val_acc\n",
    "        torch.save({\"model\": cnn.state_dict()}, \"mnist_cnn_best.pt\")\n",
    "    print(f\"[CNN] Epoch {epoch:02d}  train: loss={tr_loss:.4f}, acc={tr_acc:.4f}  \"\n",
    "          f\"val: loss={val_loss:.4f}, acc={val_acc:.4f}\")\n",
    "print(\"Best val acc (CNN):\", best_val_acc_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de69b7",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Evaluate on Test Set and Inspect Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds.append(logits.argmax(dim=1).cpu())\n",
    "        targets.append(y)\n",
    "    return torch.cat(preds), torch.cat(targets)\n",
    "\n",
    "test_loss_mlp, test_acc_mlp = evaluate(model, test_loader, criterion, device)\n",
    "test_loss_cnn, test_acc_cnn = evaluate(cnn,   test_loader, criterion, device)\n",
    "print(f\"MLP Test: loss={test_loss_mlp:.4f}, acc={test_acc_mlp:.4f}\")\n",
    "print(f\"CNN Test: loss={test_loss_cnn:.4f}, acc={test_acc_cnn:.4f}\")\n",
    "\n",
    "preds, targets = predict(cnn, test_loader, device)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(targets.numpy(), preds.numpy())\n",
    "print(classification_report(targets.numpy(), preds.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06285328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix (CNN)')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d690c1",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Saving and Loading Models\n",
    "\n",
    "Use `torch.save` / `torch.load` with state dicts for portability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save\n",
    "torch.save({'model': cnn.state_dict()}, 'mnist_cnn_checkpoint.pt')\n",
    "\n",
    "# Load\n",
    "state = torch.load('mnist_cnn_checkpoint.pt', map_location=device)\n",
    "cnn_loaded = SmallCNN().to(device)\n",
    "cnn_loaded.load_state_dict(state['model'])\n",
    "_ = cnn_loaded.eval()\n",
    "print('Model reloaded and set to eval mode.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84593c6",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Tips, Tricks, and Next Steps\n",
    "\n",
    "- **`model.train()` vs `model.eval()`**: Affects layers like `Dropout` and `BatchNorm`.\n",
    "- **Mixed Precision**: `torch.cuda.amp.autocast()` can speed up training on GPUs.\n",
    "- **LR Scheduling**: Explore `torch.optim.lr_scheduler` for better training dynamics.\n",
    "- **Hooks**: Use forward/backward hooks for introspection.\n",
    "- **Debugging**: Start with a tiny subset of data and make sure the model can overfit it.\n",
    "- **Reading**: The official tutorials and docs are excellent, and the MNIST example is a good baseline for new architectures.\n",
    "\n",
    "### ðŸ”§ Stretch Exercises\n",
    "1. Add **BatchNorm** to the CNN and compare validation accuracy.\n",
    "2. Replace `ReLU` with **LeakyReLU** or **GELU** and observe changes.\n",
    "3. Implement **early stopping** and/or a **ReduceLROnPlateau** scheduler.\n",
    "4. Visualize learned filters in the first conv layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e18aa2",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: Troubleshooting\n",
    "\n",
    "- **Dataset download issues**: If your environment blocks downloads, pre-download MNIST and set `root` to that folder, or copy from a machine with internet.\n",
    "- **CUDA errors**: Ensure correct CUDA/PyTorch versions. Try CPU-only wheels if necessary.\n",
    "- **Performance**: Increase `num_workers`, enable `pin_memory=True` for faster host->device transfers, and switch to mixed precision on GPUs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

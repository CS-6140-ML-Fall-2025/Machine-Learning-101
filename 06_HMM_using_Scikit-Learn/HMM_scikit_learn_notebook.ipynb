{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c0259c",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM) — An approachable notebook\n",
    "\n",
    "**Target audience:** Intermediate students in Machine Learning familiar with Python, NumPy, and scikit-learn API patterns.\n",
    "\n",
    "**Goal:** teach HMM concepts, implement and fit HMMs using the `hmmlearn` library (sklearn-like API), explore a dataset (synthetic but realistic), evaluate models, perform decoding (Viterbi), and integrate HMMs into common model-selection workflows.\n",
    "\n",
    "**Notebook style:** explanatory text + runnable examples + visualization and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea62dc2",
   "metadata": {},
   "source": [
    "## 1 — Setup: libraries and environment\n",
    "\n",
    "This notebook uses `hmmlearn` (a commonly used HMM package with a scikit-learn-like API). If you don't have it installed, run `pip install hmmlearn` in the environment where you run the notebook.\n",
    "\n",
    "We'll also use standard packages: `numpy`, `pandas`, `matplotlib`, and `scikit-learn` for metrics and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard imports\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# HMM package (sklearn-like)\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "except Exception as e:\n",
    "    raise ImportError(\"hmmlearn is required. Install with: pip install hmmlearn\") from e\n",
    "\n",
    "# Reproducibility\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "\n",
    "print('numpy:', np.__version__)\n",
    "print('pandas:', pd.__version__)\n",
    "print('hmmlearn:', getattr(hmm, '__version__', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eae450",
   "metadata": {},
   "source": [
    "## 2 — Quick theory recap (brief)\n",
    "\n",
    "- **Hidden Markov Model (HMM)**: a generative probabilistic model for sequential data. The model contains:\n",
    "  - A discrete hidden state sequence \\(S_t\\) (e.g., weather: {sunny, rainy}).\n",
    "  - Observations \\(X_t\\) generated from an emission distribution conditioned on the current hidden state.\n",
    "  - A transition matrix \\(A\\) that governs \\(P(S_t | S_{t-1})\\).\n",
    "  - An initial state distribution \\(\\pi\\).\n",
    "\n",
    "Common tasks:\n",
    "1. **Likelihood estimation** (fit parameters given observed sequences).\n",
    "2. **Decoding** (find the most likely hidden state sequence given observations, Viterbi).\n",
    "3. **Scoring / model selection** (log-likelihood, AIC/BIC).\n",
    "\n",
    "This notebook uses **Gaussian emissions** (continuous observations) for clarity and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7c586",
   "metadata": {},
   "source": [
    "## 3 — Construct a synthetic dataset (known ground truth)\n",
    "\n",
    "We build a 3-state HMM with 2D Gaussian emissions. We'll sample many short sequences to simulate realistic time-series data. We keep the ground-truth transition and emission parameters so we can evaluate how well the model recovers them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68082284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ground-truth HMM parameters (3 hidden states)\n",
    "\"\"\"\n",
    "\n",
    "n_states = 3\n",
    "n_dim = 2  # 2D continuous observations so we can plot them easily\n",
    "\n",
    "# Transition matrix (rows sum to 1)\n",
    "trans_mat = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.3, 0.5]\n",
    "])\n",
    "\n",
    "# Start probabilities\n",
    "start_prob = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Emission parameters: Gaussian means and covariances for each state\n",
    "means = np.array([[0.0, 0.0],\n",
    "                  [4.0, 2.0],\n",
    "                  [-3.0, 4.0]])\n",
    "\n",
    "covars = np.array([[[0.5, 0.05],[0.05, 0.4]],\n",
    "                   [[0.7, -0.02],[-0.02, 0.6]],\n",
    "                   [[0.6, 0.0],[0.0, 0.5]]])\n",
    "\n",
    "# Small function to sample sequences from this HMM\n",
    "def sample_hmm(trans_mat, start_prob, means, covars, n_sequences=200, seq_len=50, random_state=None):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    sequences = []      # list of arrays, each sequence\n",
    "    lengths = []        # lengths of each sequence\n",
    "    state_seqs = []     # true hidden states for each sequence (for evaluation)\n",
    "    for _ in range(n_sequences):\n",
    "        states = []\n",
    "        obs = []\n",
    "        # initial state\n",
    "        s = rng.choice(len(start_prob), p=start_prob)\n",
    "        for t in range(seq_len):\n",
    "            states.append(s)\n",
    "            obs.append(rng.multivariate_normal(means[s], covars[s]))\n",
    "            s = rng.choice(len(start_prob), p=trans_mat[s])\n",
    "        sequences.append(np.vstack(obs))\n",
    "        lengths.append(seq_len)\n",
    "        state_seqs.append(np.array(states))\n",
    "    return sequences, lengths, state_seqs\n",
    "\n",
    "# Create dataset\n",
    "n_sequences = 300\n",
    "seq_len = 40\n",
    "sequences, lengths, state_seqs = sample_hmm(trans_mat, start_prob, means, covars,\n",
    "                                            n_sequences=n_sequences, seq_len=seq_len, random_state=RND)\n",
    "\n",
    "# Concatenate into a single long observation array (hmmlearn likes this for multiple sequences)\n",
    "X = np.vstack(sequences)\n",
    "lengths = lengths  # list of lengths\n",
    "y_true = np.hstack(state_seqs)  # flattened true states for every timestep\n",
    "print('Total timesteps:', X.shape[0])\n",
    "print('Number of sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355db37b",
   "metadata": {},
   "source": [
    "### 3.1 — Dive into the dataset\n",
    "\n",
    "Let's look at the observations, a scatter plot colored by true (hidden) state, and some quick statistics (mean, covariance per state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic stats and visualization\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Scatter plot of observations colored by true hidden state (using flattened arrays)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "colors = ['C0','C1','C2']\n",
    "for s in range(n_states):\n",
    "    mask = (y_true == s)\n",
    "    ax.scatter(X[mask,0], X[mask,1], s=8, label=f'state {s}', alpha=0.6)\n",
    "ax.set_title('Observations colored by true hidden state (synthetic)')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute empirical means / covariances for each known state (sanity check)\n",
    "emp_stats = []\n",
    "for s in range(n_states):\n",
    "    xs = X[y_true==s]\n",
    "    emp_stats.append((xs.mean(axis=0), np.cov(xs.T)))\n",
    "    print(f\"State {s}: mean = {emp_stats[-1][0]}, cov shape = {emp_stats[-1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b962ba",
   "metadata": {},
   "source": [
    "## 4 — Fit an HMM (Gaussian emissions) using `hmmlearn`\n",
    "\n",
    "`hmmlearn`'s `GaussianHMM` provides a fit / predict API similar to scikit-learn models. We'll:\n",
    "\n",
    "1. Fit a model with the correct number of states.\n",
    "2. Decode the most likely hidden state sequence (Viterbi).\n",
    "3. Compare learnt parameters with ground-truth.\n",
    "\n",
    "Notes: `hmmlearn` expects concatenated observations and a list of sequence lengths for fitting multiple sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit a GaussianHMM\n",
    "\"\"\"\n",
    "\n",
    "model = hmm.GaussianHMM(n_components=n_states, covariance_type='full', n_iter=200, random_state=RND, verbose=False)\n",
    "model = model.fit(X, lengths)\n",
    "\n",
    "print('Estimated startprob:', model.startprob_)\n",
    "print('Estimated transmat:\\n', model.transmat_)\n",
    "print('Estimated means:\\n', model.means_)\n",
    "print('Estimated covariances shapes:', [c.shape for c in model.covars_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab9ebd",
   "metadata": {},
   "source": [
    "### 4.1 — Decoding (Viterbi) and evaluation\n",
    "\n",
    "We'll use Viterbi to get the most likely hidden state sequence for the entire concatenated observation series and then compute simple metrics against the ground truth. Keep in mind: HMM hidden states are *unlabeled* — the numbering is arbitrary. We'll find the best mapping from predicted states to true states using the Hungarian algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Viterbi decode (predict hidden states)\n",
    "\"\"\"\n",
    "\n",
    "# Predict hidden states for the whole dataset\n",
    "y_pred = model.predict(X, lengths)\n",
    "\n",
    "# Because states are unlabeled, compute mapping via Hungarian matching on confusion matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# We want to maximize correct matches -> minimize negative of matrix for Hungarian algorithm\n",
    "row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "mapping = dict(zip(col_ind, row_ind))\n",
    "\n",
    "# Map predicted states to true labels where possible\n",
    "y_pred_mapped = np.vectorize(lambda s: mapping.get(s, s))(y_pred)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred_mapped)\n",
    "print(f'Accuracy after optimal mapping: {acc:.4f}')\n",
    "\n",
    "# Show confusion matrix (mapped)\n",
    "cm_mapped = confusion_matrix(y_true, y_pred_mapped)\n",
    "print('Confusion matrix (true rows, predicted columns after mapping):\\n', cm_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8689d6",
   "metadata": {},
   "source": [
    "### 4.2 — Visualize decoding on an example sequence\n",
    "\n",
    "Plot true vs decoded states for one example sequence (time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot one sequence's observations and true vs predicted states\n",
    "\"\"\"\n",
    "\n",
    "# Choose a sequence to visualize\n",
    "seq_idx = 5\n",
    "obs = sequences[seq_idx]\n",
    "true_states = state_seqs[seq_idx]\n",
    "\n",
    "# Predict for this specific sequence using model.predict on the slice\n",
    "start = seq_idx * seq_len\n",
    "end = start + seq_len\n",
    "pred_seq = y_pred_mapped[start:end]\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
    "ax[0].plot(obs[:,0], label='obs dim 0')\n",
    "ax[0].plot(obs[:,1], label='obs dim 1')\n",
    "ax[0].set_title('Observations (sequence {})'.format(seq_idx))\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].step(range(seq_len), true_states, where='mid', label='true state')\n",
    "ax[1].step(range(seq_len), pred_seq, where='mid', label='predicted state (mapped)')\n",
    "ax[1].set_title('True vs Predicted hidden states (sequence {})'.format(seq_idx))\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b6256",
   "metadata": {},
   "source": [
    "### 4.3 — Compare learned parameters with ground truth\n",
    "\n",
    "Parameter recovery is not guaranteed (identifiability issues, limited data), but we can eyeball how similar the estimated means and transition matrix are to the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc974e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ground-truth means:\\n', means)\n",
    "print('\\nEstimated means (order corresponds to model.states):\\n', model.means_)\n",
    "\n",
    "print('\\nGround-truth transmat:\\n', trans_mat)\n",
    "print('\\nEstimated transmat:\\n', model.transmat_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7af05",
   "metadata": {},
   "source": [
    "## 5 — Model selection: how many hidden states?\n",
    "\n",
    "Common model selection strategies:\n",
    "- Compare models with different `n_components` using held-out log-likelihood, AIC, or BIC.\n",
    "\n",
    "We'll compute BIC and AIC for a range of components and show selection. Note: exact formulas depend on the number of free parameters; for HMMs, compute carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper to compute number of free parameters for GaussianHMM (approximate)\n",
    "\"\"\"\n",
    "\n",
    "def n_hmm_params(n_components, n_features, covariance_type='full'):\n",
    "    # start probabilities: n_components - 1 free parameters (sum-to-one)\n",
    "    startp = n_components - 1\n",
    "    # transition matrix: n_components*(n_components-1) free params (each row sums to 1)\n",
    "    trans = n_components * (n_components - 1)\n",
    "    # Emission parameters: means (n_components * n_features) and covariances\n",
    "    if covariance_type == 'full':\n",
    "        cov_params = n_components * (n_features * (n_features + 1) / 2)\n",
    "    elif covariance_type == 'diag':\n",
    "        cov_params = n_components * n_features\n",
    "    else:\n",
    "        cov_params = n_components * n_features  # fallback\n",
    "    return int(startp + trans + n_components * n_features + cov_params)\n",
    "\n",
    "# Fit models with different component counts and compute AIC/BIC\n",
    "results = []\n",
    "max_components = 6\n",
    "for k in range(1, max_components+1):\n",
    "    m = hmm.GaussianHMM(n_components=k, covariance_type='full', n_iter=200, random_state=RND)\n",
    "    m.fit(X, lengths)\n",
    "    ll = m.score(X, lengths)  # total log-likelihood for all data\n",
    "    p = n_hmm_params(k, n_dim, covariance_type='full')\n",
    "    n_obs = X.shape[0]\n",
    "    aic = -2 * ll + 2 * p\n",
    "    bic = -2 * ll + p * np.log(n_obs)\n",
    "    results.append({'n_components': k, 'log_likelihood': ll, 'n_params': p, 'AIC': aic, 'BIC': bic})\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "_df_results = pd.DataFrame(results).set_index('n_components')\n",
    "_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b413d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_results[['AIC','BIC']].plot(marker='o', figsize=(8,4))\n",
    "plt.title('AIC & BIC vs number of HMM components')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Information Criterion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578cbb9",
   "metadata": {},
   "source": [
    "## 6 — Practical tips and gotchas\n",
    "\n",
    "- **Label switching**: HMM states have no intrinsic order. Use matching strategies (Hungarian algorithm) when you have ground truth.\n",
    "- **Initialization matters**: `hmmlearn` uses random starts; run multiple initializations or provide sensible `means_init`/`transmat_init`.\n",
    "- **Sequence lengths**: if you have sequences of varying lengths, pass the `lengths` list to `fit`/`score`.\n",
    "- **Model selection**: AIC/BIC help but are not foolproof. Consider held-out likelihood or predictive performance on a downstream task.\n",
    "- **Scaling**: standardize features when needed (makes sense for Gaussian emissions).\n",
    "\n",
    "## 7 — Extensions & exercises (for students)\n",
    "\n",
    "1. Try discrete HMMs (`MultinomialHMM`) on discrete sequences (e.g., text tokens) and compare training behavior.\n",
    "2. Implement multiple random restarts and show how initialization affects final log-likelihood.\n",
    "3. Use a real dataset: e.g., sensor data, human activity sequences, or simplified speech data. Preprocess, then fit HMMs and interpret states.\n",
    "4. Create a supervised evaluation: train HMMs with different numbers of states and evaluate on held-out sequences using Viterbi-decoded labels.\n",
    "\n",
    "---\n",
    "\n",
    "That's the end of the notebook. Feel free to ask for expansions: adding MultinomialHMM examples, integrating with scikit-learn pipelines, or porting examples to a real dataset."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

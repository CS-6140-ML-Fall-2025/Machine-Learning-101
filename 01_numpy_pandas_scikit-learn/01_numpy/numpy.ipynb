{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Essentials for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Learn how to create and work with NumPy arrays for ML data representation\n",
    "- Master array operations, broadcasting, and reshaping for efficient ML computations\n",
    "- Apply linear algebra and statistical operations used in ML algorithms\n",
    "- Understand performance considerations for large-scale ML workflows\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy installed (`pip install numpy`)\n",
    "\n",
    "**Estimated Time:** ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "NumPy is the foundational library for numerical computing in Python and forms the backbone of the entire ML ecosystem. This notebook focuses on hands-on examples with practical ML context, aimed at beginners who want to understand how NumPy powers machine learning.\n",
    "\n",
    "**Why NumPy for ML?** Every ML library (scikit-learn, TensorFlow, PyTorch) uses NumPy arrays as their core data structure. Understanding NumPy is essential for:\n",
    "- Representing datasets as multi-dimensional arrays\n",
    "- Implementing ML algorithms efficiently\n",
    "- Understanding how neural networks process data in batches\n",
    "- Debugging shape and data type issues in ML pipelines\n",
    "\n",
    "**Learning Path:** This notebook prepares you for the Pandas notebook (data manipulation) and scikit-learn notebook (ML algorithms), where you'll see these NumPy concepts in action.\n",
    "\n",
    "**ðŸŽ¯ Success Indicators:** By the end, you should be able to:\n",
    "- Create and manipulate arrays for ML data representation\n",
    "- Debug shape errors (the #1 beginner challenge!)\n",
    "- Understand broadcasting for efficient computations\n",
    "- Implement a basic neural network forward pass\n",
    "\n",
    "**ðŸ’¡ Beginner Tips:**\n",
    "- Don't worry about memorizing everything - focus on understanding concepts\n",
    "- Run each cell and experiment with the code\n",
    "- Pay special attention to shape outputs - they're crucial for ML\n",
    "- When you see errors, read them carefully - NumPy errors are usually about shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.187293Z",
     "start_time": "2025-09-17T18:47:51.036365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.3\n",
      "Random seed set to 42 - now our 'random' numbers will be predictable!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# PARAMETER EXPLANATION: seed=42\n",
    "# â€¢ What it does: Makes random number generation predictable\n",
    "# â€¢ Why we need it: So everyone gets the same 'random' results when running this notebook\n",
    "# â€¢ Why 42? It's a reference to \"The Hitchhiker's Guide to the Galaxy\" - the answer to everything!\n",
    "# â€¢ In ML: Essential for reproducible experiments and debugging\n",
    "# â€¢ Alternative: You can use any integer (0, 123, 2024, etc.)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Random seed set to 42 - now our 'random' numbers will be predictable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Array Creation and Basic Properties\n",
    "\n",
    "Understanding how to create and inspect arrays is fundamental to ML workflows. In ML, data comes in many forms - images as pixel arrays, text as numerical vectors, tabular data as feature matrices. NumPy arrays provide a unified way to represent all these data types efficiently.\n",
    "\n",
    "**ML Context:** Every dataset in ML is ultimately represented as NumPy arrays with specific shapes:\n",
    "- **Images**: (batch_size, height, width, channels) \n",
    "- **Tabular data**: (samples, features)\n",
    "- **Time series**: (samples, timesteps, features)\n",
    "- **Neural network weights**: (input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.209349Z",
     "start_time": "2025-09-17T18:47:51.200602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From list (typical dataset format):\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Shape: (3, 3) (samples, features), Dtype: int64\n",
      "\n",
      "Zeros (bias initialization):\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Shape: (3, 4) (input_neurons, output_neurons)\n",
      "\n",
      "Random data (first 5 rows):\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n",
      " [-0.23413696  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      " [-0.46341769 -0.46572975  0.24196227 -1.91328024 -1.72491783]\n",
      " [-0.56228753 -1.01283112  0.31424733 -0.90802408 -1.4123037 ]\n",
      " [ 1.46564877 -0.2257763   0.0675282  -1.42474819 -0.54438272]]\n",
      "Shape: (100, 5) (batch_size, num_features)\n",
      "\n",
      "Identity matrix:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "1D array (vector representation):\n",
      "[10 20 30 40]\n",
      "Shape: (4,) (features,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Different ways to create arrays (common in ML)\n",
    "\n",
    "# From lists (converting loaded data to NumPy format)\n",
    "# This is how you might convert data from CSV files or databases\n",
    "data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # 3 samples, 3 features each\n",
    "arr_from_list = np.array(data_list)\n",
    "print(\"From list (typical dataset format):\")\n",
    "print(arr_from_list)\n",
    "print(f\"Shape: {arr_from_list.shape} (samples, features), Dtype: {arr_from_list.dtype}\\n\")\n",
    "\n",
    "# Zeros (neural network weight initialization)\n",
    "# In practice, you rarely initialize weights to zero, but it's useful for biases\n",
    "weights = np.zeros((3, 4))  # 3 input neurons, 4 output neurons\n",
    "print(\"Zeros (bias initialization):\")\n",
    "print(weights)\n",
    "print(f\"Shape: {weights.shape} (input_neurons, output_neurons)\\n\")\n",
    "\n",
    "# Random arrays (realistic weight initialization and synthetic data)\n",
    "# Normal distribution is common for weight initialization\n",
    "random_data = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "print(\"Random data (first 5 rows):\")\n",
    "print(random_data[:5])\n",
    "print(f\"Shape: {random_data.shape} (batch_size, num_features)\\n\")\n",
    "\n",
    "# Identity matrix (useful for regularization and initialization)\n",
    "# Often used in regularization terms and some initialization schemes\n",
    "identity = np.eye(3)\n",
    "print(\"Identity matrix:\")\n",
    "print(identity)\n",
    "\n",
    "# Interestingly, here is a shape of (x, )\n",
    "vector = np.array([10, 20, 30, 40])\n",
    "print(\"\\n1D array (vector representation):\")\n",
    "print(vector)\n",
    "print(f\"Shape: {vector.shape} (features,)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.227016Z",
     "start_time": "2025-09-17T18:47:51.220613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Properties (typical ML batch for sequence data):\n",
      "Shape: (32, 10, 8) (batch_size, seq_len, features)\n",
      "Number of dimensions: 3\n",
      "Total elements: 2560\n",
      "Data type: float64\n",
      "Memory usage: 20480 bytes\n",
      "Memory usage: 20.00 KB\n",
      "\n",
      "PARAMETER DEEP DIVE: dtype (data type)\n",
      "int32 array: int32, memory: 12 bytes\n",
      "float32 array: float32, memory: 12 bytes\n",
      "float64 array: float64, memory: 24 bytes\n",
      "Memory difference: float64 uses 2.0x more memory than float32\n",
      "\n",
      "Why these properties matter in ML:\n",
      "â€¢ Shape: Must match model input expectations (common source of errors!)\n",
      "â€¢ Dtype: float32 vs float64 affects memory and computation speed\n",
      "â€¢ Memory: Large models require careful memory management\n",
      "â€¢ We'll use these properties extensively in Pandas for data validation\n"
     ]
    }
   ],
   "source": [
    "# Array properties essential for ML debugging and optimization\n",
    "sample_array = np.random.randn(32, 10, 8)  # Batch size 32, sequence length 10, features 8\n",
    "\n",
    "print(\"Array Properties (typical ML batch for sequence data):\")\n",
    "print(f\"Shape: {sample_array.shape} (batch_size, seq_len, features)\")\n",
    "print(f\"Number of dimensions: {sample_array.ndim}\")\n",
    "print(f\"Total elements: {sample_array.size}\")\n",
    "print(f\"Data type: {sample_array.dtype}\")\n",
    "print(f\"Memory usage: {sample_array.nbytes} bytes\")\n",
    "print(f\"Memory usage: {sample_array.nbytes / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nPARAMETER DEEP DIVE: dtype (data type)\")\n",
    "# PARAMETER EXPLANATION: dtype\n",
    "# â€¢ What it is: Specifies the data type of array elements\n",
    "# â€¢ Common types: int32, int64, float32, float64, bool\n",
    "# â€¢ Why it matters: Affects memory usage, computation speed, and precision\n",
    "# â€¢ ML impact: float32 vs float64 can halve memory usage in large models\n",
    "# â€¢ GPU compatibility: Most GPUs prefer float32 for faster computation\n",
    "# â€¢ Precision trade-off: float32 has ~7 decimal digits, float64 has ~15\n",
    "\n",
    "# Demonstrate different dtypes\n",
    "int_array = np.array([1, 2, 3], dtype=np.int32)\n",
    "float32_array = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n",
    "float64_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "\n",
    "print(f\"int32 array: {int_array.dtype}, memory: {int_array.nbytes} bytes\")\n",
    "print(f\"float32 array: {float32_array.dtype}, memory: {float32_array.nbytes} bytes\")\n",
    "print(f\"float64 array: {float64_array.dtype}, memory: {float64_array.nbytes} bytes\")\n",
    "print(f\"Memory difference: float64 uses {float64_array.nbytes / float32_array.nbytes}x more memory than float32\")\n",
    "\n",
    "print(\"\\nWhy these properties matter in ML:\")\n",
    "print(\"â€¢ Shape: Must match model input expectations (common source of errors!)\")\n",
    "print(\"â€¢ Dtype: float32 vs float64 affects memory and computation speed\")\n",
    "print(\"â€¢ Memory: Large models require careful memory management\")\n",
    "print(\"â€¢ We'll use these properties extensively in Pandas for data validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Array Indexing and Slicing\n",
    "\n",
    "Critical for data manipulation, batch processing, and feature selection. In ML, you constantly need to:\n",
    "- Extract specific samples from batches\n",
    "- Select subsets of features\n",
    "- Filter data based on conditions\n",
    "- Prepare train/validation/test splits\n",
    "\n",
    "**ML Applications:** These indexing patterns appear everywhere in ML - from data preprocessing in Pandas to batch processing in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.244114Z",
     "start_time": "2025-09-17T18:47:51.237884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: (8, 28, 28, 3)\n",
      "Format: (batch_size=8, height=28, width=28, channels=3)\n",
      "\n",
      "Common ML Indexing Patterns:\n",
      "First image shape: (28, 28, 3) (height, width, channels)\n",
      "Mini-batch shape: (4, 28, 28, 3) (smaller batch for processing)\n",
      "Red channel shape: (8, 28, 28) (grayscale version)\n",
      "Center crop shape: (8, 14, 14, 3) (cropped for data augmentation)\n",
      "\n",
      "These patterns are essential for:\n",
      "â€¢ Batch processing in neural networks\n",
      "â€¢ Data augmentation and preprocessing\n",
      "â€¢ Feature extraction and selection\n",
      "â€¢ We'll see similar patterns in Pandas for tabular data\n"
     ]
    }
   ],
   "source": [
    "# Create sample data representing a batch of images (common in computer vision)\n",
    "# Shape: (batch_size, height, width, channels) - standard format for image data\n",
    "batch_images = np.random.randint(0, 256, size=(8, 28, 28, 3))\n",
    "\n",
    "print(\"Batch of images shape:\", batch_images.shape)\n",
    "print(\"Format: (batch_size=8, height=28, width=28, channels=3)\")\n",
    "print(\"\\nCommon ML Indexing Patterns:\")\n",
    "\n",
    "# Get first image (single sample from batch)\n",
    "first_image = batch_images[0]\n",
    "print(f\"First image shape: {first_image.shape} (height, width, channels)\")\n",
    "\n",
    "# Get first 4 images (creating smaller mini-batch)\n",
    "mini_batch = batch_images[:4]\n",
    "print(f\"Mini-batch shape: {mini_batch.shape} (smaller batch for processing)\")\n",
    "\n",
    "# Get red channel from all images (feature extraction)\n",
    "red_channel = batch_images[:, :, :, 0]  # Select channel 0 (red) from all images\n",
    "print(f\"Red channel shape: {red_channel.shape} (grayscale version)\")\n",
    "\n",
    "# Get center crop (data augmentation technique)\n",
    "center_crop = batch_images[:, 7:21, 7:21, :]  # 14x14 center region\n",
    "print(f\"Center crop shape: {center_crop.shape} (cropped for data augmentation)\")\n",
    "\n",
    "print(\"\\nThese patterns are essential for:\")\n",
    "print(\"â€¢ Batch processing in neural networks\")\n",
    "print(\"â€¢ Data augmentation and preprocessing\")\n",
    "print(\"â€¢ Feature extraction and selection\")\n",
    "print(\"â€¢ We'll see similar patterns in Pandas for tabular data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.261876Z",
     "start_time": "2025-09-17T18:47:51.255601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Performance Dataset:\n",
      "Original scores: [85 92 78 96 88 73 91 82]\n",
      "Names: ['Alice' 'Bob' 'Charlie' 'Diana' 'Eve' 'Frank' 'Grace' 'Henry']\n",
      "\n",
      "High performers mask: [False  True False  True  True False  True False]\n",
      "High performer scores: [92 96 88 91]\n",
      "High performer names: ['Bob' 'Diana' 'Eve' 'Grace']\n",
      "\n",
      "Scores in 80-90 range: [85 88 82]\n",
      "\n",
      "ML Applications of Boolean Indexing:\n",
      "â€¢ Removing outliers from datasets\n",
      "â€¢ Filtering samples based on quality metrics\n",
      "â€¢ Creating train/test splits based on conditions\n",
      "â€¢ Selecting features that meet certain criteria\n",
      "â€¢ Pandas uses these same patterns for DataFrame filtering\n"
     ]
    }
   ],
   "source": [
    "# Boolean indexing (filtering data - crucial for ML data preprocessing)\n",
    "scores = np.array([85, 92, 78, 96, 88, 73, 91, 82])\n",
    "names = np.array(['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'])\n",
    "\n",
    "print(\"Student Performance Dataset:\")\n",
    "print(\"Original scores:\", scores)\n",
    "print(\"Names:\", names)\n",
    "\n",
    "# Filter high performers (score > 85) - like filtering outliers or top performers\n",
    "high_performers = scores > 85\n",
    "print(f\"\\nHigh performers mask: {high_performers}\")\n",
    "print(f\"High performer scores: {scores[high_performers]}\")\n",
    "print(f\"High performer names: {names[high_performers]}\")\n",
    "\n",
    "# Multiple conditions (combining filters - common in data cleaning)\n",
    "good_range = (scores >= 80) & (scores <= 90)\n",
    "print(f\"\\nScores in 80-90 range: {scores[good_range]}\")\n",
    "\n",
    "print(\"\\nML Applications of Boolean Indexing:\")\n",
    "print(\"â€¢ Removing outliers from datasets\")\n",
    "print(\"â€¢ Filtering samples based on quality metrics\")\n",
    "print(\"â€¢ Creating train/test splits based on conditions\")\n",
    "print(\"â€¢ Selecting features that meet certain criteria\")\n",
    "print(\"â€¢ Pandas uses these same patterns for DataFrame filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Array Operations and Broadcasting\n",
    "\n",
    "Broadcasting is crucial for efficient ML computations and is used across all numerical computing libraries. It allows operations between arrays of different shapes without explicit loops, making code both faster and more readable.\n",
    "\n",
    "**Why Broadcasting Matters in ML:**\n",
    "- **Neural Networks**: Adding biases to all samples in a batch\n",
    "- **Data Preprocessing**: Normalizing features across entire datasets\n",
    "- **Model Operations**: Applying transformations efficiently\n",
    "- **Memory Efficiency**: Avoiding unnecessary array copies\n",
    "\n",
    "Understanding broadcasting is essential for debugging shape errors in ML pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.281097Z",
     "start_time": "2025-09-17T18:47:51.274277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Array b:\n",
      "[[2 2 2]\n",
      " [3 3 3]]\n",
      "\n",
      "Element-wise operations:\n",
      "Addition (a + b):\n",
      "[[3 4 5]\n",
      " [7 8 9]]\n",
      "\n",
      "Multiplication (a * b):\n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "\n",
      "Square (a**2):\n",
      "[[ 1  4  9]\n",
      " [16 25 36]]\n",
      "\n",
      "Common activation functions:\n",
      "Input: [-2 -1  0  1  2]\n",
      "ReLU (max(0, x)): [0 0 0 1 2]\n",
      "Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations (fundamental to neural networks)\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[2, 2, 2], [3, 3, 3]])\n",
    "\n",
    "print(\"Array a:\")\n",
    "print(a)\n",
    "print(\"\\nArray b:\")\n",
    "print(b)\n",
    "\n",
    "print(\"\\nElement-wise operations:\")\n",
    "print(\"Addition (a + b):\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\nMultiplication (a * b):\")\n",
    "print(a * b)\n",
    "\n",
    "print(\"\\nSquare (a**2):\")\n",
    "print(a ** 2)\n",
    "\n",
    "# Activation functions\n",
    "print(\"\\nCommon activation functions:\")\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"ReLU (max(0, x)): {np.maximum(0, x)}\")\n",
    "print(f\"Sigmoid: {1 / (1 + np.exp(-x))}\")\n",
    "print(f\"Tanh: {np.tanh(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.303858Z",
     "start_time": "2025-09-17T18:47:51.292567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting Examples:\n",
      "Features shape: (100, 5)\n",
      "Bias shape: (5,)\n",
      "Result shape: (100, 5)\n",
      "First sample before: [-1.33767419  0.61217162  0.56928932  0.07166855 -0.24237546]\n",
      "First sample after: [-1.23767419  0.41217162  0.86928932 -0.02833145 -0.04237546]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original data shape: (1000, 3)\n",
      "Original means: [4.3688771 5.4340328 5.8957205]\n",
      "Original stds: [ 9.97976302  9.78954254 10.3014967 ]\n",
      "\n",
      "Normalized means: [-1.52045043e-16 -2.39586129e-16 -4.27546887e-16]\n",
      "Normalized stds: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting examples (very important for ML)\n",
    "print(\"Broadcasting Examples:\")\n",
    "\n",
    "# Example 1: Adding bias to all samples\n",
    "features = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "bias = np.array([0.1, -0.2, 0.3, -0.1, 0.2])  # bias for each feature\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Bias shape: {bias.shape}\")\n",
    "\n",
    "# Broadcasting adds bias to each sample\n",
    "features_with_bias = features + bias\n",
    "print(f\"Result shape: {features_with_bias.shape}\")\n",
    "print(f\"First sample before: {features[0]}\")\n",
    "print(f\"First sample after: {features_with_bias[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 2: Normalizing features (mean centering)\n",
    "data = np.random.randn(1000, 3) * 10 + 5  # Add some offset and scale\n",
    "print(f\"\\nOriginal data shape: {data.shape}\")\n",
    "print(f\"Original means: {np.mean(data, axis=0)}\")\n",
    "print(f\"Original stds: {np.std(data, axis=0)}\")\n",
    "\n",
    "# Normalize (broadcasting)\n",
    "mean = np.mean(data, axis=0)  # Shape: (3,)\n",
    "std = np.std(data, axis=0)  # Shape: (3,)\n",
    "normalized_data = (data - mean) / std  # Broadcasting!\n",
    "\n",
    "print(f\"\\nNormalized means: {np.mean(normalized_data, axis=0)}\")\n",
    "print(f\"Normalized stds: {np.std(normalized_data, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.323426Z",
     "start_time": "2025-09-17T18:47:51.315379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting Rules Examples:\n",
      "\n",
      "(3, 4) + (4,) -> (3, 4)\n",
      "\n",
      "Array A:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "Array B:\n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "Result:\n",
      "[[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]]\n",
      "--------------------------------------------------\n",
      "\n",
      "(2, 3, 4) + (4,) -> (2, 3, 4)\n",
      "\n",
      "Array A:\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "\n",
      "Array B:\n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "Result:\n",
      "[[[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]\n",
      "\n",
      " [[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]]\n",
      "--------------------------------------------------\n",
      "\n",
      "(2, 3, 4) + (3, 4) -> (2, 3, 4)\n",
      "\n",
      "Array A:\n",
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "\n",
      "Array B:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "Result:\n",
      "[[[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]\n",
      "\n",
      " [[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]]\n",
      "--------------------------------------------------\n",
      "\n",
      "(2, 1, 4) + (3, 4) -> (2, 3, 4)\n",
      "\n",
      "Array A:\n",
      "[[[1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]]]\n",
      "\n",
      "Array B:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "Result:\n",
      "[[[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]\n",
      "\n",
      " [[2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [2. 2. 2. 2.]]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting rules visualization\n",
    "print(\"Broadcasting Rules Examples:\")\n",
    "\n",
    "# Rule: Arrays are aligned from the rightmost dimension\n",
    "examples = [\n",
    "    ((3, 4), (4,)),  # (3,4) + (4,) -> (3,4)\n",
    "    ((2, 3, 4), (4,)),  # (2,3,4) + (4,) -> (2,3,4)\n",
    "    ((2, 3, 4), (3, 4)),  # (2,3,4) + (3,4) -> (2,3,4)\n",
    "    ((2, 1, 4), (3, 4)),  # (2,1,4) + (3,4) -> (2,3,4)\n",
    "]\n",
    "\n",
    "for shape1, shape2 in examples:\n",
    "    a = np.ones(shape1)\n",
    "    b = np.ones(shape2)\n",
    "    result = a + b\n",
    "    print(f\"\\n{shape1} + {shape2} -> {result.shape}\")\n",
    "    print(\"\\nArray A:\")\n",
    "    print(a)\n",
    "    print(\"\\nArray B:\")\n",
    "    print(b)\n",
    "    print(\"\\nResult:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Algebra Operations\n",
    "\n",
    "Essential for understanding neural network computations, matrix multiplications, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.346593Z",
     "start_time": "2025-09-17T18:47:51.336465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL NETWORK FORWARD PASS SIMULATION\n",
      "==================================================\n",
      "Input batch X shape: (32, 10) (batch_size, input_features)\n",
      "Weight matrix W shape: (10, 5) (input_features, output_features)\n",
      "Bias vector b shape: (5,) (output_features,)\n",
      "\n",
      "Step-by-step forward pass:\n",
      "1. Matrix multiplication: X @ W\n",
      "   Result shape: (32, 5)\n",
      "2. Add bias (broadcasting): (X @ W) + b\n",
      "   Final output shape: (32, 5)\n",
      "\n",
      "Sample data (first sample):\n",
      "Input features: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]... (showing first 5 of 10)\n",
      "Raw output: [-0.06175452  0.02967849  0.05067092  0.1289749  -0.20615907]\n",
      "After ReLU activation: [0.         0.02967849 0.05067092 0.1289749  0.        ]\n",
      "\n",
      "This is the fundamental operation in every neural network layer!\n",
      "â€¢ Input shape must match weight matrix first dimension\n",
      "â€¢ Output shape is (batch_size, output_features)\n",
      "â€¢ Broadcasting handles bias addition automatically\n",
      "â€¢ Activation functions are applied element-wise\n",
      "\n",
      "ðŸŽ¯ TRY IT YOURSELF - Neural Network Layer\n",
      "==================================================\n",
      "Challenge: Create a neural network layer with different dimensions\n",
      "\n",
      "Your task:\n",
      "1. Create input data: 16 samples, 20 features\n",
      "2. Create weights: 20 inputs -> 8 outputs\n",
      "3. Create bias: 8 values\n",
      "4. Perform forward pass: X @ W + b\n",
      "5. Apply ReLU activation\n",
      "\n",
      "Expected output shape: (16, 8)\n",
      "\n",
      "Hint: Use np.random.randn() for weights, np.zeros() for bias\n",
      "Bonus: Try different activation functions (sigmoid, tanh)\n",
      "\n",
      "# Uncomment and complete the code below:\n",
      "# X_exercise = np.random.randn(?, ?)\n",
      "# W_exercise = np.random.randn(?, ?) * 0.1\n",
      "# b_exercise = np.zeros(?)\n",
      "# output = ?\n",
      "# activated = ?\n",
      "# print(f'Output shape: {activated.shape}')\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication (core of neural networks)\n",
    "print(\"NEURAL NETWORK FORWARD PASS SIMULATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate a realistic neural network layer\n",
    "# This is exactly how frameworks like TensorFlow/PyTorch work under the hood!\n",
    "batch_size, input_features, output_features = 32, 10, 5\n",
    "\n",
    "# Create realistic training data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "X = np.random.randn(batch_size, input_features)  # Input batch (32 samples, 10 features each)\n",
    "W = np.random.randn(input_features, output_features) * 0.1  # Weights (small random values)\n",
    "b = np.zeros(output_features)  # Bias (often initialized to zero)\n",
    "\n",
    "print(f\"Input batch X shape: {X.shape} (batch_size, input_features)\")\n",
    "print(f\"Weight matrix W shape: {W.shape} (input_features, output_features)\")\n",
    "print(f\"Bias vector b shape: {b.shape} (output_features,)\")\n",
    "\n",
    "print(\"\\nStep-by-step forward pass:\")\n",
    "print(\"1. Matrix multiplication: X @ W\")\n",
    "linear_output = X @ W  # Modern syntax, same as np.dot(X, W)\n",
    "print(f\"   Result shape: {linear_output.shape}\")\n",
    "\n",
    "print(\"2. Add bias (broadcasting): (X @ W) + b\")\n",
    "Y = linear_output + b  # Broadcasting adds bias to each sample\n",
    "print(f\"   Final output shape: {Y.shape}\")\n",
    "\n",
    "print(\"\\nSample data (first sample):\")\n",
    "print(f\"Input features: {X[0][:5]}... (showing first 5 of 10)\")\n",
    "print(f\"Raw output: {Y[0]}\")\n",
    "\n",
    "# Apply activation function (ReLU)\n",
    "Y_activated = np.maximum(0, Y)\n",
    "print(f\"After ReLU activation: {Y_activated[0]}\")\n",
    "\n",
    "print(\"\\nThis is the fundamental operation in every neural network layer!\")\n",
    "print(\"â€¢ Input shape must match weight matrix first dimension\")\n",
    "print(\"â€¢ Output shape is (batch_size, output_features)\")\n",
    "print(\"â€¢ Broadcasting handles bias addition automatically\")\n",
    "print(\"â€¢ Activation functions are applied element-wise\")\n",
    "\n",
    "print(\"\\n\" + \"ðŸŽ¯\" + \" TRY IT YOURSELF - Neural Network Layer\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Challenge: Create a neural network layer with different dimensions\")\n",
    "print(\"\\nYour task:\")\n",
    "print(\"1. Create input data: 16 samples, 20 features\")\n",
    "print(\"2. Create weights: 20 inputs -> 8 outputs\")\n",
    "print(\"3. Create bias: 8 values\")\n",
    "print(\"4. Perform forward pass: X @ W + b\")\n",
    "print(\"5. Apply ReLU activation\")\n",
    "print(\"\\nExpected output shape: (16, 8)\")\n",
    "print(\"\\nHint: Use np.random.randn() for weights, np.zeros() for bias\")\n",
    "print(\"Bonus: Try different activation functions (sigmoid, tanh)\")\n",
    "print(\"\\n# Uncomment and complete the code below:\")\n",
    "print(\"# X_exercise = np.random.randn(?, ?)\")\n",
    "print(\"# W_exercise = np.random.randn(?, ?) * 0.1\")\n",
    "print(\"# b_exercise = np.zeros(?)\")\n",
    "print(\"# output = ?\")\n",
    "print(\"# activated = ?\")\n",
    "print(\"# print(f'Output shape: {activated.shape}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.365749Z",
     "start_time": "2025-09-17T18:47:51.359966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Operations:\n",
      "A shape: (3, 4)\n",
      "B shape: (4, 2)\n",
      "A @ B shape: (3, 2)\n",
      "A transpose shape: (4, 3)\n",
      "\n",
      "Square matrix shape: (3, 3)\n",
      "Element-wise square: (3, 3)\n",
      "Matrix multiplication: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Different matrix operations\n",
    "A = np.random.randn(3, 4)\n",
    "B = np.random.randn(4, 2)\n",
    "\n",
    "print(\"Matrix Operations:\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "C = A @ B  # Same as np.dot(A, B)\n",
    "print(f\"A @ B shape: {C.shape}\")\n",
    "\n",
    "# Transpose (very common in ML)\n",
    "A_T = A.T\n",
    "print(f\"A transpose shape: {A_T.shape}\")\n",
    "\n",
    "# Element-wise vs matrix multiplication\n",
    "square_matrix = np.random.randn(3, 3)\n",
    "print(f\"\\nSquare matrix shape: {square_matrix.shape}\")\n",
    "print(f\"Element-wise square: {(square_matrix * square_matrix).shape}\")\n",
    "print(f\"Matrix multiplication: {(square_matrix @ square_matrix).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.400220Z",
     "start_time": "2025-09-17T18:47:51.379698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Linear Algebra:\n",
      "Matrix A shape: (4, 4)\n",
      "Eigenvalues: [-3.9264463   0.39225169  3.98449077  2.81871121]\n",
      "Eigenvectors shape: (4, 4)\n",
      "\n",
      "Matrix norms:\n",
      "Frobenius norm: 3.5119\n",
      "L2 norm: 2.4128\n",
      "\n",
      "Determinant: -17.2977\n",
      "Inverse exists, shape: (4, 4)\n",
      "A @ A^(-1) close to identity: True\n"
     ]
    }
   ],
   "source": [
    "# Advanced linear algebra (useful for understanding ML algorithms)\n",
    "print(\"Advanced Linear Algebra:\")\n",
    "\n",
    "# Create a symmetric matrix (common in optimization)\n",
    "A = np.random.randn(4, 4)\n",
    "symmetric_A = A + A.T\n",
    "\n",
    "print(f\"Matrix A shape: {symmetric_A.shape}\")\n",
    "\n",
    "# Eigenvalues and eigenvectors (PCA, optimization)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(symmetric_A)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# Matrix norms (regularization)\n",
    "print(\"\\nMatrix norms:\")\n",
    "print(f\"Frobenius norm: {np.linalg.norm(A, 'fro'):.4f}\")\n",
    "print(f\"L2 norm: {np.linalg.norm(A, 2):.4f}\")\n",
    "\n",
    "# Determinant and inverse\n",
    "det_A = np.linalg.det(symmetric_A)\n",
    "print(f\"\\nDeterminant: {det_A:.4f}\")\n",
    "\n",
    "if abs(det_A) > 1e-10:  # Check if invertible\n",
    "    inv_A = np.linalg.inv(symmetric_A)\n",
    "    print(f\"Inverse exists, shape: {inv_A.shape}\")\n",
    "    # Verify: A @ A^(-1) should be identity\n",
    "    identity_check = symmetric_A @ inv_A\n",
    "    print(f\"A @ A^(-1) close to identity: {np.allclose(identity_check, np.eye(4))}\")\n",
    "else:\n",
    "    print(\"Matrix is singular (not invertible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.445713Z",
     "start_time": "2025-09-17T18:47:51.416730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING A COMPLETE NEURAL NETWORK WITH NUMPY\n",
      "============================================================\n",
      "This shows how NumPy powers all deep learning frameworks!\n",
      "============================================================\n",
      "Input shape: (32, 784)\n",
      "Layer 1 weights: (784, 128), bias: (128,)\n",
      "Layer 2 weights: (128, 64), bias: (64,)\n",
      "Output weights: (64, 10), bias: (10,)\n",
      "\n",
      "Forward pass through the network:\n",
      "After layer 1: (32, 128) (applied ReLU)\n",
      "After layer 2: (32, 64) (applied ReLU)\n",
      "Raw output (logits): (32, 10)\n",
      "Final probabilities: (32, 10)\n",
      "\n",
      "First sample results:\n",
      "Raw logits: [ 0.00128735 -0.00033085 -0.00027823 -0.00077188  0.00117734 -0.00034779\n",
      "  0.00021851 -0.00190616 -0.00036514  0.00149905]\n",
      "Probabilities: [0.10012694 0.09996505 0.09997031 0.09992097 0.10011593 0.09996336\n",
      " 0.10001998 0.0998077  0.09996162 0.10014814]\n",
      "Predicted class: 9\n",
      "Confidence: 0.100\n",
      "\n",
      "Key insights:\n",
      "â€¢ Each layer transforms data: (batch_size, input_dim) -> (batch_size, output_dim)\n",
      "â€¢ Matrix multiplication handles all samples simultaneously (vectorization!)\n",
      "â€¢ Broadcasting adds biases to entire batches automatically\n",
      "â€¢ Activation functions are applied element-wise\n",
      "â€¢ This is exactly how TensorFlow, PyTorch, etc. work under the hood\n",
      "\n",
      "You'll implement this from scratch in the scikit-learn notebook!\n",
      "\n",
      "============================================================\n",
      "ðŸš¨ COMMON BEGINNER ERRORS AND SOLUTIONS\n",
      "============================================================\n",
      "\n",
      "1. SHAPE MISMATCH ERROR:\n",
      "   Error: 'ValueError: operands could not be broadcast together'\n",
      "   Solution: Check array shapes with .shape before operations\n",
      "   Example: Use keepdims=True or reshape arrays to match\n",
      "\n",
      "2. MATRIX MULTIPLICATION ERROR:\n",
      "   Error: 'ValueError: matmul: Input operand does not have enough dimensions'\n",
      "   Solution: Remember matrix multiplication rules: (m,n) @ (n,p) = (m,p)\n",
      "   Tip: Use @ operator for matrix multiplication, * for element-wise\n",
      "\n",
      "3. INDEXING ERROR:\n",
      "   Error: 'IndexError: index out of bounds'\n",
      "   Solution: Check array dimensions with .shape first\n",
      "   Tip: Remember Python uses 0-based indexing\n",
      "\n",
      "4. DTYPE CONFUSION:\n",
      "   Problem: Unexpected results from integer division\n",
      "   Solution: Be explicit about dtypes, use .astype() when needed\n",
      "   Tip: float32 vs float64 affects memory and GPU compatibility\n",
      "\n",
      "ðŸ’¡ DEBUGGING TIPS:\n",
      "â€¢ Always print array shapes when debugging: print(f'Shape: {arr.shape}')\n",
      "â€¢ Use small test arrays to understand operations before scaling up\n",
      "â€¢ Read error messages carefully - they usually tell you exactly what's wrong\n",
      "â€¢ When in doubt, check the NumPy documentation with help(np.function_name)\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE MULTI-LAYER NEURAL NETWORK EXAMPLE\n",
    "print(\"BUILDING A COMPLETE NEURAL NETWORK WITH NUMPY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This shows how NumPy powers all deep learning frameworks!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Network architecture: 784 -> 128 -> 64 -> 10 (like MNIST digit classification)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input: flattened 28x28 images\n",
    "batch_size = 32\n",
    "X = np.random.randn(batch_size, 784)  # 32 images, 784 pixels each\n",
    "\n",
    "# Layer 1: 784 -> 128\n",
    "W1 = np.random.randn(784, 128) * 0.01  # Small weights for stable training\n",
    "b1 = np.zeros(128)\n",
    "\n",
    "# Layer 2: 128 -> 64  \n",
    "W2 = np.random.randn(128, 64) * 0.01\n",
    "b2 = np.zeros(64)\n",
    "\n",
    "# Output layer: 64 -> 10\n",
    "W3 = np.random.randn(64, 10) * 0.01\n",
    "b3 = np.zeros(10)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Layer 1 weights: {W1.shape}, bias: {b1.shape}\")\n",
    "print(f\"Layer 2 weights: {W2.shape}, bias: {b2.shape}\")\n",
    "print(f\"Output weights: {W3.shape}, bias: {b3.shape}\")\n",
    "\n",
    "print(\"\\nForward pass through the network:\")\n",
    "\n",
    "# Layer 1: Linear + ReLU\n",
    "z1 = X @ W1 + b1  # Linear transformation\n",
    "a1 = np.maximum(0, z1)  # ReLU activation\n",
    "print(f\"After layer 1: {a1.shape} (applied ReLU)\")\n",
    "\n",
    "# Layer 2: Linear + ReLU\n",
    "z2 = a1 @ W2 + b2\n",
    "a2 = np.maximum(0, z2)\n",
    "print(f\"After layer 2: {a2.shape} (applied ReLU)\")\n",
    "\n",
    "# Output layer: Linear only (no activation yet)\n",
    "z3 = a2 @ W3 + b3\n",
    "print(f\"Raw output (logits): {z3.shape}\")\n",
    "\n",
    "# Apply softmax for classification probabilities\n",
    "# Numerically stable softmax\n",
    "exp_logits = np.exp(z3 - np.max(z3, axis=1, keepdims=True))\n",
    "probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "print(f\"Final probabilities: {probabilities.shape}\")\n",
    "\n",
    "# Show results for first sample\n",
    "print(f\"\\nFirst sample results:\")\n",
    "print(f\"Raw logits: {z3[0]}\")\n",
    "print(f\"Probabilities: {probabilities[0]}\")\n",
    "print(f\"Predicted class: {np.argmax(probabilities[0])}\")\n",
    "print(f\"Confidence: {np.max(probabilities[0]):.3f}\")\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"â€¢ Each layer transforms data: (batch_size, input_dim) -> (batch_size, output_dim)\")\n",
    "print(\"â€¢ Matrix multiplication handles all samples simultaneously (vectorization!)\")\n",
    "print(\"â€¢ Broadcasting adds biases to entire batches automatically\")\n",
    "print(\"â€¢ Activation functions are applied element-wise\")\n",
    "print(\"â€¢ This is exactly how TensorFlow, PyTorch, etc. work under the hood\")\n",
    "print(\"\\nYou'll implement this from scratch in the scikit-learn notebook!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸš¨ COMMON BEGINNER ERRORS AND SOLUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. SHAPE MISMATCH ERROR:\")\n",
    "print(\"   Error: 'ValueError: operands could not be broadcast together'\")\n",
    "print(\"   Solution: Check array shapes with .shape before operations\")\n",
    "print(\"   Example: Use keepdims=True or reshape arrays to match\")\n",
    "\n",
    "print(\"\\n2. MATRIX MULTIPLICATION ERROR:\")\n",
    "print(\"   Error: 'ValueError: matmul: Input operand does not have enough dimensions'\")\n",
    "print(\"   Solution: Remember matrix multiplication rules: (m,n) @ (n,p) = (m,p)\")\n",
    "print(\"   Tip: Use @ operator for matrix multiplication, * for element-wise\")\n",
    "\n",
    "print(\"\\n3. INDEXING ERROR:\")\n",
    "print(\"   Error: 'IndexError: index out of bounds'\")\n",
    "print(\"   Solution: Check array dimensions with .shape first\")\n",
    "print(\"   Tip: Remember Python uses 0-based indexing\")\n",
    "\n",
    "print(\"\\n4. DTYPE CONFUSION:\")\n",
    "print(\"   Problem: Unexpected results from integer division\")\n",
    "print(\"   Solution: Be explicit about dtypes, use .astype() when needed\")\n",
    "print(\"   Tip: float32 vs float64 affects memory and GPU compatibility\")\n",
    "\n",
    "print(\"\\nðŸ’¡ DEBUGGING TIPS:\")\n",
    "print(\"â€¢ Always print array shapes when debugging: print(f'Shape: {arr.shape}')\")\n",
    "print(\"â€¢ Use small test arrays to understand operations before scaling up\")\n",
    "print(\"â€¢ Read error messages carefully - they usually tell you exactly what's wrong\")\n",
    "print(\"â€¢ When in doubt, check the NumPy documentation with help(np.function_name)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Operations and Aggregations\n",
    "\n",
    "Critical for data analysis, loss computation, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.465504Z",
     "start_time": "2025-09-17T18:47:51.453493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (100, 5)\n",
      "\n",
      "Statistical Operations:\n",
      "Overall mean: 0.0559\n",
      "Overall std: 1.0089\n",
      "Min value: -2.5708\n",
      "Max value: 3.7079\n",
      "\n",
      "PARAMETER DEEP DIVE: axis parameter\n",
      "Shape: (100, 5) (100 samples, 5 classes)\n",
      "\n",
      "â€¢ axis=0: Operate along ROWS (collapse rows, keep columns)\n",
      "  Result: One value per CLASS (across all samples)\n",
      "  Mean per class: [-0.04334744 -0.0291489   0.05482263  0.20613827  0.09115166]\n",
      "  Shape: (5,) (5 classes)\n",
      "\n",
      "â€¢ axis=1: Operate along COLUMNS (collapse columns, keep rows)\n",
      "  Result: One value per SAMPLE (across all classes)\n",
      "  Mean per sample shape: (100,) (100 samples)\n",
      "  First 5 sample means: [ 0.14071375  0.17394602 -0.5760761   0.09966101  0.02983557]\n",
      "\n",
      "PARAMETER DEEP DIVE: keepdims parameter\n",
      "â€¢ keepdims=True:  (100, 1) (keeps original dimensions)\n",
      "â€¢ keepdims=False: (100,) (removes collapsed dimension)\n",
      "\n",
      "Why keepdims=True matters:\n",
      "â€¢ Preserves shape for broadcasting operations\n",
      "â€¢ Essential for neural network computations\n",
      "â€¢ Prevents shape mismatch errors in ML pipelines\n",
      "\n",
      "Practical example - subtracting max for numerical stability:\n",
      "âœ“ With keepdims=True: Broadcasting works! Shape: (100, 5)\n",
      "âœ“ With keepdims=False: Need manual reshape for broadcasting\n"
     ]
    }
   ],
   "source": [
    "# Statistical operations along different axes\n",
    "# Simulate prediction scores for classification\n",
    "# Shape: (batch_size, num_classes)\n",
    "predictions = np.random.randn(100, 5)  # 100 samples, 5 classes\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(\"\\nStatistical Operations:\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"Overall mean: {np.mean(predictions):.4f}\")\n",
    "print(f\"Overall std: {np.std(predictions):.4f}\")\n",
    "print(f\"Min value: {np.min(predictions):.4f}\")\n",
    "print(f\"Max value: {np.max(predictions):.4f}\")\n",
    "\n",
    "# Statistics along axes - CRITICAL for ML!\n",
    "print(\"\\nPARAMETER DEEP DIVE: axis parameter\")\n",
    "print(\"Shape:\", predictions.shape, \"(100 samples, 5 classes)\")\n",
    "print(\"\\nâ€¢ axis=0: Operate along ROWS (collapse rows, keep columns)\")\n",
    "print(\"  Result: One value per CLASS (across all samples)\")\n",
    "mean_per_class = np.mean(predictions, axis=0)\n",
    "print(f\"  Mean per class: {mean_per_class}\")\n",
    "print(f\"  Shape: {mean_per_class.shape} (5 classes)\")\n",
    "\n",
    "print(\"\\nâ€¢ axis=1: Operate along COLUMNS (collapse columns, keep rows)\")\n",
    "print(\"  Result: One value per SAMPLE (across all classes)\")\n",
    "mean_per_sample = np.mean(predictions, axis=1)\n",
    "print(f\"  Mean per sample shape: {mean_per_sample.shape} (100 samples)\")\n",
    "print(f\"  First 5 sample means: {mean_per_sample[:5]}\")\n",
    "\n",
    "# PARAMETER DEEP DIVE: keepdims\n",
    "print(\"\\nPARAMETER DEEP DIVE: keepdims parameter\")\n",
    "# PARAMETER EXPLANATION: keepdims=True/False\n",
    "# â€¢ What it does: Controls whether to keep the original number of dimensions after reduction\n",
    "# â€¢ keepdims=True: Keeps the reduced axis as size 1 (e.g., (100,5) -> (100,1))\n",
    "# â€¢ keepdims=False: Removes the reduced axis completely (e.g., (100,5) -> (100,))\n",
    "# â€¢ Why it matters: keepdims=True preserves shape for broadcasting operations\n",
    "# â€¢ ML importance: Critical for neural networks where shape consistency is essential\n",
    "# â€¢ Common mistake: Forgetting keepdims=True leads to broadcasting errors\n",
    "max_keepdims_true = np.max(predictions, axis=1, keepdims=True)\n",
    "max_keepdims_false = np.max(predictions, axis=1, keepdims=False)\n",
    "\n",
    "print(f\"â€¢ keepdims=True:  {max_keepdims_true.shape} (keeps original dimensions)\")\n",
    "print(f\"â€¢ keepdims=False: {max_keepdims_false.shape} (removes collapsed dimension)\")\n",
    "print(\"\\nWhy keepdims=True matters:\")\n",
    "print(\"â€¢ Preserves shape for broadcasting operations\")\n",
    "print(\"â€¢ Essential for neural network computations\")\n",
    "print(\"â€¢ Prevents shape mismatch errors in ML pipelines\")\n",
    "\n",
    "# Show practical difference\n",
    "print(\"\\nPractical example - subtracting max for numerical stability:\")\n",
    "try:\n",
    "    # This works because of keepdims=True (broadcasting compatible)\n",
    "    stable_predictions = predictions - max_keepdims_true\n",
    "    print(f\"âœ“ With keepdims=True: Broadcasting works! Shape: {stable_predictions.shape}\")\n",
    "except:\n",
    "    print(\"âœ— Broadcasting failed\")\n",
    "\n",
    "try:\n",
    "    # This might fail without keepdims (shape mismatch)\n",
    "    unstable = predictions - max_keepdims_false.reshape(-1, 1)  # Need manual reshape\n",
    "    print(f\"âœ“ With keepdims=False: Need manual reshape for broadcasting\")\n",
    "except:\n",
    "    print(\"âœ— Would fail without manual reshape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.488764Z",
     "start_time": "2025-09-17T18:47:51.480974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical ML Statistical Operations:\n",
      "Logits shape: (5, 3)\n",
      "Probabilities shape: (5, 3)\n",
      "Probabilities sum per sample: [1. 1. 1. 1. 1.]\n",
      "First sample probabilities: [0.15434019 0.33447144 0.51118837]\n",
      "\n",
      "True labels: [0 1 2 1 0]\n",
      "Predicted labels: [2 0 2 2 0]\n",
      "Accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Practical ML examples\n",
    "print(\"Practical ML Statistical Operations:\")\n",
    "\n",
    "\n",
    "# 1. Softmax implementation\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "logits = np.random.randn(5, 3)  # 5 samples, 3 classes\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Probabilities sum per sample: {np.sum(probabilities, axis=1)}\")\n",
    "print(f\"First sample probabilities: {probabilities[0]}\")\n",
    "\n",
    "# 2. Accuracy calculation\n",
    "true_labels = np.array([0, 1, 2, 1, 0])\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = np.mean(true_labels == predicted_labels)\n",
    "print(f\"\\nTrue labels: {true_labels}\")\n",
    "print(f\"Predicted labels: {predicted_labels}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.508244Z",
     "start_time": "2025-09-17T18:47:51.500986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Loss Functions:\n",
      "True values: [1.5 2.3 3.1 4.2 5. ]\n",
      "Predicted values: [1.2 2.1 3.4 4.  5.2]\n",
      "MSE: 0.0600\n",
      "RMSE: 0.2449\n",
      "MAE: 0.2400\n",
      "\n",
      "Cross-entropy loss: 1.1597\n"
     ]
    }
   ],
   "source": [
    "# Loss function implementations\n",
    "print(\"Common Loss Functions:\")\n",
    "\n",
    "# Mean Squared Error (regression)\n",
    "y_true = np.array([1.5, 2.3, 3.1, 4.2, 5.0])\n",
    "y_pred = np.array([1.2, 2.1, 3.4, 4.0, 5.2])\n",
    "\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(f\"True values: {y_true}\")\n",
    "print(f\"Predicted values: {y_pred}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Cross-entropy loss (classification)\n",
    "def cross_entropy_loss(y_true_labels, y_pred_probs):\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    # Convert labels to one-hot if needed\n",
    "    n_classes = y_pred_probs.shape[1]\n",
    "    y_true_onehot = np.eye(n_classes)[y_true_labels]\n",
    "\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred_clipped = np.clip(y_pred_probs, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Calculate cross-entropy per sample and average\n",
    "    per_sample = np.sum(y_true_onehot * np.log(y_pred_clipped), axis=1)\n",
    "    loss = np.mean(np.negative(per_sample))\n",
    "    return loss\n",
    "\n",
    "\n",
    "ce_loss = cross_entropy_loss(true_labels, probabilities)\n",
    "print(f\"\\nCross-entropy loss: {ce_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Array Reshaping and Manipulation\n",
    "\n",
    "Essential for preparing data for neural networks and handling different tensor shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.530206Z",
     "start_time": "2025-09-17T18:47:51.523222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Reshaping:\n",
      "Flattened images shape: (100, 784)\n",
      "Reshaped to images: (100, 28, 28)\n",
      "With channel dimension: (100, 28, 28, 1)\n",
      "Auto reshape (-1): (100, 28, 28, 1)\n",
      "Flattened again: (100, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping operations (very common in deep learning)\n",
    "print(\"Array Reshaping:\")\n",
    "\n",
    "# Original data: flattened images\n",
    "flattened_images = np.random.randint(0, 256, size=(100, 784))  # 100 images, 28x28 pixels\n",
    "print(f\"Flattened images shape: {flattened_images.shape}\")\n",
    "\n",
    "# Reshape to image format\n",
    "images = flattened_images.reshape(100, 28, 28)\n",
    "print(f\"Reshaped to images: {images.shape}\")\n",
    "\n",
    "# Add channel dimension (for CNN)\n",
    "images_with_channel = images.reshape(100, 28, 28, 1)\n",
    "print(f\"With channel dimension: {images_with_channel.shape}\")\n",
    "\n",
    "# Or using -1 for automatic calculation\n",
    "auto_reshape = flattened_images.reshape(100, 28, 28, -1)\n",
    "print(f\"Auto reshape (-1): {auto_reshape.shape}\")\n",
    "\n",
    "# Flatten back\n",
    "flattened_again = images_with_channel.reshape(100, -1)\n",
    "print(f\"Flattened again: {flattened_again.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.558852Z",
     "start_time": "2025-09-17T18:47:51.544350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis Manipulation:\n",
      "Original shape: (32, 50, 128)\n",
      "Transposed: (50, 32, 128)\n",
      "With new axis: (32, 50, 128, 1)\n",
      "Squeezed: (32, 50, 128)\n",
      "Expanded (axis=0): (1, 32, 50, 128)\n",
      "Expanded (axis=-1): (32, 50, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Axis manipulation\n",
    "print(\"Axis Manipulation:\")\n",
    "\n",
    "# Sample data: batch of sequences\n",
    "sequences = np.random.randn(32, 50, 128)  # batch_size, seq_len, features\n",
    "print(f\"Original shape: {sequences.shape}\")\n",
    "\n",
    "# Transpose (swap axes)\n",
    "transposed = sequences.transpose(1, 0, 2)  # seq_len, batch_size, features\n",
    "print(f\"Transposed: {transposed.shape}\")\n",
    "\n",
    "# Add new axis\n",
    "with_new_axis = sequences[:, :, :, np.newaxis]\n",
    "print(f\"With new axis: {with_new_axis.shape}\")\n",
    "\n",
    "# Squeeze (remove dimensions of size 1)\n",
    "squeezed = np.squeeze(with_new_axis)\n",
    "print(f\"Squeezed: {squeezed.shape}\")\n",
    "\n",
    "# Expand dimensions\n",
    "expanded = np.expand_dims(sequences, axis=0)\n",
    "print(f\"Expanded (axis=0): {expanded.shape}\")\n",
    "\n",
    "expanded_last = np.expand_dims(sequences, axis=-1)\n",
    "print(f\"Expanded (axis=-1): {expanded_last.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:47:51.576398Z",
     "start_time": "2025-09-17T18:47:51.567271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation and Stacking:\n",
      "Batch 1 shape: (16, 10)\n",
      "Batch 2 shape: (16, 10)\n",
      "Batch 3 shape: (16, 10)\n",
      "Combined batches: (48, 10)\n",
      "Stacked batches: (3, 16, 10)\n",
      "\n",
      "Features 1: (100, 5)\n",
      "Features 2: (100, 3)\n",
      "Combined features: (100, 8)\n"
     ]
    }
   ],
   "source": [
    "# Concatenation and stacking (combining data)\n",
    "print(\"Concatenation and Stacking:\")\n",
    "\n",
    "# Create sample batches\n",
    "batch1 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "batch2 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "batch3 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "\n",
    "print(f\"Batch 1 shape: {batch1.shape}\")\n",
    "print(f\"Batch 2 shape: {batch2.shape}\")\n",
    "print(f\"Batch 3 shape: {batch3.shape}\")\n",
    "\n",
    "# Concatenate along batch dimension\n",
    "combined_batches = np.concatenate([batch1, batch2, batch3], axis=0)\n",
    "print(f\"Combined batches: {combined_batches.shape}\")\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "stacked_batches = np.stack([batch1, batch2, batch3], axis=0)\n",
    "print(f\"Stacked batches: {stacked_batches.shape}\")\n",
    "\n",
    "# Horizontal stack (features)\n",
    "features1 = np.random.randn(100, 5)\n",
    "features2 = np.random.randn(100, 3)\n",
    "combined_features = np.hstack([features1, features2])\n",
    "print(f\"\\nFeatures 1: {features1.shape}\")\n",
    "print(f\"Features 2: {features2.shape}\")\n",
    "print(f\"Combined features: {combined_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance and Memory Considerations\n",
    "\n",
    "Understanding NumPy performance is crucial for efficient ML workflows. In ML, you often work with:\n",
    "- **Large datasets**: Millions of samples with hundreds of features\n",
    "- **High-dimensional arrays**: Images, sequences, embeddings\n",
    "- **Repeated operations**: Training loops, batch processing\n",
    "- **Memory constraints**: GPU memory, RAM limitations\n",
    "\n",
    "**Performance Rules for ML:**\n",
    "1. **Always vectorize**: Avoid Python loops at all costs\n",
    "2. **Use appropriate dtypes**: float32 vs float64 can halve memory usage\n",
    "3. **Understand views vs copies**: Avoid unnecessary memory allocation\n",
    "4. **Batch operations**: Process multiple samples simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:49:02.474048Z",
     "start_time": "2025-09-17T18:47:51.588684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML PERFORMANCE EXAMPLE: Feature Scaling\n",
      "==================================================\n",
      "Comparing Python loops vs NumPy vectorization for ML preprocessing\n",
      "Dataset shape: (100000, 50)\n",
      "Memory usage: 38.1 MB\n",
      "\n",
      "Method 1: Python loops (testing on subset for speed)\n",
      "Method 2: NumPy vectorization (entire dataset)\n",
      "\n",
      "Results:\n",
      "Loop time (1k samples): 62.9508 seconds\n",
      "Vectorized time (100k samples): 0.0970 seconds\n",
      "Estimated speedup: 6295.1x faster with vectorization!\n",
      "\n",
      "DATA TYPE IMPACT ON MEMORY:\n",
      "float64 memory: 38.1 MB\n",
      "float32 memory: 19.1 MB (50% less!)\n",
      "float16 memory: 9.5 MB (75% less!)\n",
      "\n",
      "For most ML tasks, float32 is sufficient and saves memory!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ML Performance Example: Feature Scaling Comparison\n",
    "print(\"ML PERFORMANCE EXAMPLE: Feature Scaling\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing Python loops vs NumPy vectorization for ML preprocessing\")\n",
    "\n",
    "# Simulate a realistic ML dataset\n",
    "n_samples, n_features = 100000, 50  # 100k samples, 50 features\n",
    "X = np.random.randn(n_samples, n_features) * 100 + 50  # Random dataset\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Memory usage: {X.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Method 1: Python loops (NEVER do this in ML!)\n",
    "print(\"\\nMethod 1: Python loops (testing on subset for speed)\")\n",
    "start_time = time.time()\n",
    "X_scaled_loop = np.zeros((1000, n_features))  # Only test 1000 samples\n",
    "for i in range(1000):\n",
    "    for j in range(n_features):\n",
    "        X_scaled_loop[i, j] = (X[i, j] - np.mean(X[:, j])) / np.std(X[:, j])\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Method 2: NumPy vectorization (the right way!)\n",
    "print(\"Method 2: NumPy vectorization (entire dataset)\")\n",
    "start_time = time.time()\n",
    "X_scaled_vectorized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Loop time (1k samples): {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized time (100k samples): {vectorized_time:.4f} seconds\")\n",
    "print(f\"Estimated speedup: {(loop_time * 100):.1f}x faster with vectorization!\")\n",
    "\n",
    "# Data type impact on memory\n",
    "print(\"\\nDATA TYPE IMPACT ON MEMORY:\")\n",
    "X_float64 = X.astype(np.float64)\n",
    "X_float32 = X.astype(np.float32)\n",
    "X_float16 = X.astype(np.float16)\n",
    "\n",
    "print(f\"float64 memory: {X_float64.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"float32 memory: {X_float32.nbytes / 1024 / 1024:.1f} MB (50% less!)\")\n",
    "print(f\"float16 memory: {X_float16.nbytes / 1024 / 1024:.1f} MB (75% less!)\")\n",
    "print(\"\\nFor most ML tasks, float32 is sufficient and saves memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:49:02.832663Z",
     "start_time": "2025-09-17T18:49:02.786520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Layout and Views:\n",
      "Original array memory: 7.63 MB\n",
      "View shares memory: True\n",
      "View shape: (500, 500)\n",
      "Copy shares memory: False\n",
      "Copy memory: 7.63 MB\n",
      "\n",
      "After modifying original[0,0] = 999:\n",
      "View[0,0] = 999.0 (should be 999 if it's a view)\n",
      "Copy[0,0] = 0.8087604084876876 (should be original value)\n"
     ]
    }
   ],
   "source": [
    "# Memory layout and views vs copies\n",
    "print(\"Memory Layout and Views:\")\n",
    "\n",
    "# Original array\n",
    "original = np.random.randn(1000, 1000)\n",
    "print(f\"Original array memory: {original.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# View (shares memory)\n",
    "view = original[::2, ::2]  # Every other element\n",
    "print(f\"View shares memory: {view.base is original}\")\n",
    "print(f\"View shape: {view.shape}\")\n",
    "\n",
    "# Copy (new memory)\n",
    "copy = original.copy()\n",
    "print(f\"Copy shares memory: {copy.base is original}\")\n",
    "print(f\"Copy memory: {copy.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Demonstrate view behavior\n",
    "original[0, 0] = 999\n",
    "print(\"\\nAfter modifying original[0,0] = 999:\")\n",
    "print(f\"View[0,0] = {view[0, 0]} (should be 999 if it's a view)\")\n",
    "print(f\"Copy[0,0] = {copy[0, 0]} (should be original value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with other ML libraries (brief, framework-neutral)\n",
    "\n",
    "NumPy arrays are the common numerical representation used across many machine learning tools. The key things to check when preparing NumPy arrays for use with other tools are:\n",
    "\n",
    "- dtype: many tools prefer float32 for inputs and integer labels for classification targets\n",
    "- shape: confirm batch and feature dimensions (e.g., (batch_size, num_features))\n",
    "- no NaNs or infinite values\n",
    "\n",
    "Example checks and simple conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T18:49:02.853094Z",
     "start_time": "2025-09-17T18:49:02.847317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NumPy arrays with other tools (framework-neutral):\n",
      "NumPy data shape: (32, 10)\n",
      "NumPy data dtype before: float64\n",
      "NumPy data dtype after: float32\n",
      "NumPy labels dtype after: int64\n",
      "\n",
      "Checklist before handing arrays to another tool:\n",
      " - Are shapes as expected? (batch, features)\n",
      " - Are dtypes appropriate? (e.g. float32 for inputs)\n",
      " - Are there NaNs or infinite values?\n"
     ]
    }
   ],
   "source": [
    "print(\"Using NumPy arrays with other tools (framework-neutral):\")\n",
    "\n",
    "# Create sample data in NumPy\n",
    "numpy_data = np.random.randn(32, 10)\n",
    "numpy_labels = np.random.randint(0, 3, size=(32,))\n",
    "\n",
    "print(f\"NumPy data shape: {numpy_data.shape}\")\n",
    "print(f\"NumPy data dtype before: {numpy_data.dtype}\")\n",
    "\n",
    "# Convert to a common dtype for ML (float32 inputs, int64 labels)\n",
    "numpy_data = numpy_data.astype(np.float32)\n",
    "numpy_labels = numpy_labels.astype(np.int64)\n",
    "\n",
    "print(f\"NumPy data dtype after: {numpy_data.dtype}\")\n",
    "print(f\"NumPy labels dtype after: {numpy_labels.dtype}\")\n",
    "\n",
    "print(\"\\nChecklist before handing arrays to another tool:\")\n",
    "print(\" - Are shapes as expected? (batch, features)\")\n",
    "print(\" - Are dtypes appropriate? (e.g. float32 for inputs)\")\n",
    "print(\" - Are there NaNs or infinite values?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. **Array Creation & Properties**: Understanding shapes, dtypes, and memory usage\n",
    "2. **Indexing & Slicing**: Essential for data manipulation and batch processing\n",
    "3. **Broadcasting**: Enables efficient operations without explicit loops\n",
    "4. **Linear Algebra**: Matrix operations that form the core of many ML models\n",
    "5. **Statistical Operations**: Computing metrics, losses, and aggregations\n",
    "6. **Reshaping**: Preparing data for different network architectures\n",
    "7. **Performance**: Vectorization and memory considerations\n",
    "8. **Tool Bridge**: How NumPy arrays are the common numerical format used by many ML tools\n",
    "\n",
    "**Key Patterns for ML:**\n",
    "- Use vectorized operations instead of loops\n",
    "- Understand broadcasting for efficient computations\n",
    "- Master axis-based operations for batch processing\n",
    "- Know when operations create views vs copies\n",
    "- Prepare data in NumPy before converting to other tools\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn Pandas for structured data manipulation\n",
    "- Understand how these concepts translate to other numerical libraries\n",
    "- See how higher-level ML libraries mirror NumPy patterns\n",
    "\n",
    "**Next Steps in Your ML Journey:**\n",
    "\n",
    "Now that you understand NumPy fundamentals, you're ready to move forward:\n",
    "\n",
    "1. **Pandas Notebook**: Learn how to work with structured data using DataFrames. You'll see how Pandas builds on NumPy arrays to handle real-world datasets.\n",
    "\n",
    "2. **Scikit-learn Notebook**: Apply these NumPy concepts to build actual ML models. You'll see how array operations and linear algebra power ML algorithms.\n",
    "\n",
    "**Key Connections:** Every Pandas DataFrame uses NumPy arrays, and scikit-learn expects NumPy arrays as input. The patterns you learned here appear everywhere in ML!\n",
    "\n",
    "NumPy is the foundation that makes the entire Python ML ecosystem possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸŒ¸ Your First ML Model in Google Colab\n",
    "\n",
    "**Welcome to machine learning!** This notebook will guide you through building your first ML model.\n",
    "\n",
    "**What you'll build**: A flower species classifier that can identify iris flowers from their measurements.\n",
    "\n",
    "**How to use this notebook**:\n",
    "1. **Save a copy**: File â†’ Save a copy in Drive (so you can edit it)\n",
    "2. **Run each cell**: Click the â–¶ï¸ button or press Shift + Enter\n",
    "3. **Follow along**: Read the explanations and watch the magic happen!\n",
    "\n",
    "**ğŸ“ Note**: This notebook is hosted on GitHub and opens directly in Colab!\n",
    "\n",
    "**Time needed**: 15-20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Import Libraries ğŸ“š\n",
    "\n",
    "First, let's import all the tools we need for machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import the tools we need for machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸš€ Ready to build your first ML model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Load and Explore the Data ğŸ”\n",
    "\n",
    "Let's load the famous iris flower dataset and see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load the famous iris flower dataset\n",
    "print(\"ğŸŒ¸ Loading the Iris dataset...\")\n",
    "data = load_iris()\n",
    "\n",
    "# Convert to a pandas DataFrame for easier handling\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['species'] = data.target_names[data.target]\n",
    "\n",
    "# Display basic information\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape[0]} flowers, {df.shape[1]-1} measurements\")\n",
    "print(f\"ğŸ·ï¸  Species: {', '.join(data.target_names)}\")\n",
    "print(f\"ğŸ“ Measurements: {', '.join(data.feature_names)}\")\n",
    "\n",
    "# Show the first few flowers\n",
    "print(\"\\nğŸ” First 5 flowers in our dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for any missing data\n",
    "print(f\"\\nâ“ Missing values: {df.isnull().sum().sum()}\")\n",
    "print(\"âœ… Dataset is clean and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Visualize the Data ğŸ“ˆ\n",
    "\n",
    "Let's create beautiful visualizations to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Create beautiful visualizations to understand our data\n",
    "print(\"ğŸ“ˆ Creating data visualizations...\")\n",
    "\n",
    "# Set up a 2x2 grid of plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸŒ¸ Iris Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Sepal measurements by species\n",
    "axes[0, 0].scatter(df['sepal length (cm)'], df['sepal width (cm)'], \n",
    "                  c=data.target, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0, 0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0, 0].set_ylabel('Sepal Width (cm)')\n",
    "axes[0, 0].set_title('Sepal Measurements by Species')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Petal measurements by species  \n",
    "axes[0, 1].scatter(df['petal length (cm)'], df['petal width (cm)'], \n",
    "                  c=data.target, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0, 1].set_xlabel('Petal Length (cm)')\n",
    "axes[0, 1].set_ylabel('Petal Width (cm)')\n",
    "axes[0, 1].set_title('Petal Measurements by Species')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of measurements\n",
    "df[data.feature_names].hist(bins=20, ax=axes[1, 0], alpha=0.7, color='skyblue')\n",
    "axes[1, 0].set_title('Distribution of All Measurements')\n",
    "\n",
    "# Plot 4: Species count\n",
    "species_counts = df['species'].value_counts()\n",
    "bars = axes[1, 1].bar(species_counts.index, species_counts.values, \n",
    "                     color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1, 1].set_title('Number of Flowers per Species')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Key observations:\")\n",
    "print(\"â€¢ Each species has distinct petal characteristics\")\n",
    "print(\"â€¢ Setosa has the smallest petals\")\n",
    "print(\"â€¢ Virginica has the largest petals\")\n",
    "print(\"â€¢ This should make classification easier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Prepare Data for Machine Learning ğŸ”§\n",
    "\n",
    "Now let's split our data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Prepare our data for training a machine learning model\n",
    "print(\"ğŸ”§ Preparing data for machine learning...\")\n",
    "\n",
    "# Separate features (measurements) from target (species)\n",
    "X = data.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = data.target  # Target: species (0=setosa, 1=versicolor, 2=virginica)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,      # Use 30% for testing\n",
    "    random_state=42,    # For reproducible results\n",
    "    stratify=y          # Ensure equal representation of each species\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“š Training set: {X_train.shape[0]} flowers\")\n",
    "print(f\"ğŸ§ª Testing set: {X_test.shape[0]} flowers\")\n",
    "print(f\"ğŸ“Š Features per flower: {X_train.shape[1]}\")\n",
    "\n",
    "# Show the split by species\n",
    "train_species = pd.Series(y_train).value_counts().sort_index()\n",
    "test_species = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "print(\"\\nğŸŒ¸ Training set by species:\")\n",
    "for i, species in enumerate(data.target_names):\n",
    "    print(f\"  {species}: {train_species[i]} flowers\")\n",
    "\n",
    "print(\"\\nğŸ§ª Testing set by species:\")\n",
    "for i, species in enumerate(data.target_names):\n",
    "    print(f\"  {species}: {test_species[i]} flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "## Step 5: Train the Machine Learning Model ğŸ¤–\n",
    "\n",
    "Time to create and train our AI model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Create and train our machine learning model\n",
    "print(\"ğŸ¤– Training the machine learning model...\")\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,    # Use 100 decision trees\n",
    "    random_state=42,     # For reproducible results\n",
    "    max_depth=3          # Prevent overfitting\n",
    ")\n",
    "\n",
    "# Train the model on our training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"ğŸ¯ Model trained successfully!\")\n",
    "print(f\"ğŸ“ˆ Accuracy on test set: {accuracy:.1%}\")\n",
    "\n",
    "# Show which features are most important\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ” Most important features for classification:\")\n",
    "for _, row in feature_importance.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "## Step 6: Evaluate Model Performance ğŸ“Š\n",
    "\n",
    "Let's see how well our model performs in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate how well our model performs\n",
    "print(\"ğŸ“Š Evaluating model performance...\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nğŸ“‹ Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "# Create a confusion matrix to see where the model makes mistakes\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=data.target_names, \n",
    "            yticklabels=data.target_names,\n",
    "            cbar_kws={'label': 'Number of Flowers'})\n",
    "plt.title('ğŸ¯ Confusion Matrix: Actual vs Predicted Species', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Species', fontsize=12)\n",
    "plt.ylabel('Actual Species', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-species accuracy\n",
    "print(\"\\nğŸŒ¸ Accuracy by species:\")\n",
    "for i, species in enumerate(data.target_names):\n",
    "    species_mask = y_test == i\n",
    "    if species_mask.sum() > 0:\n",
    "        species_accuracy = (y_pred[species_mask] == i).mean()\n",
    "        print(f\"  {species}: {species_accuracy:.1%}\")\n",
    "\n",
    "# Show any misclassifications\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "if len(misclassified) > 0:\n",
    "    print(f\"\\nâŒ Misclassified flowers: {len(misclassified)}\")\n",
    "    print(\"These are the flowers our model got wrong - let's learn from them!\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ Perfect classification! No mistakes on the test set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7"
   },
   "source": [
    "## Step 7: Make Predictions on New Flowers ğŸ”®\n",
    "\n",
    "Now for the exciting part - let's use our model to predict new flower species!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict"
   },
   "outputs": [],
   "source": [
    "# Use our trained model to predict new flower species\n",
    "print(\"ğŸ”® Making predictions on new flowers...\")\n",
    "\n",
    "# Create some example new flowers to classify\n",
    "new_flowers = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Small petals - likely Setosa\n",
    "    [6.2, 2.8, 4.8, 1.8],  # Large petals - likely Virginica  \n",
    "    [5.7, 2.8, 4.1, 1.3],  # Medium petals - likely Versicolor\n",
    "    [4.9, 3.1, 1.5, 0.1],  # Very small petals - likely Setosa\n",
    "    [7.2, 3.0, 5.8, 1.6]   # Very large petals - likely Virginica\n",
    "])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_flowers)\n",
    "probabilities = model.predict_proba(new_flowers)\n",
    "\n",
    "print(\"\\nğŸŒ¸ Prediction Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (flower, pred, prob) in enumerate(zip(new_flowers, predictions, probabilities)):\n",
    "    species = data.target_names[pred]\n",
    "    confidence = prob.max()\n",
    "    \n",
    "    print(f\"\\nğŸŒº Flower #{i+1}:\")\n",
    "    print(f\"   Measurements: {flower}\")\n",
    "    print(f\"   Predicted species: {species}\")\n",
    "    print(f\"   Confidence: {confidence:.1%}\")\n",
    "    \n",
    "    # Show probability for each species\n",
    "    print(\"   Probabilities:\")\n",
    "    for j, (species_name, probability) in enumerate(zip(data.target_names, prob)):\n",
    "        emoji = \"ğŸ¯\" if j == pred else \"  \"\n",
    "        print(f\"     {emoji} {species_name}: {probability:.1%}\")\n",
    "\n",
    "print(\"\\nâœ¨ Amazing! Your model can now identify iris species from measurements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_features"
   },
   "source": [
    "## ğŸš€ Google Colab Special Features\n",
    "\n",
    "Since you're using Colab, let's explore some unique features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available (Colab's superpower!)\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"ğŸ”¥ GPU available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ® GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(\"ğŸ’¡ Tip: Use Runtime â†’ Change runtime type â†’ GPU for faster training!\")\n",
    "    else:\n",
    "        print(\"ğŸ’¡ Tip: Enable GPU in Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ PyTorch not installed, but that's okay for this example!\")\n",
    "\n",
    "# Show system information\n",
    "import platform\n",
    "print(f\"\\nğŸ’» System: {platform.system()}\")\n",
    "print(f\"ğŸ Python version: {platform.python_version()}\")\n",
    "print(f\"ğŸ“ You're running this in the cloud! â˜ï¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save your model to Google Drive (optional)\n",
    "print(\"ğŸ’¾ Want to save your model? Uncomment the code below:\")\n",
    "print(\"\")\n",
    "\n",
    "# Uncomment these lines to save to Google Drive:\n",
    "# from google.colab import drive\n",
    "# import joblib\n",
    "# \n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Save the model\n",
    "# joblib.dump(model, '/content/drive/MyDrive/iris_classifier.pkl')\n",
    "# print(\"âœ… Model saved to your Google Drive!\")\n",
    "\n",
    "print(\"ğŸ”— Sharing tip: Click 'Share' button (top right) to send this notebook to friends!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully built your first machine learning model in Google Colab!\n",
    "\n",
    "### What you accomplished:\n",
    "âœ… Loaded and explored a dataset of 150 flowers  \n",
    "âœ… Visualized data patterns across 4 features  \n",
    "âœ… Trained a Random Forest model with 100 trees  \n",
    "âœ… Achieved 95%+ accuracy on unseen data  \n",
    "âœ… Made predictions on new flower measurements  \n",
    "\n",
    "### Key ML concepts you learned:\n",
    "ğŸ§  Data loading and exploration  \n",
    "ğŸ§  Data visualization and pattern recognition  \n",
    "ğŸ§  Train/test split for model evaluation  \n",
    "ğŸ§  Model training and prediction  \n",
    "ğŸ§  Performance evaluation and interpretation  \n",
    "\n",
    "### Next steps:\n",
    "ğŸš€ Try different algorithms (SVM, Neural Networks)  \n",
    "ğŸš€ Work with larger, more complex datasets  \n",
    "ğŸš€ Learn feature engineering and data preprocessing  \n",
    "ğŸš€ Build models for regression and clustering  \n",
    "\n",
    "### Resources:\n",
    "ğŸ“š [Complete ML Guide](../04-first-ml-example.md)  \n",
    "ğŸ“š [Next Steps](../05-next-steps.md)  \n",
    "ğŸ“š [Jupyter Version](jupyter-sample.ipynb)  \n",
    "ğŸ“š [Python Script Version](python-sample.py)  \n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The same code works in Jupyter and Python IDEs too! You now have transferable skills across all ML environments. ğŸŒŸ\n",
    "\n",
    "**Happy learning!** ğŸ“âœ¨"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
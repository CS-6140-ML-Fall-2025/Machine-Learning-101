{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master DataFrame operations for real-world ML data preprocessing\n",
    "- Clean, transform, and engineer features from messy datasets\n",
    "- Handle missing values, outliers, and categorical data for ML pipelines\n",
    "- Prepare clean, ML-ready datasets and convert to NumPy arrays for modeling\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals (complete NumPy notebook first)\n",
    "\n",
    "**Estimated Time:** ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is the essential library for working with structured data in Python and forms the data preprocessing backbone of most ML projects. This notebook focuses on practical, hands-on examples that prepare you for real-world ML workflows.\n",
    "\n",
    "**Why Pandas for ML?** Most real-world data comes as CSV files, databases, or APIs - not clean NumPy arrays. Pandas bridges this gap by:\n",
    "- Loading data from various sources (CSV, Excel, databases, APIs)\n",
    "- Handling mixed data types (numbers, text, dates) in a single structure\n",
    "- Providing powerful tools for data cleaning and transformation\n",
    "- Seamlessly converting to NumPy arrays for ML algorithms\n",
    "\n",
    "**Learning Path Connection:** This notebook builds on NumPy concepts (arrays, indexing, broadcasting) and prepares clean datasets for the scikit-learn notebook where you'll build ML models.\n",
    "\n",
    "**What You'll Build:** By the end, you'll transform a messy real-world dataset into a clean, ML-ready format - exactly what you do in every ML project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.513770Z",
     "start_time": "2025-09-11T05:57:23.270384Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PARAMETER EXPLANATION: Random seed (same as NumPy notebook!)\n",
    "# • What it does: Makes random number generation predictable and reproducible\n",
    "# • Why seed=42: Reference to \"The Hitchhiker's Guide to the Galaxy\" (the answer to everything!)\n",
    "# • ML importance: Essential for reproducible experiments and consistent results\n",
    "# • Data science workflow: Always set seed when generating synthetic data\n",
    "# • Debugging: Helps isolate issues by ensuring same 'random' data every time\n",
    "# • Alternative values: Any integer works (0, 123, 2024, your birthday, etc.)\n",
    "np.random.seed(42)\n",
    "\n",
    "# PARAMETER EXPLANATION: Pandas display options\n",
    "# • max_columns: Maximum number of columns to display before truncating\n",
    "# • max_rows: Maximum number of rows to display before truncating  \n",
    "# • Why limit: Prevents overwhelming output with large DataFrames\n",
    "# • Jupyter tip: Keeps notebook output clean and readable\n",
    "# • Can also use: pd.set_option('display.width', 1000) for wider display\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Random seed set to 42 - our synthetic data will be reproducible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrame Creation and Basic Operations\n",
    "\n",
    "Understanding how to create and manipulate DataFrames is fundamental to ML data preprocessing. While NumPy arrays are great for numerical computations, real-world data is messy and mixed - that's where Pandas shines!\n",
    "\n",
    "**Key Difference from NumPy:**\n",
    "- **NumPy arrays**: Homogeneous data (all same type), great for math operations\n",
    "- **Pandas DataFrames**: Heterogeneous data (mixed types), great for real-world datasets\n",
    "- **Under the hood**: DataFrames use NumPy arrays for each column!\n",
    "\n",
    "**ML Context:** Most ML projects start with loading CSV files, database tables, or API responses into DataFrames for cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.529643Z",
     "start_time": "2025-09-11T05:57:23.518444Z"
    }
   },
   "outputs": [],
   "source": [
    "# CREATING A REALISTIC ML DATASET\n",
    "# This simulates what you'd get from a CSV file or database in a real ML project\n",
    "print(\"Building a Customer Analytics Dataset (typical ML scenario)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)  # For reproducible results (remember this from NumPy!)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data with mixed data types (like real datasets)\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),                    # Integer IDs\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),    # Numerical (continuous)\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),         # Numerical (skewed)\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                 n_samples, p=[0.3, 0.4, 0.2, 0.1]),  # Categorical (ordinal)\n",
    "    'experience_years': np.random.exponential(5, n_samples),   # Numerical (continuous)\n",
    "    'num_purchases': np.random.poisson(3, n_samples),          # Numerical (count)\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),  # Numerical (bounded)\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Binary target\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),  # Categorical\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_samples, freq='D')[:n_samples]  # Datetime\n",
    "}\n",
    "\n",
    "# Create DataFrame (this is where Pandas shines vs NumPy!)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "print(\"Adding realistic data quality issues (missing values, like real datasets)...\")\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan           # Missing income data\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan  # Missing satisfaction scores\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape} (samples, features)\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Data types: {len(df.dtypes.unique())} different types (mixed data!)\")\n",
    "\n",
    "print(\"\\nThis is exactly what real ML datasets look like:\")\n",
    "print(\"• Mixed data types (numbers, text, dates)\")\n",
    "print(\"• Missing values that need handling\")\n",
    "print(\"• Different scales and distributions\")\n",
    "print(\"• Categorical variables needing encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.550257Z",
     "start_time": "2025-09-11T05:57:23.538021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic DataFrame information (essential for ML)\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Analysis\n",
    "\n",
    "Understanding your data is crucial before building ML models. This is where you discover patterns, identify problems, and make decisions about preprocessing. In the NumPy notebook, we worked with clean arrays - here we deal with real-world messiness!\n",
    "\n",
    "**ML Exploration Checklist:**\n",
    "1. **Data types**: What kind of features do we have?\n",
    "2. **Missing values**: How much data is missing and why?\n",
    "3. **Distributions**: Are features normally distributed or skewed?\n",
    "4. **Correlations**: Which features relate to our target variable?\n",
    "5. **Outliers**: Are there extreme values that need handling?\n",
    "\n",
    "**Connection to NumPy:** Remember the statistical operations from NumPy? Pandas makes them easier with labeled data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.563801Z",
     "start_time": "2025-09-11T05:57:23.559650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorical data analysis\n",
    "print(\"Categorical Data Analysis:\")\n",
    "\n",
    "# Value counts for categorical features\n",
    "print(\"Education distribution:\")\n",
    "print(df['education'].value_counts())\n",
    "\n",
    "# PARAMETER EXPLANATION: normalize=True/False in value_counts()\n",
    "print(\"\\nPARAMETER EXPLANATION: normalize parameter\")\n",
    "print(\"• What it does: Controls whether to return counts or proportions\")\n",
    "print(\"• normalize=False (default): Returns raw counts\")\n",
    "print(\"• normalize=True: Returns proportions (values sum to 1.0)\")\n",
    "print(\"• ML usage: Proportions help understand class balance and distribution\")\n",
    "print(\"• Tip: Multiply by 100 to get percentages\")\n",
    "print(\"• Why useful: Easier to compare distributions across different sized datasets\")\n",
    "\n",
    "print(\"\\nEducation percentages:\")\n",
    "proportions = df['education'].value_counts(normalize=True)\n",
    "percentages = proportions * 100\n",
    "print(f\"Raw proportions (sum={proportions.sum():.1f}):\")\n",
    "print(proportions)\n",
    "print(f\"\\nAs percentages:\")\n",
    "print(percentages)\n",
    "\n",
    "print(\"\\nRegion distribution:\")\n",
    "print(df['region'].value_counts())\n",
    "\n",
    "print(\"\\nPremium customers:\")\n",
    "print(df['is_premium'].value_counts())\n",
    "print(f\"Premium rate: {df['is_premium'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.579509Z",
     "start_time": "2025-09-11T05:57:23.574950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical data analysis\n",
    "print(\"Numerical Data Analysis:\")\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
    "\n",
    "# Correlation analysis (important for feature selection)\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"\\nCorrelation with target (is_premium):\")\n",
    "target_corr = correlation_matrix['is_premium'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Identify highly correlated features (multicollinearity)\n",
    "print(\"\\nHighly correlated feature pairs (|correlation| > 0.5):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "\n",
    "for col1, col2, corr in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.598940Z",
     "start_time": "2025-09-11T05:57:23.592028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Groupby analysis (understanding patterns)\n",
    "print(\"Group Analysis:\")\n",
    "\n",
    "# Analyze by education level\n",
    "education_analysis = df.groupby('education').agg({\n",
    "    'age': ['mean', 'std'],\n",
    "    'income': ['mean', 'median'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Analysis by Education Level:\")\n",
    "print(education_analysis)\n",
    "\n",
    "# Analyze by region\n",
    "print(\"\\nAnalysis by Region:\")\n",
    "region_analysis = df.groupby('region').agg({\n",
    "    'income': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'satisfaction_score': 'mean'\n",
    "}).round(2)\n",
    "print(region_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Essential steps before feeding data to ML models. This is where Pandas really shines - handling the messy, real-world data issues that would be painful with pure NumPy arrays.\n",
    "\n",
    "**Why Cleaning Matters for ML:**\n",
    "- **Garbage in, garbage out**: Poor data quality leads to poor model performance\n",
    "- **Algorithm assumptions**: Many ML algorithms assume clean, complete data\n",
    "- **Feature consistency**: Models need consistent data types and scales\n",
    "- **Missing data**: Can break training or lead to biased predictions\n",
    "\n",
    "**NumPy Connection:** Remember how we had to be careful about array shapes and dtypes? Pandas makes this easier with automatic type inference and flexible operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.613480Z",
     "start_time": "2025-09-11T05:57:23.608501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# PARAMETER DEEP DIVE: inplace parameter\n",
    "print(\"\\nPARAMETER EXPLANATION: inplace=True/False\")\n",
    "print(\"• What it does: Controls whether to modify the original DataFrame or return a new one\")\n",
    "print(\"• inplace=True: Modifies the original DataFrame directly (no return value)\")\n",
    "print(\"• inplace=False (default): Returns a new DataFrame, leaves original unchanged\")\n",
    "print(\"• Memory impact: inplace=True saves memory by not creating copies\")\n",
    "print(\"• Safety: inplace=False is safer - you can always go back to original data\")\n",
    "print(\"• ML workflow: Often use inplace=True after confirming operations are correct\")\n",
    "print(\"• Common mistake: Forgetting to assign result when inplace=False\")\n",
    "\n",
    "# Demonstrate the difference\n",
    "print(\"\\nDemonstration:\")\n",
    "demo_series = pd.Series([1, None, 3, None, 5])\n",
    "print(f\"Original: {demo_series.tolist()}\")\n",
    "\n",
    "# Method 1: inplace=False (default)\n",
    "filled_copy = demo_series.fillna(999)  # Returns new Series\n",
    "print(f\"After fillna(999): Original still {demo_series.tolist()}\")\n",
    "print(f\"New series: {filled_copy.tolist()}\")\n",
    "\n",
    "# Method 2: inplace=True\n",
    "demo_series.fillna(999, inplace=True)  # Modifies original\n",
    "print(f\"After fillna(999, inplace=True): Original now {demo_series.tolist()}\")\n",
    "\n",
    "# Strategy 1: Fill numerical missing values with median\n",
    "df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
    "df_clean['satisfaction_score'].fillna(df_clean['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Alternative strategies\n",
    "print(\"\\nAlternative missing value strategies:\")\n",
    "print(\"1. Forward fill: df.fillna(method='ffill')\")\n",
    "print(\"2. Backward fill: df.fillna(method='bfill')\")\n",
    "print(\"3. Interpolation: df.interpolate()\")\n",
    "print(\"4. Drop rows: df.dropna()\")\n",
    "print(\"5. Drop columns: df.dropna(axis=1)\")\n",
    "\n",
    "# PARAMETER DEEP DIVE: axis parameter in Pandas\n",
    "print(\"\\nPARAMETER EXPLANATION: axis parameter\")\n",
    "print(\"• What it controls: Which dimension to operate along\")\n",
    "print(\"• axis=0 or axis='index': Operate along ROWS (down the DataFrame)\")\n",
    "print(\"• axis=1 or axis='columns': Operate along COLUMNS (across the DataFrame)\")\n",
    "print(\"• Memory tip: axis=0 affects rows, axis=1 affects columns\")\n",
    "print(\"• ML context: axis=0 for sample-wise operations, axis=1 for feature-wise\")\n",
    "print(\"• Connection to NumPy: Same concept as NumPy axis parameter!\")\n",
    "\n",
    "# Demonstrate axis parameter\n",
    "demo_df = pd.DataFrame({\n",
    "    'A': [1, 2, None],\n",
    "    'B': [None, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "print(\"\\nDemo DataFrame:\")\n",
    "print(demo_df)\n",
    "print(f\"\\nMissing values per column (axis=0): \\n{demo_df.isnull().sum(axis=0)}\")\n",
    "print(f\"\\nMissing values per row (axis=1): \\n{demo_df.isnull().sum(axis=1)}\")\n",
    "print(f\"\\nDrop rows with any NaN (axis=0): \\n{demo_df.dropna(axis=0)}\")\n",
    "print(f\"\\nDrop columns with any NaN (axis=1): \\n{demo_df.dropna(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.649668Z",
     "start_time": "2025-09-11T05:57:23.640714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "print(\"Outlier Detection and Handling:\")\n",
    "\n",
    "# PARAMETER EXPLANATION: quantile() parameter\n",
    "print(\"\\nPARAMETER EXPLANATION: quantile parameter\")\n",
    "print(\"• What it does: Returns the value at a specific percentile of the data\")\n",
    "print(\"• Range: 0.0 to 1.0 (0% to 100%)\")\n",
    "print(\"• Common values:\")\n",
    "print(\"  - 0.25 = 25th percentile (Q1, first quartile)\")\n",
    "print(\"  - 0.50 = 50th percentile (median, Q2)\")\n",
    "print(\"  - 0.75 = 75th percentile (Q3, third quartile)\")\n",
    "print(\"  - 0.95 = 95th percentile (common outlier threshold)\")\n",
    "print(\"• ML usage: Outlier detection, data capping, understanding distributions\")\n",
    "print(\"• IQR method: Uses Q1 and Q3 to identify outliers\")\n",
    "print(\"• Why 0.95: Keeps 95% of data, removes extreme 5%\")\n",
    "\n",
    "# Demonstrate quantiles\n",
    "demo_data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])  # Note the outlier 100\n",
    "print(\"\\nDemo with data [1,2,3,4,5,6,7,8,9,100]:\")\n",
    "print(f\"25th percentile (Q1): {demo_data.quantile(0.25)}\")\n",
    "print(f\"50th percentile (median): {demo_data.quantile(0.50)}\")\n",
    "print(f\"75th percentile (Q3): {demo_data.quantile(0.75)}\")\n",
    "print(f\"95th percentile: {demo_data.quantile(0.95)}\")\n",
    "print(f\"Note: 95th percentile (9.5) would cap the outlier 100\")\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "# Check for outliers in income\n",
    "income_outliers = detect_outliers_iqr(df_clean['income'])\n",
    "print(f\"\\nIncome outliers: {income_outliers.sum()} ({income_outliers.mean():.1%})\")\n",
    "\n",
    "# Visualize outliers\n",
    "print(\"Income statistics:\")\n",
    "print(f\"Mean: ${df_clean['income'].mean():.0f}\")\n",
    "print(f\"Median: ${df_clean['income'].median():.0f}\")\n",
    "print(f\"95th percentile: ${df_clean['income'].quantile(0.95):.0f}\")\n",
    "print(f\"99th percentile: ${df_clean['income'].quantile(0.99):.0f}\")\n",
    "print(f\"Max: ${df_clean['income'].max():.0f}\")\n",
    "\n",
    "# Handle outliers (cap at 95th percentile)\n",
    "income_cap = df_clean['income'].quantile(0.95)\n",
    "\n",
    "# PARAMETER EXPLANATION: clip() upper and lower parameters\n",
    "print(\"\\nPARAMETER EXPLANATION: clip() function\")\n",
    "print(\"• What it does: Limits values to a specified range\")\n",
    "print(\"• upper parameter: Sets maximum allowed value (clips values above this)\")\n",
    "print(\"• lower parameter: Sets minimum allowed value (clips values below this)\")\n",
    "print(\"• ML usage: Handle outliers without removing data points\")\n",
    "print(\"• Alternative to: Removing outliers entirely (preserves sample size)\")\n",
    "print(\"• Example: clip(lower=0, upper=100) keeps values between 0 and 100\")\n",
    "\n",
    "# Demonstrate clip function\n",
    "demo_values = pd.Series([-5, 2, 8, 15, 25, 150])\n",
    "print(f\"\\nDemo values: {demo_values.tolist()}\")\n",
    "print(f\"After clip(upper=20): {demo_values.clip(upper=20).tolist()}\")\n",
    "print(f\"After clip(lower=0, upper=20): {demo_values.clip(lower=0, upper=20).tolist()}\")\n",
    "\n",
    "df_clean['income_capped'] = df_clean['income'].clip(upper=income_cap)\n",
    "\n",
    "print(\"\\nAfter capping at 95th percentile:\")\n",
    "print(f\"Max income: ${df_clean['income_capped'].max():.0f}\")\n",
    "print(f\"Values capped: {(df_clean['income'] > income_cap).sum()} out of {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.666587Z",
     "start_time": "2025-09-11T05:57:23.658915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data type optimization (important for large datasets)\n",
    "print(\"Data Type Optimization:\")\n",
    "print(f\"Memory usage before optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize integer columns\n",
    "int_cols = df_clean.select_dtypes(include=['int64']).columns\n",
    "for col in int_cols:\n",
    "    if col != 'customer_id':  # Keep ID as int64\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], downcast='integer')\n",
    "\n",
    "# Optimize float columns\n",
    "float_cols = df_clean.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], downcast='float')\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "categorical_cols = ['education', 'region']\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "print(f\"Memory usage after optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {(1 - df_clean.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creating new features that can improve ML model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.683678Z",
     "start_time": "2025-09-11T05:57:23.675379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature engineering examples\n",
    "print(\"Feature Engineering:\")\n",
    "\n",
    "# 1. Binning continuous variables\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'],\n",
    "                              bins=[0, 25, 35, 50, 100],\n",
    "                              labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "df_clean['income_tier'] = pd.qcut(df_clean['income_capped'],\n",
    "                                 q=4,\n",
    "                                 labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Age group distribution:\")\n",
    "print(df_clean['age_group'].value_counts())\n",
    "\n",
    "print(\"\\nIncome tier distribution:\")\n",
    "print(df_clean['income_tier'].value_counts())\n",
    "\n",
    "# 2. Mathematical transformations\n",
    "# PARAMETER EXPLANATION: np.log1p() function\n",
    "# • What it does: Calculates log(1 + x) more accurately than np.log(1 + x)\n",
    "# • Why log1p: Avoids numerical precision issues when x is close to zero\n",
    "# • ML usage: Log transforms reduce skewness in income/price data\n",
    "# • Why +1: Handles zero values (log(0) is undefined, but log(1+0) = 0)\n",
    "# • Alternative: np.log(df_clean['income_capped'] + 1) but less precise\n",
    "df_clean['log_income'] = np.log1p(df_clean['income_capped'])  # log(1+x) to handle zeros\n",
    "\n",
    "# PARAMETER EXPLANATION: Adding +1 in denominators\n",
    "# • Why +1: Prevents division by zero errors\n",
    "# • Example: If num_purchases=0, division by (0+1)=1 instead of crashing\n",
    "# • ML safety: Ensures robust feature engineering without runtime errors\n",
    "# • Alternative: Use np.where() to handle zero cases explicitly\n",
    "df_clean['income_per_purchase'] = df_clean['income_capped'] / (df_clean['num_purchases'] + 1)\n",
    "df_clean['satisfaction_squared'] = df_clean['satisfaction_score'] ** 2\n",
    "\n",
    "# 3. Date-based features\n",
    "# PARAMETER EXPLANATION: .dt accessor\n",
    "# • What it is: Special accessor for datetime operations on Series\n",
    "# • Usage: Only works on datetime-type Series (not regular objects)\n",
    "# • Common attributes: .year, .month, .day, .dayofweek, .quarter\n",
    "# • ML usage: Extract time-based features from timestamps\n",
    "# • dayofweek: 0=Monday, 1=Tuesday, ..., 6=Sunday\n",
    "# • Why useful: Captures seasonal patterns and cyclical behavior\n",
    "# • Error prevention: Will fail if Series is not datetime type\n",
    "df_clean['signup_year'] = df_clean['signup_date'].dt.year\n",
    "df_clean['signup_month'] = df_clean['signup_date'].dt.month\n",
    "df_clean['signup_dayofweek'] = df_clean['signup_date'].dt.dayofweek\n",
    "df_clean['days_since_signup'] = (datetime.now() - df_clean['signup_date']).dt.days\n",
    "\n",
    "print(\"\\nDatetime feature examples:\")\n",
    "print(f\"Sample signup_date: {df_clean['signup_date'].iloc[0]}\")\n",
    "print(f\"Extracted year: {df_clean['signup_year'].iloc[0]}\")\n",
    "print(f\"Extracted month: {df_clean['signup_month'].iloc[0]}\")\n",
    "print(f\"Day of week (0=Mon): {df_clean['signup_dayofweek'].iloc[0]}\")"\n",
    "\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = ['age_group', 'income_tier', 'log_income', 'income_per_purchase',\n",
    "                'satisfaction_squared', 'signup_year', 'signup_month', 'days_since_signup']\n",
    "print(df_clean[new_features].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.699471Z",
     "start_time": "2025-09-11T05:57:23.692049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Interaction features\n",
    "print(\"Interaction Features:\")\n",
    "\n",
    "# Create interaction between important features\n",
    "df_clean['age_income_interaction'] = df_clean['age'] * df_clean['log_income']\n",
    "df_clean['experience_satisfaction'] = df_clean['experience_years'] * df_clean['satisfaction_score']\n",
    "\n",
    "# Boolean combinations\n",
    "df_clean['high_income_high_satisfaction'] = (\n",
    "    (df_clean['income_tier'] == 'Very High') &\n",
    "    (df_clean['satisfaction_score'] > 4)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['experienced_premium'] = (\n",
    "    (df_clean['experience_years'] > 5) &\n",
    "    (df_clean['is_premium'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "print(\"Interaction features:\")\n",
    "interaction_features = ['age_income_interaction', 'experience_satisfaction',\n",
    "                       'high_income_high_satisfaction', 'experienced_premium']\n",
    "print(df_clean[interaction_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.717235Z",
     "start_time": "2025-09-11T05:57:23.708218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregation features (useful for time series or grouped data)\n",
    "print(\"Aggregation Features:\")\n",
    "\n",
    "# Features based on region\n",
    "region_stats = df_clean.groupby('region').agg({\n",
    "    'income_capped': ['mean', 'std'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "region_stats.columns = ['_'.join(col).strip() for col in region_stats.columns]\n",
    "region_stats = region_stats.add_prefix('region_')\n",
    "\n",
    "# Merge back to main dataframe\n",
    "df_clean = df_clean.merge(region_stats, left_on='region', right_index=True, how='left')\n",
    "\n",
    "print(\"Region-based features:\")\n",
    "region_features = [col for col in df_clean.columns if col.startswith('region_')]\n",
    "print(df_clean[['region'] + region_features].head())\n",
    "\n",
    "# Relative features (compare individual to group)\n",
    "df_clean['income_vs_region_mean'] = df_clean['income_capped'] / df_clean['region_income_capped_mean']\n",
    "df_clean['satisfaction_vs_region_mean'] = df_clean['satisfaction_score'] / df_clean['region_satisfaction_score_mean']\n",
    "\n",
    "print(\"\\nRelative features:\")\n",
    "print(df_clean[['income_vs_region_mean', 'satisfaction_vs_region_mean']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "Converting categorical variables to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:23.734561Z",
     "start_time": "2025-09-11T05:57:23.728206Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "print(\"One-Hot Encoding:\")\n",
    "\n",
    "# Select categorical columns for encoding\n",
    "categorical_cols = ['education', 'region', 'age_group', 'income_tier']\n",
    "\n",
    "# PARAMETER EXPLANATION: drop_first=True/False in get_dummies()\n",
    "print(\"\\nPARAMETER EXPLANATION: drop_first parameter\")\n",
    "print(\"• What it does: Controls whether to drop the first dummy variable\")\n",
    "print(\"• drop_first=False (default): Creates dummy for every category\")\n",
    "print(\"• drop_first=True: Drops first category to avoid multicollinearity\")\n",
    "print(\"• Why drop_first=True: Prevents 'dummy variable trap' in linear models\")\n",
    "print(\"• ML impact: Linear regression can fail with perfect multicollinearity\")\n",
    "print(\"• Example: For [A, B, C], creates only B and C columns (A is implied when both are 0)\")\n",
    "print(\"• Trade-off: Saves memory and prevents multicollinearity vs. interpretability\")\n",
    "\n",
    "# Demonstrate the difference\n",
    "demo_data = pd.DataFrame({'category': ['A', 'B', 'C', 'A', 'B']})\n",
    "print(\"\\nDemo with small dataset:\")\n",
    "print(\"Original:\", demo_data['category'].tolist())\n",
    "print(\"\\nWith drop_first=False:\")\n",
    "print(pd.get_dummies(demo_data['category'], drop_first=False))\n",
    "print(\"\\nWith drop_first=True:\")\n",
    "print(pd.get_dummies(demo_data['category'], drop_first=True))\n",
    "print(\"Note: Category 'A' is implied when both B and C are 0\")\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nShape before encoding: {df_clean.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Show new columns\n",
    "new_cols = [col for col in df_encoded.columns if any(cat in col for cat in categorical_cols)]\n",
    "print(f\"\\nNew encoded columns ({len(new_cols)}):\")\n",
    "for col in new_cols[:10]:  # Show first 10\n",
    "    print(f\"  {col}\")\n",
    "if len(new_cols) > 10:\n",
    "    print(f\"  ... and {len(new_cols) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.288633Z",
     "start_time": "2025-09-11T05:57:23.737753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label encoding (for ordinal variables)\n",
    "print(\"Label Encoding:\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy for label encoding\n",
    "df_label_encoded = df_clean.copy()\n",
    "\n",
    "# Education has natural ordering\n",
    "education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_label_encoded['education_encoded'] = df_label_encoded['education'].map(education_order)\n",
    "\n",
    "# For non-ordinal categories, use LabelEncoder\n",
    "le_region = LabelEncoder()\n",
    "df_label_encoded['region_encoded'] = le_region.fit_transform(df_label_encoded['region'])\n",
    "\n",
    "print(\"Education encoding:\")\n",
    "print(df_label_encoded[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "print(\"\\nRegion encoding:\")\n",
    "print(df_label_encoded[['region', 'region_encoded']].drop_duplicates().sort_values('region_encoded'))\n",
    "\n",
    "# Show encoding mapping\n",
    "print(\"\\nRegion encoding mapping:\")\n",
    "for i, region in enumerate(le_region.classes_):\n",
    "    print(f\"  {region}: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.297140Z",
     "start_time": "2025-09-11T05:57:24.291607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target encoding (advanced technique)\n",
    "print(\"Target Encoding:\")\n",
    "\n",
    "# Calculate mean target value for each category\n",
    "def target_encode(df, categorical_col, target_col, smoothing=1):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing to prevent overfitting\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = df[target_col].mean()\n",
    "\n",
    "    # Calculate category means and counts\n",
    "    category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
    "\n",
    "    # Apply smoothing\n",
    "    smoothed_means = (\n",
    "        (category_stats['mean'] * category_stats['count'] + global_mean * smoothing) /\n",
    "        (category_stats['count'] + smoothing)\n",
    "    )\n",
    "\n",
    "    return smoothed_means\n",
    "\n",
    "# Target encode education based on premium rate\n",
    "education_target_encoding = target_encode(df_clean, 'education', 'is_premium')\n",
    "df_clean['education_target_encoded'] = df_clean['education'].map(education_target_encoding)\n",
    "\n",
    "print(\"Education target encoding (premium rate):\")\n",
    "print(education_target_encoding.sort_values(ascending=False))\n",
    "\n",
    "# Target encode region\n",
    "region_target_encoding = target_encode(df_clean, 'region', 'is_premium')\n",
    "df_clean['region_target_encoded'] = df_clean['region'].map(region_target_encoding)\n",
    "\n",
    "print(\"\\nRegion target encoding (premium rate):\")\n",
    "print(region_target_encoding.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Normalization\n",
    "\n",
    "Preparing numerical features for ML algorithms that are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.326245Z",
     "start_time": "2025-09-11T05:57:24.307473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"Feature Scaling:\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['age', 'income_capped', 'experience_years', 'satisfaction_score',\n",
    "                     'log_income', 'days_since_signup']\n",
    "\n",
    "print(\"Original feature statistics:\")\n",
    "print(df_clean[numerical_features].describe())\n",
    "\n",
    "# Standard scaling (z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard_scaled = df_clean.copy()\n",
    "df_standard_scaled[numerical_features] = scaler_standard.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
    "print(df_standard_scaled[numerical_features].describe())\n",
    "\n",
    "# Min-Max scaling (0-1 range)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax_scaled = df_clean.copy()\n",
    "df_minmax_scaled[numerical_features] = scaler_minmax.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Min-Max Scaling (range 0-1):\")\n",
    "print(df_minmax_scaled[numerical_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.346729Z",
     "start_time": "2025-09-11T05:57:24.335082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Robust scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust_scaled = df_clean.copy()\n",
    "df_robust_scaled[numerical_features] = scaler_robust.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"After Robust Scaling (median=0, IQR=1):\")\n",
    "print(df_robust_scaled[numerical_features].describe())\n",
    "\n",
    "# Compare scaling methods visually\n",
    "print(\"\\nScaling Comparison for 'income_capped':\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df_clean['income_capped'],\n",
    "    'Standard': df_standard_scaled['income_capped'],\n",
    "    'MinMax': df_minmax_scaled['income_capped'],\n",
    "    'Robust': df_robust_scaled['income_capped']\n",
    "})\n",
    "\n",
    "print(comparison_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting and Sampling\n",
    "\n",
    "Preparing data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.427607Z",
     "start_time": "2025-09-11T05:57:24.356531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "print(\"Data Splitting:\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df_encoded.columns\n",
    "                  if col not in ['customer_id', 'is_premium', 'signup_date', 'education', 'region']]\n",
    "\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['is_premium']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation (80% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {X_train.shape[0]} ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Validation: {X_val.shape[0]} ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test: {X_test.shape[0]} ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(f\"Train: {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.446305Z",
     "start_time": "2025-09-11T05:57:24.440209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handling imbalanced data\n",
    "print(\"Handling Imbalanced Data:\")\n",
    "\n",
    "# Check class imbalance\n",
    "class_counts = y_train.value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Class distribution: {class_counts.to_dict()}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 2:  # If significantly imbalanced\n",
    "    print(\"\\nDataset is imbalanced. Strategies to consider:\")\n",
    "\n",
    "    # 1. Undersampling majority class\n",
    "    majority_class = y_train.value_counts().index[0]\n",
    "    minority_class = y_train.value_counts().index[1]\n",
    "\n",
    "    majority_indices = y_train[y_train == majority_class].index\n",
    "    minority_indices = y_train[y_train == minority_class].index\n",
    "\n",
    "    # Random undersample majority class\n",
    "    undersampled_majority = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "    balanced_indices = np.concatenate([undersampled_majority, minority_indices])\n",
    "\n",
    "    X_train_balanced = X_train.loc[balanced_indices]\n",
    "    y_train_balanced = y_train.loc[balanced_indices]\n",
    "\n",
    "    print(f\"1. Undersampling - New size: {len(X_train_balanced)}\")\n",
    "    print(f\"   New distribution: {y_train_balanced.value_counts().to_dict()}\")\n",
    "\n",
    "    # 2. Class weights (for algorithms that support it)\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights, strict=False))\n",
    "\n",
    "    print(f\"2. Class weights: {class_weight_dict}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset is reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Converting cleaned data for other tools (framework-neutral)\n",
    "\n",
    "After cleaning and encoding, you will often convert DataFrame features to NumPy arrays. The guidance below is framework-neutral and focuses on checks and formats most tools expect.\n",
    "\n",
    "- Use numeric dtypes for features (float32 is common for inputs)\n",
    "- Use integer dtypes for labels (int32/int64 depending on the tool)\n",
    "- Verify shapes: (n_samples, n_features) for feature matrices\n",
    "- Check for NaNs and infinite values before exporting\n",
    "\n",
    "Example: convert train/val/test splits to NumPy and save them for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T05:57:24.461688Z",
     "start_time": "2025-09-11T05:57:24.456749Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Converting DataFrame to NumPy arrays (neutral):\")\n",
    "\n",
    "# Convert only numeric columns to NumPy arrays (ensure appropriate dtype)\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_train_np = X_train[numeric_columns].values.astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.int64)\n",
    "\n",
    "X_val_np = X_val[numeric_columns].values.astype(np.float32)\n",
    "y_val_np = y_val.values.astype(np.int64)\n",
    "\n",
    "X_test_np = X_test[numeric_columns].values.astype(np.float32)\n",
    "y_test_np = y_test.values.astype(np.int64)\n",
    "\n",
    "print(f\"Training data shape: {X_train_np.shape}\")\n",
    "print(f\"Training data dtype: {X_train_np.dtype}\")\n",
    "print(f\"Training labels dtype: {y_train_np.dtype}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\" - Save these NumPy arrays to disk for reproducibility and to reuse in other tools\")\n",
    "print(\" - Save preprocessing objects (scalers, encoders) so the same transforms are applied in production\")\n",
    "\n",
    "# Example saving (commented out so the notebook can run without writing files)\n",
    "# np.save('data/processed/X_train.npy', X_train_np)\n",
    "# np.save('data/processed/y_train.npy', y_train_np)\n",
    "# np.save('data/processed/X_val.npy', X_val_np)\n",
    "# np.save('data/processed/y_val.npy', y_val_np)\n",
    "# np.save('data/processed/X_test.npy', X_test_np)\n",
    "# np.save('data/processed/y_test.npy', y_test_np)\n",
    "\n",
    "# Save feature names and preprocessing info\n",
    "# import pickle\n",
    "# with open('data/processed/preprocessing_info.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessing_summary, f)\n",
    "\n",
    "# Save scalers for future use\n",
    "# with open('data/processed/scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler_standard, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways (updated)\n",
    "\n",
    "**What we've learned (short):**\n",
    "- Explore data with .info(), .describe(), and .value_counts()\n",
    "- Handle missing values and outliers with straightforward strategies\n",
    "- Create new features and encode categorical variables\n",
    "- Scale numeric features and split data into train/val/test\n",
    "- Convert cleaned DataFrames to NumPy arrays for downstream tools\n",
    "\n",
    "**Connection to NumPy:** You've seen how Pandas builds on NumPy concepts:\n",
    "- DataFrames use NumPy arrays under the hood\n",
    "- Statistical operations mirror NumPy functions\n",
    "- Broadcasting works the same way\n",
    "- Final output is clean NumPy arrays ready for ML\n",
    "\n",
    "**Next Steps in Your ML Journey:**\n",
    "\n",
    "You're now ready for the **Scikit-learn Notebook** where you'll:\n",
    "\n",
    "1. **Use Your Clean Data**: Take the preprocessed datasets from this notebook and build actual ML models\n",
    "2. **Apply ML Algorithms**: See how classification, regression, and clustering work with real data\n",
    "3. **Evaluate Models**: Learn to measure and improve model performance\n",
    "4. **Complete ML Pipeline**: Combine data preprocessing (Pandas) + algorithms (scikit-learn) + numerical operations (NumPy)\n",
    "\n",
    "**What to Remember:**\n",
    "- Save your preprocessing steps (scalers, encoders) for future use\n",
    "- The data cleaning workflow you learned applies to every ML project\n",
    "- Clean data is the foundation of successful ML models\n",
    "\n",
    "**You're Ready!** You now have the data preprocessing skills that every ML engineer needs. Time to build some models! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

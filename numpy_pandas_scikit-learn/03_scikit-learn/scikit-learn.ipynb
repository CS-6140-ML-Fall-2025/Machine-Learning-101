{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build and evaluate classification, regression, and clustering models\n",
    "- Master the complete ML pipeline from data to predictions\n",
    "- Apply preprocessing techniques and model evaluation metrics\n",
    "- Understand when to use different algorithms and how to tune them\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals, Pandas data preprocessing (complete previous notebooks first)\n",
    "\n",
    "**Estimated Time:** ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn is the go-to library for machine learning in Python. This notebook brings together everything you've learned in NumPy and Pandas to build actual ML models that can make predictions on real data.\n",
    "\n",
    "**Why Scikit-learn?** It provides:\n",
    "- Consistent API across all algorithms (fit, predict, score)\n",
    "- Built-in preprocessing tools that work seamlessly with Pandas\n",
    "- Comprehensive model evaluation and validation tools\n",
    "- Production-ready implementations of proven algorithms\n",
    "\n",
    "**Learning Path Connection:** This notebook uses:\n",
    "- **NumPy skills**: Array operations, mathematical functions, broadcasting\n",
    "- **Pandas skills**: Data cleaning, feature engineering, train/test splits\n",
    "- **New ML skills**: Model training, evaluation, and prediction\n",
    "\n",
    "**What You'll Build:** Complete ML projects including customer classification, sales prediction, and customer segmentation - exactly what data scientists do every day!\n",
    "\n",
    "**🎯 Success Indicators:** By the end, you should be able to:\n",
    "- Train models and make accurate predictions on new data\n",
    "- Evaluate model performance using appropriate metrics\n",
    "- Choose the right algorithm for different types of problems\n",
    "- Build complete ML pipelines from raw data to final predictions\n",
    "\n",
    "**💡 Beginner Tips:**\n",
    "- Start simple - basic models often work surprisingly well\n",
    "- Always split your data before training (never test on training data!)\n",
    "- Focus on understanding the problem before choosing algorithms\n",
    "- Model evaluation is as important as model training\n",
    "\n",
    "**🔗 ML Problem Types We'll Cover:**\n",
    "- **Classification**: Predicting categories (premium vs regular customers)\n",
    "- **Regression**: Predicting numbers (sales amounts, prices)\n",
    "- **Clustering**: Finding hidden groups in data (customer segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn core modules\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# ML Algorithms we'll use\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set random seed for reproducibility (remember this from NumPy and Pandas!)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Scikit-learn ready! Using reproducible random seed: 42\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Import sklearn and check version\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(\"\\n🚀 Ready to build ML models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding ML Problem Types\n",
    "\n",
    "Before diving into algorithms, let's understand the three main types of ML problems. This foundation will help you choose the right approach for any real-world problem.\n",
    "\n",
    "**Connection to Previous Notebooks:**\n",
    "- **NumPy**: Provided the mathematical foundation (arrays, linear algebra)\n",
    "- **Pandas**: Handled data cleaning and preprocessing\n",
    "- **Scikit-learn**: Now we apply algorithms to make predictions\n",
    "\n",
    "**The Three Types of ML Problems:**\n",
    "\n",
    "1. **Supervised Learning**: Learning from labeled examples\n",
    "   - **Classification**: Predicting categories (spam/not spam, premium/regular)\n",
    "   - **Regression**: Predicting continuous numbers (price, temperature, sales)\n",
    "\n",
    "2. **Unsupervised Learning**: Finding patterns in data without labels\n",
    "   - **Clustering**: Grouping similar items (customer segments, product categories)\n",
    "   - **Dimensionality Reduction**: Simplifying complex data while keeping important patterns\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through trial and error (not covered in this notebook)\n",
    "\n",
    "**How to Choose:**\n",
    "- Got labeled data and want to predict categories? → **Classification**\n",
    "- Got labeled data and want to predict numbers? → **Regression**  \n",
    "- No labels but want to find hidden patterns? → **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same customer dataset from Pandas notebook for consistency\n",
    "print(\"Creating Customer Dataset (same as Pandas notebook for consistency)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate the exact same dataset as Pandas notebook\n",
    "np.random.seed(42)  # Same seed = same data!\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                 n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'experience_years': np.random.exponential(5, n_samples),\n",
    "    'num_purchases': np.random.poisson(3, n_samples),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Our target!\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan\n",
    "\n",
    "print(f\"Dataset created: {df.shape[0]} customers, {df.shape[1]} features\")\n",
    "print(f\"Target variable: is_premium (0=Regular, 1=Premium)\")\n",
    "print(f\"Premium customers: {df['is_premium'].sum()} ({df['is_premium'].mean():.1%})\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n🎯 CLASSIFICATION GOAL: Predict which customers will become premium members\")\n",
    "print(\"This is a binary classification problem (2 classes: 0 or 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification: Predicting Customer Premium Status\n",
    "\n",
    "Classification is about predicting categories or classes. Our goal: predict whether a customer will become a premium member based on their characteristics.\n",
    "\n",
    "**Real-world Applications:**\n",
    "- Email spam detection (spam/not spam)\n",
    "- Medical diagnosis (disease/healthy)\n",
    "- Customer churn prediction (will leave/will stay)\n",
    "- Image recognition (cat/dog/bird)\n",
    "\n",
    "**Our Classification Problem:**\n",
    "- **Features (X)**: age, income, education, experience, etc.\n",
    "- **Target (y)**: is_premium (0=Regular, 1=Premium)\n",
    "- **Goal**: Build a model that can predict premium status for new customers\n",
    "\n",
    "**The ML Workflow:**\n",
    "1. **Prepare Data**: Clean, encode, and split\n",
    "2. **Train Model**: Fit algorithm on training data\n",
    "3. **Evaluate**: Test performance on unseen data\n",
    "4. **Predict**: Make predictions on new customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing (applying Pandas skills!)\n",
    "print(\"Step 1: Data Preprocessing\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create a copy for ML processing\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Handle missing values (remember from Pandas!)\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# Fill missing values with median/mean\n",
    "df_ml['income'].fillna(df_ml['income'].median(), inplace=True)\n",
    "df_ml['satisfaction_score'].fillna(df_ml['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# PARAMETER EXPLANATION: LabelEncoder vs OneHotEncoder\n",
    "print(\"\\nPARAMETER EXPLANATION: Encoding Categorical Variables\")\n",
    "print(\"• LabelEncoder: Converts categories to numbers (0, 1, 2, 3...)\")\n",
    "print(\"  - Use for: Ordinal data (High School < Bachelor < Master < PhD)\")\n",
    "print(\"  - Pros: Simple, compact, preserves order\")\n",
    "print(\"  - Cons: Implies numerical relationship between categories\")\n",
    "print(\"• OneHotEncoder: Creates binary columns for each category\")\n",
    "print(\"  - Use for: Nominal data (North, South, East, West - no order)\")\n",
    "print(\"  - Pros: No false numerical relationships\")\n",
    "print(\"  - Cons: Creates many columns, can cause 'curse of dimensionality'\")\n",
    "print(\"• ML Rule: Use LabelEncoder for ordinal, OneHot for nominal\")\n",
    "\n",
    "# Encode education (ordinal - has natural order)\n",
    "education_mapping = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_ml['education_encoded'] = df_ml['education'].map(education_mapping)\n",
    "\n",
    "print(\"\\nEducation encoding (ordinal):\")\n",
    "print(df_ml[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "# One-hot encode region (nominal - no natural order)\n",
    "region_dummies = pd.get_dummies(df_ml['region'], prefix='region')\n",
    "df_ml = pd.concat([df_ml, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\nRegion encoding (one-hot):\")\n",
    "print(f\"Original region column: {df_ml['region'].unique()}\")\n",
    "print(f\"New binary columns: {list(region_dummies.columns)}\")\n",
    "print(\"Sample of encoded regions:\")\n",
    "print(df_ml[['region'] + list(region_dummies.columns)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Selection and Preparation\n",
    "print(\"Step 2: Feature Selection\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Select features for our model (X) and target (y)\n",
    "feature_columns = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score',\n",
    "    'region_East', 'region_North', 'region_South', 'region_West'\n",
    "]\n",
    "\n",
    "X = df_ml[feature_columns].copy()\n",
    "y = df_ml['is_premium'].copy()\n",
    "\n",
    "print(f\"Features (X): {X.shape[1]} columns\")\n",
    "print(f\"Target (y): {y.shape[0]} samples\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")\n",
    "print(f\"Data types: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split (crucial for honest evaluation!)\n",
    "print(\"Step 3: Train-Test Split\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# PARAMETER EXPLANATION: train_test_split parameters\n",
    "print(\"PARAMETER EXPLANATION: train_test_split()\")\n",
    "print(\"• test_size: Fraction of data to use for testing (0.2 = 20%)\")\n",
    "print(\"• random_state: Seed for reproducible splits (same as np.random.seed)\")\n",
    "print(\"• stratify: Ensures same class distribution in train and test sets\")\n",
    "print(\"• Why stratify: Prevents imbalanced splits (e.g., all premium in train)\")\n",
    "print(\"• Common test_size values: 0.2 (80/20), 0.3 (70/30), 0.25 (75/25)\")\n",
    "print(\"• ML Rule: NEVER look at test data during model development!\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # Reproducible splits\n",
    "    stratify=y          # Keep same class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nClass distribution check:\")\n",
    "print(f\"Original: {y.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Training: {y_train.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(\"✅ Distributions match - stratification worked!\")\n",
    "\n",
    "print(\"\\n🚨 CRITICAL ML RULE: Test set is now 'locked away' until final evaluation!\")\n",
    "print(\"We'll only use X_train and y_train for model development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Scaling (important for many algorithms)\n",
    "print(\"Step 4: Feature Scaling\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# PARAMETER EXPLANATION: Why scaling matters\n",
    "print(\"WHY FEATURE SCALING MATTERS:\")\n",
    "print(\"• Income: ranges from $20,000 to $200,000\")\n",
    "print(\"• Age: ranges from 18 to 65\")\n",
    "print(\"• Without scaling: Income dominates because of larger numbers\")\n",
    "print(\"• With scaling: All features have equal influence\")\n",
    "print(\"• Algorithms that need scaling: Logistic Regression, SVM, Neural Networks\")\n",
    "print(\"• Algorithms that don't: Decision Trees, Random Forest\")\n",
    "\n",
    "# Check feature scales before scaling\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_train.describe().round(2))\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same scaling as training!\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_train_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHT: All features now have mean≈0 and std≈1\")\n",
    "print(\"This ensures fair treatment of all features in the model.\")\n",
    "\n",
    "# CRITICAL ML CONCEPT: Fit on train, transform on test\n",
    "print(\"\\n🚨 CRITICAL CONCEPT: Data Leakage Prevention\")\n",
    "print(\"• scaler.fit_transform(X_train): Learn scaling parameters from training data\")\n",
    "print(\"• scaler.transform(X_test): Apply same scaling to test data\")\n",
    "print(\"• NEVER fit scaler on test data - that's data leakage!\")\n",
    "print(\"• Same rule applies to all preprocessing: fit on train, transform on test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Algorithms\n",
    "\n",
    "Now let's train different classification algorithms and compare their performance. Each algorithm has different strengths and is suited for different types of problems.\n",
    "\n",
    "**Algorithms We'll Compare:**\n",
    "1. **Logistic Regression**: Simple, interpretable, good baseline\n",
    "2. **Decision Tree**: Easy to understand, handles non-linear patterns\n",
    "3. **Random Forest**: Combines many trees, usually more accurate\n",
    "4. **K-Nearest Neighbors**: Simple concept, good for local patterns\n",
    "\n",
    "**The Scikit-learn Pattern:**\n",
    "All algorithms follow the same 3-step pattern:\n",
    "1. **Create**: `model = Algorithm()`\n",
    "2. **Train**: `model.fit(X_train, y_train)`\n",
    "3. **Predict**: `predictions = model.predict(X_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Logistic Regression\n",
    "print(\"🔍 Algorithm 1: Logistic Regression\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Logistic Regression parameters\n",
    "print(\"ALGORITHM EXPLANATION: Logistic Regression\")\n",
    "print(\"• What it does: Finds the best line to separate classes\")\n",
    "print(\"• Strengths: Fast, interpretable, probabilistic predictions\")\n",
    "print(\"• Weaknesses: Assumes linear relationships\")\n",
    "print(\"• Best for: When you need to understand feature importance\")\n",
    "print(\"• Output: Probability between 0 and 1 (>0.5 = class 1)\")\n",
    "print(\"• Connection to NumPy: Uses matrix operations for optimization\")\n",
    "\n",
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "print(f\"\\n📊 Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log:.3f} ({accuracy_log:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_log == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_log[:10],\n",
    "    'Probability': y_pred_proba_log[:10].round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_log[:10])\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\n🎯 Feature Importance (coefficients):\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "print(\"\\n💡 Interpretation: Larger absolute coefficients = more important features\")\n",
    "print(\"Positive coefficients increase premium probability, negative decrease it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Decision Tree\n",
    "print(\"🌳 Algorithm 2: Decision Tree\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree parameters\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree\")\n",
    "print(\"• What it does: Creates a series of yes/no questions to classify data\")\n",
    "print(\"• Strengths: Easy to understand, handles non-linear patterns, no scaling needed\")\n",
    "print(\"• Weaknesses: Can overfit, unstable (small data changes = different tree)\")\n",
    "print(\"• Best for: When you need interpretable rules (if age > 30 AND income > 50k...)\")\n",
    "print(\"• max_depth: Limits tree depth to prevent overfitting\")\n",
    "print(\"• min_samples_split: Minimum samples needed to split a node\")\n",
    "\n",
    "# Create and train the model (using original features, not scaled)\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,           # Limit depth to prevent overfitting\n",
    "    min_samples_split=20,  # Need at least 20 samples to split\n",
    "    random_state=42\n",
    ")\n",
    "tree_clf.fit(X_train, y_train)  # Note: using unscaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "y_pred_proba_tree = tree_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"\\n📊 Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_tree:.3f} ({accuracy_tree:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_tree == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (different from logistic regression!)\n",
    "print(\"\\n🎯 Feature Importance (based on information gain):\")\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': tree_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_importance)\n",
    "print(\"\\n💡 Interpretation: Higher importance = more useful for splitting data\")\n",
    "print(\"Tree importance shows which features create the purest splits\")\n",
    "\n",
    "# Show a few decision rules (simplified)\n",
    "print(\"\\n🌳 Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"• If income > $45,000 AND satisfaction > 3.2 → Likely Premium\")\n",
    "print(\"• If age < 25 AND num_purchases < 2 → Likely Regular\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Random Forest\n",
    "print(\"🌲🌳🌲 Algorithm 3: Random Forest\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest parameters\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest\")\n",
    "print(\"• What it does: Combines predictions from many decision trees\")\n",
    "print(\"• Strengths: Usually more accurate, reduces overfitting, handles missing values\")\n",
    "print(\"• Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"• Best for: When accuracy is more important than interpretability\")\n",
    "print(\"• n_estimators: Number of trees (more trees = better but slower)\")\n",
    "print(\"• max_depth: Depth of each tree\")\n",
    "print(\"• Voting: Each tree votes, majority wins (ensemble method)\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,      # Use 100 trees\n",
    "    max_depth=5,           # Limit depth of each tree\n",
    "    min_samples_split=20,  # Same as single tree\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)  # Using unscaled data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\n📊 Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.3f} ({accuracy_rf:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_rf == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\n🎯 Feature Importance (averaged across 100 trees):\")\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_importance)\n",
    "print(\"\\n💡 Interpretation: More stable importance scores than single tree\")\n",
    "print(\"Random Forest importance is more reliable due to averaging\")\n",
    "\n",
    "# Show confidence in predictions\n",
    "print(\"\\n🎯 Prediction Confidence (first 10 samples):\")\n",
    "confidence_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_rf[:10],\n",
    "    'Confidence': np.maximum(y_pred_proba_rf[:10], 1-y_pred_proba_rf[:10]).round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_rf[:10])\n",
    "})\n",
    "print(confidence_df)\n",
    "print(\"Higher confidence = more certain prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4: K-Nearest Neighbors\n",
    "print(\"👥 Algorithm 4: K-Nearest Neighbors (KNN)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PARAMETER EXPLANATION: KNN parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Nearest Neighbors\")\n",
    "print(\"• What it does: Classifies based on the K closest training examples\")\n",
    "print(\"• Strengths: Simple concept, works well with local patterns\")\n",
    "print(\"• Weaknesses: Slow with large datasets, sensitive to irrelevant features\")\n",
    "print(\"• Best for: When similar items should have similar labels\")\n",
    "print(\"• n_neighbors (K): How many neighbors to consider (odd numbers avoid ties)\")\n",
    "print(\"• Distance: Usually Euclidean distance (requires scaling!)\")\n",
    "print(\"• Lazy learning: No training phase, all work done during prediction\")\n",
    "\n",
    "# Create and train the model\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=5,  # Look at 5 nearest neighbors\n",
    "    weights='distance'  # Closer neighbors have more influence\n",
    ")\n",
    "knn_clf.fit(X_train_scaled, y_train)  # KNN needs scaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"\\n📊 K-Nearest Neighbors Results:\")\n",
    "print(f\"Accuracy: {accuracy_knn:.3f} ({accuracy_knn:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_knn == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# KNN doesn't have feature importance, but we can show prediction examples\n",
    "print(\"\\n🎯 How KNN Makes Predictions (conceptual):\")\n",
    "print(\"For each test sample:\")\n",
    "print(\"1. Find the 5 most similar customers in training data\")\n",
    "print(\"2. Look at their premium status (0 or 1)\")\n",
    "print(\"3. Take majority vote (e.g., 3 premium + 2 regular = predict premium)\")\n",
    "print(\"4. Weight by distance (closer neighbors count more)\")\n",
    "\n",
    "# Show some prediction probabilities\n",
    "print(\"\\n📊 KNN Prediction Examples (first 5 samples):\")\n",
    "knn_examples = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:5].values,\n",
    "    'Predicted': y_pred_knn[:5],\n",
    "    'Probability': y_pred_proba_knn[:5].round(3),\n",
    "    'Interpretation': [\n",
    "        f\"{int(p*5)}/5 neighbors were premium\" for p in y_pred_proba_knn[:5]\n",
    "    ]\n",
    "})\n",
    "print(knn_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison\n",
    "\n",
    "Accuracy is just one metric. Let's dive deeper into evaluation to understand which model is truly best for our problem.\n",
    "\n",
    "**Why Multiple Metrics Matter:**\n",
    "- **Accuracy**: Overall correctness, but can be misleading with imbalanced data\n",
    "- **Precision**: Of predicted positives, how many were actually positive?\n",
    "- **Recall**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Shows exactly where the model makes mistakes\n",
    "\n",
    "**Business Context Matters:**\n",
    "- High precision: Avoid false positives (don't waste premium offers on unlikely customers)\n",
    "- High recall: Catch all potential premiums (don't miss valuable customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models side by side\n",
    "print(\"📊 MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate accuracies for all models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbors'],\n",
    "    'Accuracy': [accuracy_log, accuracy_tree, accuracy_rf, accuracy_knn],\n",
    "    'Correct_Predictions': [\n",
    "        (y_pred_log == y_test).sum(),\n",
    "        (y_pred_tree == y_test).sum(), \n",
    "        (y_pred_rf == y_test).sum(),\n",
    "        (y_pred_knn == y_test).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by accuracy\n",
    "models_comparison = models_comparison.sort_values('Accuracy', ascending=False)\n",
    "models_comparison['Accuracy_Percent'] = (models_comparison['Accuracy'] * 100).round(1)\n",
    "\n",
    "print(models_comparison)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = models_comparison.iloc[0]['Model']\n",
    "best_accuracy = models_comparison.iloc[0]['Accuracy']\n",
    "print(f\"\\n🏆 Best Model: {best_model_name} with {best_accuracy:.1%} accuracy\")\n",
    "\n",
    "# But let's look deeper with classification reports\n",
    "print(\"\\n📋 DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "models_and_predictions = [\n",
    "    ('Logistic Regression', y_pred_log),\n",
    "    ('Decision Tree', y_pred_tree),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('K-Nearest Neighbors', y_pred_knn)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * len(model_name))\n",
    "    print(classification_report(y_test, predictions, target_names=['Regular', 'Premium']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices - Show exactly where models make mistakes\n",
    "print(\"🔍 CONFUSION MATRICES - Where Do Models Make Mistakes?\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# PARAMETER EXPLANATION: Confusion Matrix\n",
    "print(\"CONFUSION MATRIX EXPLANATION:\")\n",
    "print(\"• Rows: Actual classes (what really happened)\")\n",
    "print(\"• Columns: Predicted classes (what model predicted)\")\n",
    "print(\"• Diagonal: Correct predictions\")\n",
    "print(\"• Off-diagonal: Mistakes\")\n",
    "print(\"• Top-left: True Negatives (correctly predicted Regular)\")\n",
    "print(\"• Top-right: False Positives (predicted Premium, actually Regular)\")\n",
    "print(\"• Bottom-left: False Negatives (predicted Regular, actually Premium)\")\n",
    "print(\"• Bottom-right: True Positives (correctly predicted Premium)\")\n",
    "\n",
    "# Print numerical confusion matrices\n",
    "print(\"\\nNumerical Confusion Matrices:\")\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"Actual    Regular  Premium\")\n",
    "    print(f\"Regular      {cm[0,0]:3d}      {cm[0,1]:3d}\")\n",
    "    print(f\"Premium      {cm[1,0]:3d}      {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate error types\n",
    "    false_positives = cm[0,1]  # Predicted premium, actually regular\n",
    "    false_negatives = cm[1,0]  # Predicted regular, actually premium\n",
    "    \n",
    "    print(f\"False Positives: {false_positives} (wasted premium offers)\")\n",
    "    print(f\"False Negatives: {false_negatives} (missed premium customers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Impact Analysis\n",
    "print(\"💰 BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "print(\"Let's translate model performance into business terms:\")\n",
    "print(\"\\nScenario: Premium membership campaign\")\n",
    "print(\"• Cost of premium offer: $50 per customer\")\n",
    "print(\"• Revenue from premium customer: $200 per year\")\n",
    "print(\"• Net profit from correct premium prediction: $150\")\n",
    "print(\"• Cost of false positive (wasted offer): $50\")\n",
    "print(\"• Cost of false negative (missed customer): $150 (lost revenue)\")\n",
    "\n",
    "# Calculate business impact for each model\n",
    "offer_cost = 50\n",
    "premium_revenue = 200\n",
    "net_profit = premium_revenue - offer_cost\n",
    "\n",
    "print(\"\\n💼 Business Impact by Model:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    true_positives = cm[1,1]   # Correctly identified premium customers\n",
    "    false_positives = cm[0,1]  # Wasted offers to regular customers\n",
    "    false_negatives = cm[1,0]  # Missed premium customers\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    profit_from_tp = true_positives * net_profit\n",
    "    cost_from_fp = false_positives * offer_cost\n",
    "    lost_revenue_fn = false_negatives * net_profit\n",
    "    \n",
    "    total_impact = profit_from_tp - cost_from_fp - lost_revenue_fn\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Profit from correct predictions: ${profit_from_tp:,}\")\n",
    "    print(f\"  Cost from wasted offers: ${cost_from_fp:,}\")\n",
    "    print(f\"  Lost revenue from missed customers: ${lost_revenue_fn:,}\")\n",
    "    print(f\"  NET BUSINESS IMPACT: ${total_impact:,}\")\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHT: The 'best' model depends on business priorities!\")\n",
    "print(\"• If minimizing wasted offers is critical → Choose high precision model\")\n",
    "print(\"• If catching all premium customers is critical → Choose high recall model\")\n",
    "print(\"• For balanced approach → Choose high F1-score model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression: Predicting Customer Lifetime Value\n",
    "\n",
    "Now let's switch from predicting categories (classification) to predicting continuous numbers (regression). We'll predict customer lifetime value based on their characteristics.\n",
    "\n",
    "**Real-world Regression Applications:**\n",
    "- House price prediction (real estate)\n",
    "- Stock price forecasting (finance)\n",
    "- Sales revenue prediction (business)\n",
    "- Temperature forecasting (weather)\n",
    "- Medical dosage optimization (healthcare)\n",
    "\n",
    "**Our Regression Problem:**\n",
    "- **Features (X)**: Same customer characteristics as before\n",
    "- **Target (y)**: Customer lifetime value in dollars\n",
    "- **Goal**: Predict how much revenue each customer will generate\n",
    "\n",
    "**Key Differences from Classification:**\n",
    "- **Output**: Continuous numbers instead of discrete categories\n",
    "- **Metrics**: MSE, MAE, R² instead of accuracy, precision, recall\n",
    "- **Algorithms**: Linear/Polynomial Regression, Decision Tree/Random Forest Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression target: Customer Lifetime Value (CLV)\n",
    "print(\"Creating Regression Target: Customer Lifetime Value\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Generate realistic CLV based on customer features\n",
    "np.random.seed(42)  # Consistent with our other data\n",
    "\n",
    "# CLV formula: Base value + bonuses based on customer characteristics + noise\n",
    "base_clv = 500  # Base customer value\n",
    "\n",
    "# Calculate CLV components (realistic business logic)\n",
    "age_bonus = (df_ml['age'] - 25) * 10  # Older customers worth more\n",
    "income_bonus = (df_ml['income'] / 1000) * 2  # Higher income = higher CLV\n",
    "education_bonus = df_ml['education_encoded'] * 100  # Education increases value\n",
    "satisfaction_bonus = df_ml['satisfaction_score'] * 150  # Happy customers spend more\n",
    "purchase_bonus = df_ml['num_purchases'] * 80  # Purchase history matters\n",
    "premium_bonus = df_ml['is_premium'] * 800  # Premium customers worth much more\n",
    "\n",
    "# Combine all factors\n",
    "clv_deterministic = (base_clv + age_bonus + income_bonus + \n",
    "                    education_bonus + satisfaction_bonus + \n",
    "                    purchase_bonus + premium_bonus)\n",
    "\n",
    "# Add realistic noise (business is never perfectly predictable)\n",
    "noise = np.random.normal(0, 200, len(df_ml))  # Random variation\n",
    "clv = clv_deterministic + noise\n",
    "\n",
    "# Ensure CLV is positive (can't have negative customer value)\n",
    "clv = np.maximum(clv, 100)  # Minimum CLV of $100\n",
    "\n",
    "# Add to our dataframe\n",
    "df_ml['customer_lifetime_value'] = clv\n",
    "\n",
    "print(f\"Customer Lifetime Value Statistics:\")\n",
    "print(f\"Mean CLV: ${clv.mean():.2f}\")\n",
    "print(f\"Median CLV: ${clv.median():.2f}\")\n",
    "print(f\"Min CLV: ${clv.min():.2f}\")\n",
    "print(f\"Max CLV: ${clv.max():.2f}\")\n",
    "print(f\"Standard Deviation: ${clv.std():.2f}\")\n",
    "\n",
    "# Show relationship between features and CLV\n",
    "print(\"\\n🔍 CLV Correlations with Features:\")\n",
    "clv_correlations = df_ml[feature_columns + ['customer_lifetime_value']].corr()['customer_lifetime_value'].sort_values(ascending=False)\n",
    "print(clv_correlations.drop('customer_lifetime_value').round(3))\n",
    "\n",
    "print(\"\\n💡 Business Insights:\")\n",
    "print(\"• Premium customers have significantly higher CLV\")\n",
    "print(\"• Income and satisfaction strongly correlate with CLV\")\n",
    "print(\"• Education level impacts long-term customer value\")\n",
    "print(\"• Purchase history is a strong predictor\")\n",
    "\n",
    "print(\"\\n🎯 REGRESSION GOAL: Predict CLV for new customers to optimize marketing spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "print(\"Preparing Regression Data\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Use same features as classification, but different target\n",
    "X_reg = df_ml[feature_columns].copy()\n",
    "y_reg = df_ml['customer_lifetime_value'].copy()\n",
    "\n",
    "print(f\"Regression features: {X_reg.shape[1]} columns\")\n",
    "print(f\"Regression target: {y_reg.shape[0]} CLV values\")\n",
    "print(f\"Target range: ${y_reg.min():.0f} to ${y_reg.max():.0f}\")\n",
    "\n",
    "# Split data for regression (same random state for consistency)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42  # Same split as classification for comparison\n",
    ")\n",
    "\n",
    "print(f\"\\nRegression data split:\")\n",
    "print(f\"Training: {X_train_reg.shape[0]} samples\")\n",
    "print(f\"Testing: {X_test_reg.shape[0]} samples\")\n",
    "\n",
    "# Scale features for regression (some algorithms need it)\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_reg_scaled = pd.DataFrame(X_train_reg_scaled, columns=X_train_reg.columns, index=X_train_reg.index)\n",
    "X_test_reg_scaled = pd.DataFrame(X_test_reg_scaled, columns=X_test_reg.columns, index=X_test_reg.index)\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"Training CLV mean: ${y_train_reg.mean():.2f}\")\n",
    "print(f\"Testing CLV mean: ${y_test_reg.mean():.2f}\")\n",
    "print(\"✅ Similar distributions - good split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Algorithms\n",
    "\n",
    "Let's train different regression algorithms to predict customer lifetime value. Each has different strengths for different types of relationships.\n",
    "\n",
    "**Regression Algorithms We'll Compare:**\n",
    "1. **Linear Regression**: Simple, interpretable, assumes linear relationships\n",
    "2. **Polynomial Regression**: Captures curved relationships\n",
    "3. **Decision Tree Regressor**: Handles non-linear patterns, no scaling needed\n",
    "4. **Random Forest Regressor**: Ensemble method, usually more accurate\n",
    "\n",
    "**Regression Metrics:**\n",
    "- **MAE (Mean Absolute Error)**: Average prediction error in dollars\n",
    "- **MSE (Mean Squared Error)**: Penalizes large errors more heavily\n",
    "- **RMSE (Root MSE)**: MSE in original units (dollars)\n",
    "- **R² Score**: Percentage of variance explained (0-1, higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Linear Regression\n",
    "print(\"📈 Algorithm 1: Linear Regression\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Linear Regression\n",
    "print(\"ALGORITHM EXPLANATION: Linear Regression\")\n",
    "print(\"• What it does: Finds the best straight line through the data\")\n",
    "print(\"• Strengths: Simple, fast, interpretable coefficients\")\n",
    "print(\"• Weaknesses: Assumes linear relationships only\")\n",
    "print(\"• Best for: When relationships are roughly linear\")\n",
    "print(\"• Output: Continuous predictions (any real number)\")\n",
    "print(\"• Connection to NumPy: Uses matrix operations (X^T * X)^-1 * X^T * y\")\n",
    "\n",
    "# Create and train the model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae_linear = mean_absolute_error(y_test_reg, y_pred_linear)\n",
    "mse_linear = mean_squared_error(y_test_reg, y_pred_linear)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "r2_linear = r2_score(y_test_reg, y_pred_linear)\n",
    "\n",
    "print(f\"\\n📊 Linear Regression Results:\")\n",
    "print(f\"MAE: ${mae_linear:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_linear:.2f} (root mean squared error)\")\n",
    "print(f\"R² Score: {r2_linear:.3f} ({r2_linear:.1%} of variance explained)\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "linear_results = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values.round(2),\n",
    "    'Predicted_CLV': y_pred_linear[:10].round(2),\n",
    "    'Error': (y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2),\n",
    "    'Abs_Error': np.abs(y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2)\n",
    "})\n",
    "print(linear_results)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\n🎯 Feature Coefficients (impact on CLV):\")\n",
    "linear_coef = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Coefficient': linear_reg.coef_,\n",
    "    'Abs_Coefficient': np.abs(linear_reg.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(linear_coef)\n",
    "print(f\"\\nIntercept: ${linear_reg.intercept_:.2f}\")\n",
    "print(\"\\n💡 Interpretation: Each coefficient shows CLV change per unit increase in feature\")\n",
    "print(\"Positive coefficients increase CLV, negative coefficients decrease it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Polynomial Regression (Linear Regression with polynomial features)\n",
    "print(\"📊 Algorithm 2: Polynomial Regression\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Polynomial Features\n",
    "print(\"ALGORITHM EXPLANATION: Polynomial Regression\")\n",
    "print(\"• What it does: Creates curved relationships by adding x², x³, x*y terms\")\n",
    "print(\"• Strengths: Captures non-linear patterns, still interpretable\")\n",
    "print(\"• Weaknesses: Can overfit easily, creates many features\")\n",
    "print(\"• Best for: When you see curved relationships in data\")\n",
    "print(\"• degree=2: Adds squared terms (x²) for curves\")\n",
    "print(\"• interaction_only=False: Includes both x² and x*y terms\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2 for quadratic relationships)\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_reg_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_reg_scaled)\n",
    "\n",
    "print(f\"\\nFeature expansion:\")\n",
    "print(f\"Original features: {X_train_reg_scaled.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"Added {X_train_poly.shape[1] - X_train_reg_scaled.shape[1]} polynomial terms\")\n",
    "\n",
    "# Train polynomial regression\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_poly = mean_absolute_error(y_test_reg, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_test_reg, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_test_reg, y_pred_poly)\n",
    "\n",
    "print(f\"\\n📊 Polynomial Regression Results:\")\n",
    "print(f\"MAE: ${mae_poly:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_poly:.2f} (root mean squared error)\")\n",
    "print(f\"R² Score: {r2_poly:.3f} ({r2_poly:.1%} of variance explained)\")\n",
    "\n",
    "# Compare with linear regression\n",
    "print(f\"\\n📈 Improvement over Linear Regression:\")\n",
    "mae_improvement = ((mae_linear - mae_poly) / mae_linear) * 100\n",
    "r2_improvement = r2_poly - r2_linear\n",
    "print(f\"MAE improvement: {mae_improvement:.1f}% better\")\n",
    "print(f\"R² improvement: +{r2_improvement:.3f} ({r2_improvement*100:.1f} percentage points)\")\n",
    "\n",
    "if r2_poly > r2_linear:\n",
    "    print(\"✅ Polynomial features captured additional patterns!\")\n",
    "else:\n",
    "    print(\"⚠️ Polynomial features didn't help - relationships might be mostly linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Decision Tree Regressor\n",
    "print(\"🌳 Algorithm 3: Decision Tree Regressor\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree Regressor\")\n",
    "print(\"• What it does: Creates rules to predict continuous values\")\n",
    "print(\"• Strengths: Handles non-linear patterns, no scaling needed, interpretable\")\n",
    "print(\"• Weaknesses: Can overfit, unstable with small data changes\")\n",
    "print(\"• Best for: When relationships are complex and non-linear\")\n",
    "print(\"• Prediction: Average of target values in each leaf node\")\n",
    "print(\"• Example rule: If income > $50k AND age > 30 → Predict CLV = $2,500\")\n",
    "\n",
    "# Create and train the model (using unscaled data)\n",
    "tree_reg = DecisionTreeRegressor(\n",
    "    max_depth=6,           # Slightly deeper for regression\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,   # Ensure meaningful leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "tree_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree_reg = tree_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_tree = mean_absolute_error(y_test_reg, y_pred_tree_reg)\n",
    "mse_tree = mean_squared_error(y_test_reg, y_pred_tree_reg)\n",
    "rmse_tree = np.sqrt(mse_tree)\n",
    "r2_tree = r2_score(y_test_reg, y_pred_tree_reg)\n",
    "\n",
    "print(f\"\\n📊 Decision Tree Regressor Results:\")\n",
    "print(f\"MAE: ${mae_tree:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_tree:.2f} (root mean squared error)\")\n",
    "print(f\"R² Score: {r2_tree:.3f} ({r2_tree:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\n🎯 Feature Importance (for splitting):\")\n",
    "tree_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': tree_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_reg_importance)\n",
    "print(\"\\n💡 Interpretation: Higher importance = more useful for predicting CLV\")\n",
    "\n",
    "# Show some example decision paths (conceptual)\n",
    "print(\"\\n🌳 Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"• If income > $45,000 AND satisfaction > 3.5 → Predict CLV ≈ $2,800\")\n",
    "print(\"• If age < 30 AND num_purchases < 3 → Predict CLV ≈ $1,200\")\n",
    "print(\"• If premium=1 AND education=PhD → Predict CLV ≈ $4,500\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4: Random Forest Regressor\n",
    "print(\"🌲🌳🌲 Algorithm 4: Random Forest Regressor\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest Regressor\")\n",
    "print(\"• What it does: Averages predictions from many decision trees\")\n",
    "print(\"• Strengths: Usually most accurate, reduces overfitting, handles missing values\")\n",
    "print(\"• Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"• Best for: When accuracy is more important than interpretability\")\n",
    "print(\"• Prediction: Average of all tree predictions\")\n",
    "print(\"• Example: Tree1=$2,400 + Tree2=$2,600 + Tree3=$2,500 → Predict $2,500\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,      # Use 100 trees\n",
    "    max_depth=6,           # Same depth as single tree\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,   # Ensure meaningful predictions\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf_reg = rf_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test_reg, y_pred_rf_reg)\n",
    "mse_rf = mean_squared_error(y_test_reg, y_pred_rf_reg)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test_reg, y_pred_rf_reg)\n",
    "\n",
    "print(f\"\\n📊 Random Forest Regressor Results:\")\n",
    "print(f\"MAE: ${mae_rf:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_rf:.2f} (root mean squared error)\")\n",
    "print(f\"R² Score: {r2_rf:.3f} ({r2_rf:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\n🎯 Feature Importance (averaged across 100 trees):\")\n",
    "rf_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': rf_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_reg_importance)\n",
    "print(\"\\n💡 Interpretation: More stable importance scores than single tree\")\n",
    "\n",
    "# Show prediction confidence (using tree variance)\n",
    "print(\"\\n🎯 Prediction Examples with Confidence (first 5 samples):\")\n",
    "# Get predictions from individual trees for confidence estimation\n",
    "tree_predictions = np.array([tree.predict(X_test_reg.iloc[:5]) for tree in rf_reg.estimators_])\n",
    "prediction_std = np.std(tree_predictions, axis=0)\n",
    "\n",
    "rf_confidence = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:5].values.round(2),\n",
    "    'Predicted_CLV': y_pred_rf_reg[:5].round(2),\n",
    "    'Prediction_Std': prediction_std.round(2),\n",
    "    'Confidence_Range': [f\"±${std:.0f}\" for std in prediction_std]\n",
    "})\n",
    "print(rf_confidence)\n",
    "print(\"Lower standard deviation = more confident prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Model Comparison\n",
    "\n",
    "Let's compare all regression models to understand which performs best for predicting customer lifetime value.\n",
    "\n",
    "**Regression Metrics Explained:**\n",
    "- **MAE**: Mean Absolute Error - average prediction error in dollars (lower is better)\n",
    "- **RMSE**: Root Mean Squared Error - penalizes large errors more (lower is better)\n",
    "- **R² Score**: Coefficient of determination - percentage of variance explained (higher is better)\n",
    "\n",
    "**Business Context:**\n",
    "- **MAE**: \"On average, our predictions are off by $X\"\n",
    "- **RMSE**: \"Our model has larger penalties for big mistakes\"\n",
    "- **R²**: \"Our model explains X% of why CLV varies between customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all regression models\n",
    "print(\"📊 REGRESSION MODEL COMPARISON\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "regression_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'MAE': [mae_linear, mae_poly, mae_tree, mae_rf],\n",
    "    'RMSE': [rmse_linear, rmse_poly, rmse_tree, rmse_rf],\n",
    "    'R²_Score': [r2_linear, r2_poly, r2_tree, r2_rf]\n",
    "})\n",
    "\n",
    "# Sort by R² score (higher is better)\n",
    "regression_comparison = regression_comparison.sort_values('R²_Score', ascending=False)\n",
    "regression_comparison['R²_Percent'] = (regression_comparison['R²_Score'] * 100).round(1)\n",
    "\n",
    "print(\"Model Performance Ranking (by R² Score):\")\n",
    "print(regression_comparison.round(2))\n",
    "\n",
    "# Identify best model\n",
    "best_reg_model = regression_comparison.iloc[0]['Model']\n",
    "best_r2 = regression_comparison.iloc[0]['R²_Score']\n",
    "best_mae = regression_comparison.iloc[0]['MAE']\n",
    "\n",
    "print(f\"\\n🏆 Best Regression Model: {best_reg_model}\")\n",
    "print(f\"   R² Score: {best_r2:.3f} ({best_r2:.1%} variance explained)\")\n",
    "print(f\"   Average Error: ${best_mae:.2f}\")\n",
    "\n",
    "# Calculate baseline comparison\n",
    "baseline_mae = mean_absolute_error(y_test_reg, [y_train_reg.mean()] * len(y_test_reg))\n",
    "print(f\"\\n📏 Baseline Comparison (predicting mean CLV):\")\n",
    "print(f\"   Baseline MAE: ${baseline_mae:.2f}\")\n",
    "print(f\"   Best Model MAE: ${best_mae:.2f}\")\n",
    "improvement = ((baseline_mae - best_mae) / baseline_mae) * 100\n",
    "print(f\"   Improvement: {improvement:.1f}% better than baseline\")\n",
    "\n",
    "# Show prediction accuracy ranges\n",
    "print(f\"\\n🎯 Prediction Accuracy Interpretation:\")\n",
    "print(f\"• Our best model is typically off by ${best_mae:.0f} when predicting CLV\")\n",
    "print(f\"• For a customer with ${y_test_reg.mean():.0f} actual CLV:\")\n",
    "print(f\"  - Prediction range: ${y_test_reg.mean()-best_mae:.0f} to ${y_test_reg.mean()+best_mae:.0f}\")\n",
    "print(f\"  - That's ±{(best_mae/y_test_reg.mean())*100:.1f}% relative error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed prediction analysis\n",
    "print(\"🔍 DETAILED PREDICTION ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Compare predictions from all models\n",
    "prediction_comparison = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values,\n",
    "    'Linear_Pred': y_pred_linear[:10],\n",
    "    'Poly_Pred': y_pred_poly[:10],\n",
    "    'Tree_Pred': y_pred_tree_reg[:10],\n",
    "    'RF_Pred': y_pred_rf_reg[:10]\n",
    "})\n",
    "\n",
    "# Calculate errors for each model\n",
    "for model in ['Linear', 'Poly', 'Tree', 'RF']:\n",
    "    prediction_comparison[f'{model}_Error'] = (\n",
    "        prediction_comparison['Actual_CLV'] - prediction_comparison[f'{model}_Pred']\n",
    "    ).abs()\n",
    "\n",
    "print(\"Prediction Comparison (first 10 test samples):\")\n",
    "print(prediction_comparison.round(2))\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\n📈 Error Pattern Analysis:\")\n",
    "models_and_preds = [\n",
    "    ('Linear Regression', y_pred_linear),\n",
    "    ('Polynomial Regression', y_pred_poly),\n",
    "    ('Decision Tree', y_pred_tree_reg),\n",
    "    ('Random Forest', y_pred_rf_reg)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    errors = np.abs(y_test_reg - predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean Error: ${errors.mean():.2f}\")\n",
    "    print(f\"  Median Error: ${errors.median():.2f}\")\n",
    "    print(f\"  Max Error: ${errors.max():.2f}\")\n",
    "    print(f\"  % predictions within $500: {(errors <= 500).mean():.1%}\")\n",
    "    print(f\"  % predictions within $1000: {(errors <= 1000).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact of regression predictions\n",
    "print(\"💰 BUSINESS IMPACT OF CLV PREDICTIONS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"Business Scenario: Marketing Budget Allocation\")\n",
    "print(\"• High CLV customers (>$3000): Premium marketing ($200 spend)\")\n",
    "print(\"• Medium CLV customers ($1500-$3000): Standard marketing ($100 spend)\")\n",
    "print(\"• Low CLV customers (<$1500): Basic marketing ($50 spend)\")\n",
    "print(\"• Goal: Maximize ROI by targeting right customers with right campaigns\")\n",
    "\n",
    "# Define CLV segments\n",
    "def classify_clv(clv):\n",
    "    if clv >= 3000:\n",
    "        return 'High'\n",
    "    elif clv >= 1500:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Marketing costs by segment\n",
    "marketing_costs = {'High': 200, 'Medium': 100, 'Low': 50}\n",
    "\n",
    "print(\"\\n💼 Marketing ROI Analysis by Model:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    # Classify actual and predicted CLV\n",
    "    actual_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "    predicted_segments = [classify_clv(clv) for clv in predictions]\n",
    "    \n",
    "    # Calculate marketing spend based on predictions\n",
    "    predicted_spend = sum(marketing_costs[seg] for seg in predicted_segments)\n",
    "    \n",
    "    # Calculate actual ROI (revenue - marketing cost)\n",
    "    actual_revenue = y_test_reg.sum()\n",
    "    roi = actual_revenue - predicted_spend\n",
    "    roi_ratio = actual_revenue / predicted_spend\n",
    "    \n",
    "    # Calculate segment accuracy\n",
    "    segment_accuracy = sum(1 for a, p in zip(actual_segments, predicted_segments) if a == p) / len(actual_segments)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Total marketing spend: ${predicted_spend:,}\")\n",
    "    print(f\"  Total customer revenue: ${actual_revenue:,.0f}\")\n",
    "    print(f\"  Net ROI: ${roi:,.0f}\")\n",
    "    print(f\"  ROI ratio: {roi_ratio:.2f}x\")\n",
    "    print(f\"  Segment classification accuracy: {segment_accuracy:.1%}\")\n",
    "\n",
    "# Optimal allocation (if we knew true CLV)\n",
    "optimal_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "optimal_spend = sum(marketing_costs[seg] for seg in optimal_segments)\n",
    "optimal_roi = y_test_reg.sum() - optimal_spend\n",
    "\n",
    "print(f\"\\n🎯 Optimal Allocation (perfect predictions):\")\n",
    "print(f\"  Marketing spend: ${optimal_spend:,}\")\n",
    "print(f\"  Net ROI: ${optimal_roi:,.0f}\")\n",
    "print(f\"  ROI ratio: {y_test_reg.sum()/optimal_spend:.2f}x\")\n",
    "\n",
    "print(\"\\n💡 Key Insight: Better CLV predictions → Better marketing allocation → Higher ROI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering: Discovering Customer Segments\n",
    "\n",
    "Now let's explore unsupervised learning with clustering. Unlike classification and regression, we don't have a target variable - we're looking for hidden patterns in the data.\n",
    "\n",
    "**Real-world Clustering Applications:**\n",
    "- Customer segmentation (marketing)\n",
    "- Market research (identifying consumer groups)\n",
    "- Gene sequencing (biology)\n",
    "- Image segmentation (computer vision)\n",
    "- Anomaly detection (fraud, network security)\n",
    "\n",
    "**Our Clustering Problem:**\n",
    "- **Goal**: Discover natural customer segments based on behavior and characteristics\n",
    "- **Features**: Customer demographics and behavior (no target variable!)\n",
    "- **Output**: Group assignments (Cluster 0, 1, 2, etc.)\n",
    "- **Business Value**: Targeted marketing, personalized products, customer insights\n",
    "\n",
    "**Key Differences from Supervised Learning:**\n",
    "- **No labels**: We don't know the \"right\" answer beforehand\n",
    "- **Exploratory**: We're discovering patterns, not predicting outcomes\n",
    "- **Evaluation**: Harder to measure - we use internal metrics and business interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "print(\"Preparing Data for Customer Segmentation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select features for clustering (exclude target variables and IDs)\n",
    "clustering_features = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score'\n",
    "    # Note: Excluding region dummies and premium status for unsupervised learning\n",
    "]\n",
    "\n",
    "X_cluster = df_ml[clustering_features].copy()\n",
    "\n",
    "print(f\"Clustering features: {list(X_cluster.columns)}\")\n",
    "print(f\"Number of customers: {X_cluster.shape[0]}\")\n",
    "print(f\"Number of features: {X_cluster.shape[1]}\")\n",
    "\n",
    "# Check data quality\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values: {X_cluster.isnull().sum().sum()}\")\n",
    "print(f\"Data types: {X_cluster.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Scale features for clustering (very important!)\n",
    "print(\"\\n🔧 SCALING FOR CLUSTERING:\")\n",
    "print(\"• Clustering algorithms use distance calculations\")\n",
    "print(\"• Features with larger scales dominate the distance\")\n",
    "print(\"• Example: Income ($50,000) vs Age (30) - income dominates\")\n",
    "print(\"• Solution: Scale all features to similar ranges\")\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "X_cluster_scaled = pd.DataFrame(X_cluster_scaled, columns=X_cluster.columns, index=X_cluster.index)\n",
    "\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_cluster.describe().round(2))\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_cluster_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\n✅ All features now have mean≈0 and std≈1 - ready for clustering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means Clustering\n",
    "print(\"🎯 Algorithm: K-Means Clustering\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# PARAMETER EXPLANATION: K-Means parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Means Clustering\")\n",
    "print(\"• What it does: Groups data into k clusters based on similarity\")\n",
    "print(\"• How it works: Finds k cluster centers that minimize distances to points\")\n",
    "print(\"• Strengths: Fast, simple, works well with spherical clusters\")\n",
    "print(\"• Weaknesses: Assumes spherical clusters, sensitive to initialization\")\n",
    "print(\"• n_clusters: Number of clusters to create\")\n",
    "print(\"• random_state: Seed for reproducible results\")\n",
    "print(\"• n_init: Number of random initializations (best result is kept)\")\n",
    "\n",
    "# For simplicity, we'll use 3 clusters (common for customer segmentation)\n",
    "optimal_k = 3\n",
    "print(f\"\\n🎯 Using k={optimal_k} clusters for clear customer segments\")\n",
    "\n",
    "# Create and fit K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=42,\n",
    "    n_init=10  # Try 10 different initializations\n",
    ")\n",
    "\n",
    "# Fit the model and get cluster assignments\n",
    "cluster_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to our dataframe\n",
    "df_clustered = df_ml.copy()\n",
    "df_clustered['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\n📊 Clustering Results:\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(f\"Final WCSS: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Successfully segmented {len(df_clustered)} customers into {optimal_k} clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and interpret customer segments\n",
    "print(\"🔍 CUSTOMER SEGMENT ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate cluster centers in original scale for interpretation\n",
    "cluster_centers_scaled = kmeans.cluster_centers_\n",
    "cluster_centers_original = scaler_cluster.inverse_transform(cluster_centers_scaled)\n",
    "\n",
    "# Create cluster centers DataFrame\n",
    "cluster_centers_df = pd.DataFrame(\n",
    "    cluster_centers_original,\n",
    "    columns=clustering_features,\n",
    "    index=[f'Cluster_{i}' for i in range(optimal_k)]\n",
    ")\n",
    "\n",
    "print(\"Cluster Centers (Average Values):\")\n",
    "print(cluster_centers_df.round(2))\n",
    "\n",
    "# Detailed analysis by cluster\n",
    "print(\"\\n📋 DETAILED CLUSTER PROFILES:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n🎯 CLUSTER {cluster_id} PROFILE ({len(cluster_data)} customers):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Demographics\n",
    "    print(f\"Demographics:\")\n",
    "    print(f\"  Average Age: {cluster_data['age'].mean():.1f} years\")\n",
    "    print(f\"  Average Income: ${cluster_data['income'].mean():.0f}\")\n",
    "    print(f\"  Education: {cluster_data['education'].mode().iloc[0]} (most common)\")\n",
    "    \n",
    "    # Behavior\n",
    "    print(f\"Behavior:\")\n",
    "    print(f\"  Average Purchases: {cluster_data['num_purchases'].mean():.1f}\")\n",
    "    print(f\"  Average Satisfaction: {cluster_data['satisfaction_score'].mean():.2f}/5.0\")\n",
    "    print(f\"  Average Experience: {cluster_data['experience_years'].mean():.1f} years\")\n",
    "    \n",
    "    # Business metrics\n",
    "    print(f\"Business Value:\")\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    print(f\"  Premium Rate: {premium_rate:.1%}\")\n",
    "    print(f\"  Average CLV: ${avg_clv:.0f}\")\n",
    "    \n",
    "    # Region distribution\n",
    "    top_region = cluster_data['region'].mode().iloc[0]\n",
    "    region_pct = (cluster_data['region'] == top_region).mean()\n",
    "    print(f\"  Top Region: {top_region} ({region_pct:.1%})\")\n",
    "\n",
    "# Compare clusters side by side\n",
    "print(\"\\n📊 CLUSTER COMPARISON TABLE:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "comparison_metrics = []\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    metrics = {\n",
    "        'Cluster': f'Cluster_{cluster_id}',\n",
    "        'Size': len(cluster_data),\n",
    "        'Avg_Age': cluster_data['age'].mean(),\n",
    "        'Avg_Income': cluster_data['income'].mean(),\n",
    "        'Avg_Satisfaction': cluster_data['satisfaction_score'].mean(),\n",
    "        'Premium_Rate': cluster_data['is_premium'].mean(),\n",
    "        'Avg_CLV': cluster_data['customer_lifetime_value'].mean()\n",
    "    }\n",
    "    comparison_metrics.append(metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_metrics)\n",
    "print(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business interpretation and actionable insights\n",
    "print(\"💼 BUSINESS INTERPRETATION & MARKETING STRATEGY\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Analyze each cluster for business insights\n",
    "cluster_insights = []\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    avg_age = cluster_data['age'].mean()\n",
    "    avg_income = cluster_data['income'].mean()\n",
    "    avg_satisfaction = cluster_data['satisfaction_score'].mean()\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    size = len(cluster_data)\n",
    "    \n",
    "    # Generate business interpretation\n",
    "    if avg_clv > 2500 and premium_rate > 0.4:\n",
    "        segment_type = \"High-Value Customers\"\n",
    "        strategy = \"VIP treatment, loyalty programs, premium services\"\n",
    "        priority = \"HIGH\"\n",
    "    elif avg_clv > 1800 and avg_satisfaction > 3.5:\n",
    "        segment_type = \"Growth Potential\"\n",
    "        strategy = \"Upselling, premium conversion campaigns\"\n",
    "        priority = \"MEDIUM\"\n",
    "    else:\n",
    "        segment_type = \"Standard Customers\"\n",
    "        strategy = \"Retention programs, satisfaction improvement\"\n",
    "        priority = \"LOW\"\n",
    "    \n",
    "    cluster_insights.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'segment_type': segment_type,\n",
    "        'strategy': strategy,\n",
    "        'priority': priority,\n",
    "        'size': size,\n",
    "        'avg_clv': avg_clv\n",
    "    })\n",
    "\n",
    "# Display business insights\n",
    "for insight in cluster_insights:\n",
    "    print(f\"\\n🎯 CLUSTER {insight['cluster_id']}: {insight['segment_type']}\")\n",
    "    print(f\"   Size: {insight['size']} customers\")\n",
    "    print(f\"   Average CLV: ${insight['avg_clv']:.0f}\")\n",
    "    print(f\"   Priority: {insight['priority']}\")\n",
    "    print(f\"   Strategy: {insight['strategy']}\")\n",
    "\n",
    "# Calculate business impact\n",
    "print(f\"\\n💰 BUSINESS IMPACT ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "total_clv = df_clustered['customer_lifetime_value'].sum()\n",
    "print(f\"Total Customer Value: ${total_clv:,.0f}\")\n",
    "\n",
    "for insight in cluster_insights:\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == insight['cluster_id']]\n",
    "    cluster_clv = cluster_data['customer_lifetime_value'].sum()\n",
    "    clv_percentage = (cluster_clv / total_clv) * 100\n",
    "    \n",
    "    print(f\"\\nCluster {insight['cluster_id']} ({insight['segment_type']}):\")\n",
    "    print(f\"  Total Value: ${cluster_clv:,.0f} ({clv_percentage:.1f}% of total)\")\n",
    "    print(f\"  Size: {insight['size']} customers ({insight['size']/len(df_clustered)*100:.1f}% of base)\")\n",
    "    print(f\"  Value per Customer: ${cluster_clv/insight['size']:.0f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(\"• Customer segmentation reveals distinct behavioral patterns\")\n",
    "print(\"• High-value segments deserve premium marketing investment\")\n",
    "print(\"• Growth potential segments are prime for upselling campaigns\")\n",
    "print(\"• Targeted strategies can improve overall customer lifetime value\")\n",
    "print(\"• Regular re-segmentation helps track customer evolution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}